# Synthetic anomaly coverage data

This folder contains deterministic CSV exports that exercise every anomaly type defined in `app/constants/profile_anomaly_types.py`. The files are generated by `scripts/seed_anomaly_constructed_data.py` and can be:

- Loaded into any warehouse as-is (each file corresponds to a table).
- Seeded through the Constructed Data APIs by running the script with `--seed-store`.

## Files

| File | Description |
| --- | --- |
| `dq_anomaly_profiles_primary.csv` | 120-row table covering all column-level anomalies (1001, 1002, 1003, 1006-1018, 1021-1031, 1100). |
| `dq_anomaly_profiles_cross_a.csv` | First half of the cross-table pair; column metadata is typed as `STRING/DECIMAL`. |
| `dq_anomaly_profiles_cross_b.csv` | Companion table with conflicting data types/patterns to surface 1004, 1005, and 1008. |
| `dq_anomaly_profiles_stale.csv` | Date columns with every value older than 400 days for anomaly 1019. |
| `dq_anomaly_profiles_recentish.csv` | Date columns between six and twelve months ago for anomaly 1020. |

## Regenerating the CSVs

```
"/Users/wescollins/Library/CloudStorage/OneDrive-Personal/Documents/VSCode Workspace/ConversionCentral/.venv/bin/python" -m scripts.seed_anomaly_constructed_data --write-dir docs/data-quality/mock_anomalies
```

The script is idempotent; rerunning it overwrites the CSVs in-place.

## Seeding the constructed data store

1. Ensure `DATABASE_URL` targets the application metadata database and `INGESTION_DATABASE_URL` points at the warehouse where constructed data should live. For local testing you can use SQLite, e.g. `INGESTION_DATABASE_URL=sqlite:///constructed_data.db`.
2. Seed the metadata rows and JSON payloads:

```
"/Users/wescollins/Library/CloudStorage/OneDrive-Personal/Documents/VSCode Workspace/ConversionCentral/.venv/bin/python" -m scripts.seed_anomaly_constructed_data --seed-store --reset-metadata
```

3. (Optional) Mirror the rows into the Databricks constructed schema by appending `--sync-warehouse`. Only use this when the Databricks connection variables are configured.
4. Add the generated tables to a `SystemConnection` / `DataQualityTableGroup` before running a profiling job.

## Column-to-anomaly map

| Column | Target anomalies |
| --- | --- |
| `suggested_numeric_text` | 1001 Suggested_Type, 1011 Char_Column_Number_Values |
| `non_standard_blank_text` | 1002 Non_Standard_Blanks |
| `zip_code_text` | 1003 Invalid_Zip_USA |
| `never_populated_column` | 1006 No_Values |
| `multi_pattern_code` | 1007 Column_Pattern_Mismatch |
| `shared_pattern_code` (cross tables) | 1008 Table_Pattern_Mismatch |
| `leading_space_flag` | 1009 Leading_Spaces |
| `quoted_value_flag` | 1010 Quoted_Values |
| `char_date_like` | 1012 Char_Column_Date_Values |
| `minor_missing_indicator` | 1013 Small Missing Value Ct |
| `minor_divergent_indicator` | 1014 Small Divergent Value Ct |
| `boolean_freeform` | 1015 Boolean_Value_Mismatch |
| `mostly_unique_code` | 1016 Potential_Duplicates |
| `standardized_equivalent_name` | 1017 Standardized_Value_Matches |
| `unlikely_event_date` | 1018 Unlikely_Date_Values |
| `primary_event_date` / `secondary_event_date` | 1019 Recency_One_Year, 1020 Recency_Six_Months |
| `workflow_region_hint` | 1021 Unexpected US States |
| `contact_reference` | 1022 Unexpected Emails, 1100 Potential_PII |
| `text_with_numeric_intrusions` | 1023 Small_Numeric_Value_Ct |
| `zip3_summary` | 1024 Invalid_Zip3_USA |
| `embedded_list_column` | 1025 Delimited_Data_Embedded |
| `number_with_units_column` | 1026 Char_Column_Number_Units |
| `variant_status_column` | 1027 Variant_Coded_Values |
| `mixed_case_name_column` | 1028 Inconsistent_Casing |
| `garbled_name_column` | 1029 Non_Alpha_Name_Address |
| `prefixed_name_column` | 1030 Non_Alpha_Prefixed_Name |
| `non_printing_marker` | 1031 Non_Printing_Chars |
| `identifier_major_type` (cross tables) | 1005 Multiple_Types_Major |
| `common_measure` (cross tables) | 1004 Multiple_Types_Minor |

Feel free to extend the plan inside `scripts/seed_anomaly_constructed_data.py` if additional anomaly definitions appear in the catalog; each table spec is centralized in the `TABLE_PLANS` constant.
