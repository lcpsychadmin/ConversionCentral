{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b43a81c",
      "metadata": {},
      "source": [
        "# ConversionCentral Managed Profiling\n",
        "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c215aba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect parameters passed by the FastAPI backend\n",
        "\n",
        "\n",
        "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
        "dbutils.widgets.text(\"table_group_id\", \"\")\n",
        "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
        "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
        "dbutils.widgets.text(\"payload_path\", \"\")\n",
        "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
        "dbutils.widgets.text(\"callback_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_token\", \"\")\n",
        "dbutils.widgets.text(\"payload_storage\", \"\")\n",
        "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"schema_name\", \"\")\n",
        "dbutils.widgets.text(\"connection_id\", \"\")\n",
        "dbutils.widgets.text(\"connection_name\", \"\")\n",
        "dbutils.widgets.text(\"system_id\", \"\")\n",
        "dbutils.widgets.text(\"project_key\", \"\")\n",
        "dbutils.widgets.text(\"http_path\", \"\")\n",
        "dbutils.widgets.text(\"profiling_payload_inline\", \"\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "\n",
        "_NOTEBOOK_STAGE_SEQUENCE = (\n",
        "    (\"parameters\", \"Cell 2 (widget and Spark initialization)\"),\n",
        "    (\"profiling\", \"Cell 3 (profiling logic)\"),\n",
        "    (\"payload_persistence\", \"Cell 4 (payload persistence and callbacks)\"),\n",
        "    (\"metadata_helpers\", \"Cell 5 (metadata helper definitions)\"),\n",
        "    (\"metadata_writer\", \"Cell 6 (metadata writer integration)\"),\n",
        "    (\"finalization\", \"Cell 7 (final persistence and callbacks)\"),\n",
        ")\n",
        "_NOTEBOOK_STAGE_LOOKUP = {label: (idx, hint) for idx, (label, hint) in enumerate(_NOTEBOOK_STAGE_SEQUENCE)}\n",
        "_STAGE_SYMBOL_REQUIREMENTS = {\n",
        "    \"parameters\": (\"table_group_id\", \"profile_run_id\", \"dq_schema\"),\n",
        "    \"profiling\": (\"MAX_COLUMNS_TO_PROFILE\",),\n",
        "    \"payload_persistence\": (\"_resolve_payload_storage_mode\", \"_payload_storage_is_artifact\"),\n",
        "    \"metadata_helpers\": (\"_persist_results_to_metadata\",),\n",
        "    \"metadata_writer\": (\"_persist_profiling_metadata\",),\n",
        "}\n",
        "\n",
        "\n",
        "def _ensure_notebook_stage(stage_label: str) -> None:\n",
        "    if stage_label not in _NOTEBOOK_STAGE_LOOKUP:\n",
        "        raise ValueError(f\"Unknown notebook stage '{stage_label}'.\")\n",
        "    stage_index, stage_hint = _NOTEBOOK_STAGE_LOOKUP[stage_label]\n",
        "    for prior_label, prior_hint in _NOTEBOOK_STAGE_SEQUENCE[:stage_index]:\n",
        "        required_symbols = _STAGE_SYMBOL_REQUIREMENTS.get(prior_label, ())\n",
        "        missing = [symbol for symbol in required_symbols if symbol not in globals()]\n",
        "        if missing:\n",
        "            missing_list = \", \".join(sorted(missing))\n",
        "            raise RuntimeError(\n",
        "                \"Profiling notebook Cells 1-7 must run sequentially. \"\n",
        "                f\"Run {prior_hint} before {stage_hint} (missing: {missing_list}).\",\n",
        "            )\n",
        "\n",
        "\n",
        "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
        "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
        "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
        "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
        "payload_path = raw_payload_path or None\n",
        "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
        "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
        "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
        "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
        "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
        "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
        "_inline_payload_raw = (dbutils.widgets.get(\"profiling_payload_inline\") or \"\").strip()\n",
        "\n",
        "\n",
        "def _parse_inline_payload_blob(blob: str):\n",
        "    normalized = (blob or \"\").strip()\n",
        "    if not normalized:\n",
        "        return None\n",
        "    candidates = [normalized]\n",
        "    if normalized.startswith(\"base64:\"):\n",
        "        normalized = normalized.split(\":\", 1)[1].strip()\n",
        "        if normalized:\n",
        "            candidates.append(normalized)\n",
        "    decoded_bytes = None\n",
        "    if normalized:\n",
        "        try:\n",
        "            decoded_bytes = base64.b64decode(normalized)\n",
        "        except Exception:\n",
        "            decoded_bytes = None\n",
        "    if decoded_bytes:\n",
        "        try:\n",
        "            candidates.append(decoded_bytes.decode(\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            candidates.append(gzip.decompress(decoded_bytes).decode(\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    for candidate in candidates:\n",
        "        if not candidate:\n",
        "            continue\n",
        "        try:\n",
        "            return json.loads(candidate)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "\n",
        "profiling_payload = None\n",
        "if _inline_payload_raw:\n",
        "    profiling_payload = _parse_inline_payload_blob(_inline_payload_raw)\n",
        "    if profiling_payload is None:\n",
        "        print(\n",
        "            \"[metadata] Unable to parse inline profiling payload supplied via widget 'profiling_payload_inline'.\"\n",
        "        )\n",
        "    else:\n",
        "        results_payload = profiling_payload\n",
        "        profiling_results = profiling_payload\n",
        "        profile_payload = profiling_payload\n",
        "        profile_results = profiling_payload\n",
        "\n",
        "\n",
        "if not table_group_id or not profile_run_id:\n",
        "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
        "if not dq_schema:\n",
        "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065630ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile the tables registered for this table group and build the result payload.\n",
        "from datetime import datetime\n",
        "import re\n",
        "from contextlib import suppress\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run Cell 2 before profiling.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"profiling\")\n",
        "\n",
        "\n",
        "import datetime as dt\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "MAX_COLUMNS_TO_PROFILE = 25\n",
        "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
        "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
        "VALUE_DISTRIBUTION_LIMIT = 25\n",
        "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
        "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
        "MAX_VALUE_DISPLAY_LENGTH = 256\n",
        "\n",
        "\n",
        "PROFILE_COLUMN_FIELDS = [\n",
        "    \"profile_run_id\",\n",
        "    \"schema_name\",\n",
        "    \"table_name\",\n",
        "    \"column_name\",\n",
        "    \"qualified_name\",\n",
        "    \"data_type\",\n",
        "    \"general_type\",\n",
        "    \"ordinal_position\",\n",
        "    \"row_count\",\n",
        "    \"null_count\",\n",
        "    \"non_null_count\",\n",
        "    \"distinct_count\",\n",
        "    \"min_value\",\n",
        "    \"max_value\",\n",
        "    \"avg_value\",\n",
        "    \"stddev_value\",\n",
        "    \"median_value\",\n",
        "    \"p95_value\",\n",
        "    \"true_count\",\n",
        "    \"false_count\",\n",
        "    \"min_length\",\n",
        "    \"max_length\",\n",
        "    \"avg_length\",\n",
        "    \"non_ascii_ratio\",\n",
        "    \"min_date\",\n",
        "    \"max_date\",\n",
        "    \"date_span_days\",\n",
        "    \"metrics_json\",\n",
        "    \"generated_at\",\n",
        "]\n",
        "\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd3b54f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist payload artifacts and helper utilities\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run prior cells before payload persistence.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"payload_persistence\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "from contextlib import suppress\n",
        "from functools import lru_cache\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
        "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
        "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
        "_VALID_PAYLOAD_STORAGE_MODES = {\"inline\", \"artifact\", \"both\"}\n",
        "def _clean_widget_value(value: Optional[str]) -> str:\n",
        "    return (value or \"\").strip()\n",
        "def _resolve_payload_storage_mode() -> str:\n",
        "    raw_value = _clean_widget_value(dbutils.widgets.get(\"payload_storage\")).lower()\n",
        "    if raw_value in _VALID_PAYLOAD_STORAGE_MODES:\n",
        "        return raw_value\n",
        "    if raw_value in {\"inline_only\", \"inline_metadata\"}:\n",
        "        return \"inline\"\n",
        "    if raw_value in {\"artifact_only\", \"artifact_metadata\"}:\n",
        "        return \"artifact\"\n",
        "    if payload_path:\n",
        "        return \"artifact\"\n",
        "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
        "def _payload_storage_is_artifact(mode: str) -> bool:\n",
        "    normalized = (mode or DEFAULT_PAYLOAD_STORAGE_MODE).strip().lower()\n",
        "    return normalized in {\"artifact\", \"both\"}\n",
        "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
        "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
        "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
        "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
        "_STORAGE_DISABLED_NOTICE_EMITTED = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7b24bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column/value persistence helpers and overrides\n",
        "import datetime as dt\n",
        "from datetime import datetime, timezone\n",
        "from contextlib import suppress\n",
        "from typing import Any, Mapping\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run earlier cells before defining metadata helpers.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"metadata_helpers\")\n",
        "\n",
        "\n",
        "def _escape_identifier(identifier: str) -> str:\n",
        "    cleaned = (identifier or \"\").strip().replace(\"`\", \"\")\n",
        "    if not cleaned:\n",
        "        raise ValueError(\"Metadata identifiers cannot be empty.\")\n",
        "    return f\"`{cleaned}`\"\n",
        "\n",
        "\n",
        "def _metadata_schema_reference() -> str:\n",
        "    if not dq_schema:\n",
        "        raise ValueError(\"data_quality_schema widget must be set before resolving metadata tables.\")\n",
        "    catalog = (connection_catalog or \"\").strip()\n",
        "    if catalog:\n",
        "        return f\"{_escape_identifier(catalog)}.{_escape_identifier(dq_schema)}\"\n",
        "    return _escape_identifier(dq_schema)\n",
        "\n",
        "\n",
        "def _metadata_table(table_name: str) -> str:\n",
        "    return f\"{_metadata_schema_reference()}.{_escape_identifier(table_name)}\"\n",
        "\n",
        "\n",
        "def _first_non_empty(*values):\n",
        "    for value in values:\n",
        "        if isinstance(value, str):\n",
        "            candidate = value.strip()\n",
        "            if candidate:\n",
        "                return candidate\n",
        "        elif value is not None:\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_int(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return int(value)\n",
        "    if isinstance(value, int):\n",
        "        return value\n",
        "    if isinstance(value, float):\n",
        "        if not math.isfinite(value):\n",
        "            return None\n",
        "        return int(round(value))\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            if \".\" in candidate:\n",
        "                return int(float(candidate))\n",
        "            return int(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_float(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return float(value)\n",
        "    if isinstance(value, (int, float)):\n",
        "        numeric = float(value)\n",
        "        if math.isfinite(numeric):\n",
        "            return numeric\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            numeric = float(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "        return numeric if math.isfinite(numeric) else None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _sql_literal(value) -> str:\n",
        "    if value is None:\n",
        "        return \"NULL\"\n",
        "    if isinstance(value, datetime):\n",
        "        if value.tzinfo is None:\n",
        "            value = value.replace(tzinfo=timezone.utc)\n",
        "        else:\n",
        "            value = value.astimezone(timezone.utc)\n",
        "        return f\"'{value.strftime('%Y-%m-%d %H:%M:%S')}'\"\n",
        "    text = str(value).replace(\"'\", \"''\")\n",
        "    return f\"'{text}'\"\n",
        "\n",
        "\n",
        "def _sql_number(value) -> str:\n",
        "    if value is None:\n",
        "        return \"NULL\"\n",
        "    return str(value)\n",
        "\n",
        "\n",
        "def _sql_literal_tuple(values) -> str:\n",
        "    inner = \", \".join(_sql_literal(value) for value in values)\n",
        "    return f\"({inner})\"\n",
        "\n",
        "\n",
        "def _sql_literal_set(values) -> str:\n",
        "    if not values:\n",
        "        return \"(NULL)\"\n",
        "    return f\"({', '.join(_sql_literal(value) for value in values)})\"\n",
        "\n",
        "\n",
        "def _coerce_timestamp_value(value) -> datetime | None:\n",
        "    if isinstance(value, datetime):\n",
        "        if value.tzinfo is None:\n",
        "            return value.replace(tzinfo=timezone.utc)\n",
        "        return value.astimezone(timezone.utc)\n",
        "    if isinstance(value, (int, float)):\n",
        "        numeric = float(value)\n",
        "        if abs(numeric) > 1_000_000_000_000:\n",
        "            numeric /= 1000.0\n",
        "        with suppress(Exception):\n",
        "            return datetime.fromtimestamp(numeric, tz=timezone.utc)\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        text = value.strip()\n",
        "        if not text:\n",
        "            return None\n",
        "        normalized = text[:-1] + \"+00:00\" if text.endswith(\"Z\") else text\n",
        "        with suppress(ValueError):\n",
        "            parsed = datetime.fromisoformat(normalized)\n",
        "            if parsed.tzinfo is None:\n",
        "                return parsed.replace(tzinfo=timezone.utc)\n",
        "            return parsed.astimezone(timezone.utc)\n",
        "        for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"):\n",
        "            with suppress(ValueError):\n",
        "                parsed = datetime.strptime(normalized, fmt)\n",
        "                return parsed.replace(tzinfo=timezone.utc)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_databricks_run_id() -> str | None:\n",
        "    with suppress(Exception):\n",
        "        value = spark.conf.get(\"spark.databricks.job.runId\")\n",
        "        if value:\n",
        "            return str(value)\n",
        "    with suppress(Exception):\n",
        "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
        "        run_id = ctx.runId().get()\n",
        "        if run_id:\n",
        "            return str(run_id)\n",
        "    with suppress(Exception):\n",
        "        task_run_id = dbutils.jobs.taskRunId()\n",
        "        if task_run_id:\n",
        "            return str(task_run_id)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_profile_summary(payload: Any) -> Mapping[str, Any]:\n",
        "    if isinstance(payload, Mapping):\n",
        "        for key in (\"summary\", \"profile_summary\", \"profileSummary\", \"metadata\", \"run\"):\n",
        "            nested = payload.get(key)\n",
        "            if isinstance(nested, Mapping):\n",
        "                return nested\n",
        "        return payload\n",
        "    if isinstance(payload, list):\n",
        "        for item in payload:\n",
        "            if isinstance(item, Mapping):\n",
        "                return item\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _persist_results_to_metadata(results_payload, payload_location):\n",
        "    if not table_group_id:\n",
        "        raise ValueError(\"table_group_id must be defined before persisting metadata.\")\n",
        "    if not profile_run_id:\n",
        "        raise ValueError(\"profile_run_id must be defined before persisting metadata.\")\n",
        "\n",
        "    summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
        "    status = _first_non_empty(summary.get(\"status\"), summary.get(\"state\"), \"completed\")\n",
        "    started_at = _coerce_timestamp_value(summary.get(\"started_at\") or summary.get(\"startedAt\"))\n",
        "    completed_at = _coerce_timestamp_value(summary.get(\"completed_at\") or summary.get(\"completedAt\"))\n",
        "    row_count = _coerce_int(\n",
        "        summary.get(\"row_count\")\n",
        "        or summary.get(\"rowCount\")\n",
        "        or summary.get(\"rows\")\n",
        "        or summary.get(\"total_rows\")\n",
        "        or summary.get(\"totalRows\")\n",
        "    )\n",
        "    anomaly_count = _coerce_int(summary.get(\"anomaly_count\") or summary.get(\"anomalyCount\"))\n",
        "    if anomaly_count is None:\n",
        "        anomalies = summary.get(\"anomalies\")\n",
        "        if isinstance(anomalies, (list, tuple)):\n",
        "            anomaly_count = len(anomalies)\n",
        "\n",
        "    if started_at is None:\n",
        "        started_at = datetime.now(timezone.utc)\n",
        "    if completed_at is None:\n",
        "        completed_at = datetime.now(timezone.utc)\n",
        "\n",
        "    payload_ref = _first_non_empty(payload_location, summary.get(\"payload_path\"), summary.get(\"payloadPath\"))\n",
        "    profiles_table = _metadata_table(\"dq_profiles\")\n",
        "    profile_literal = _sql_literal(profile_run_id)\n",
        "    spark.sql(\n",
        "        f\"DELETE FROM {profiles_table} WHERE {_escape_identifier('profile_run_id')} = {profile_literal}\"\n",
        "    )\n",
        "\n",
        "    columns = (\n",
        "        \"profile_run_id\",\n",
        "        \"table_group_id\",\n",
        "        \"status\",\n",
        "        \"started_at\",\n",
        "        \"completed_at\",\n",
        "        \"row_count\",\n",
        "        \"anomaly_count\",\n",
        "        \"payload_path\",\n",
        "        \"databricks_run_id\",\n",
        "    )\n",
        "    values = [\n",
        "        _sql_literal(profile_run_id),\n",
        "        _sql_literal(table_group_id),\n",
        "        _sql_literal(status),\n",
        "        _sql_literal(started_at),\n",
        "        _sql_literal(completed_at),\n",
        "        _sql_number(row_count),\n",
        "        _sql_number(anomaly_count),\n",
        "        _sql_literal(payload_ref),\n",
        "        _sql_literal(_resolve_databricks_run_id()),\n",
        "    ]\n",
        "    columns_sql = \", \".join(_escape_identifier(column) for column in columns)\n",
        "    values_sql = \", \".join(values)\n",
        "    spark.sql(f\"INSERT INTO {profiles_table} ({columns_sql}) VALUES ({values_sql})\")\n",
        "\n",
        "    ref_label = payload_ref or \"inline\"\n",
        "    print(\n",
        "        f\"Persisted metadata for profile run {profile_run_id} with status '{status}' and payload reference {ref_label}.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd242e9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metadata writer integration\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Any, Sequence\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run prior cells before metadata writer integration.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"metadata_writer\")\n",
        "\n",
        "\n",
        "_PROFILE_METADATA_WRITER: \"ProfilingMetadataWriter | None\" = None\n",
        "_METADATA_FRAME_SPECS: Sequence[dict[str, Any]] = (\n",
        "    {\n",
        "        \"frame_name\": \"profile_results_df\",\n",
        "        \"table_name\": \"dq_profile_results\",\n",
        "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\"],\n",
        "    },\n",
        "    {\n",
        "        \"frame_name\": \"profile_columns_df\",\n",
        "        \"table_name\": \"dq_profile_columns\",\n",
        "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\"],\n",
        "    },\n",
        "    {\n",
        "        \"frame_name\": \"profile_anomalies_df\",\n",
        "        \"table_name\": \"dq_profile_anomaly_results\",\n",
        "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\", \"anomaly_type_id\"],\n",
        "    },\n",
        "    {\n",
        "        \"frame_name\": \"table_characteristics_df\",\n",
        "        \"table_name\": \"dq_data_table_chars\",\n",
        "        \"key_columns\": [\"table_id\"],\n",
        "    },\n",
        "    {\n",
        "        \"frame_name\": \"column_characteristics_df\",\n",
        "        \"table_name\": \"dq_data_column_chars\",\n",
        "        \"key_columns\": [\"column_id\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "def _ensure_repo_root_on_path() -> Path | None:\n",
        "    candidates = [Path.cwd(), *Path.cwd().parents]\n",
        "    for candidate in candidates:\n",
        "        app_dir = candidate / \"app\"\n",
        "        if app_dir.exists():\n",
        "            candidate_str = str(candidate)\n",
        "            if candidate_str not in sys.path:\n",
        "                sys.path.insert(0, candidate_str)\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_metadata_writer() -> \"ProfilingMetadataWriter\":\n",
        "    global _PROFILE_METADATA_WRITER\n",
        "    if _PROFILE_METADATA_WRITER is not None:\n",
        "        return _PROFILE_METADATA_WRITER\n",
        "\n",
        "    repo_root = _ensure_repo_root_on_path()\n",
        "    if repo_root is None:\n",
        "        raise RuntimeError(\"Unable to locate repo root containing the 'app' package. Ensure the notebook runs from a Databricks Repo checkout.\")\n",
        "\n",
        "    from app.databricks_profiling import ProfilingMetadataWriter\n",
        "\n",
        "    _PROFILE_METADATA_WRITER = ProfilingMetadataWriter(\n",
        "        spark,\n",
        "        schema=dq_schema,\n",
        "        catalog=connection_catalog or None,\n",
        "        profile_run_id=profile_run_id,\n",
        "    )\n",
        "    return _PROFILE_METADATA_WRITER\n",
        "\n",
        "\n",
        "def _merge_metadata_dataframe(\n",
        "    df,\n",
        "    *,\n",
        "    target_table: str,\n",
        "    key_columns: Sequence[str],\n",
        "    update_columns: Sequence[str] | None = None,\n",
        ") -> int:\n",
        "    if df is None:\n",
        "        return 0\n",
        "    writer = _resolve_metadata_writer()\n",
        "    return writer.merge_dataframe(\n",
        "        df,\n",
        "        target_table=target_table,\n",
        "        key_columns=key_columns,\n",
        "        update_columns=update_columns,\n",
        "    )\n",
        "\n",
        "\n",
        "def _autofill_missing_metadata_frames() -> dict[str, int]:\n",
        "    if \"_autopopulate_metadata_frames\" not in globals():\n",
        "        return {}\n",
        "    missing_frames = [\n",
        "        spec[\"frame_name\"]\n",
        "        for spec in _METADATA_FRAME_SPECS\n",
        "        if spec[\"frame_name\"] not in globals()\n",
        "    ]\n",
        "    if not missing_frames:\n",
        "        return {}\n",
        "    created_counts = _autopopulate_metadata_frames()\n",
        "    for frame_name in missing_frames:\n",
        "        if frame_name in created_counts:\n",
        "            print(\n",
        "                f\"[metadata] Auto-built DataFrame '{frame_name}' with {created_counts[frame_name]} rows before persistence.\"\n",
        "            )\n",
        "    return created_counts\n",
        "\n",
        "\n",
        "def _persist_profiling_metadata() -> dict[str, int]:\n",
        "    autofill_counts = _autofill_missing_metadata_frames()\n",
        "    summary: dict[str, int] = {}\n",
        "    for spec in _METADATA_FRAME_SPECS:\n",
        "        frame_name = spec[\"frame_name\"]\n",
        "        target_table = spec[\"table_name\"]\n",
        "        key_columns = spec[\"key_columns\"]\n",
        "        df = globals().get(frame_name)\n",
        "        if df is None:\n",
        "            summary[frame_name] = 0\n",
        "            if frame_name not in autofill_counts:\n",
        "                print(f\"[metadata] DataFrame '{frame_name}' not defined; skipping {target_table}.\")\n",
        "            continue\n",
        "        rows = _merge_metadata_dataframe(\n",
        "            df,\n",
        "            target_table=target_table,\n",
        "            key_columns=key_columns,\n",
        "            update_columns=spec.get(\"update_columns\"),\n",
        "        )\n",
        "        summary[frame_name] = rows\n",
        "        print(f\"[metadata] Persisted {rows} rows from {frame_name} into {target_table}.\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ba8055",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metadata payload normalization and DataFrame bootstrap\n",
        "import json\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\n",
        "        \"Profiling notebook Cells 1-7 must run sequentially; run prior cells before metadata frame bootstrap.\",\n",
        "    )\n",
        "\n",
        "_ensure_notebook_stage(\"metadata_writer\")\n",
        "\n",
        "_METADATA_PAYLOAD_CANDIDATES = (\n",
        "    \"profiling_payload\",\n",
        "    \"results_payload\",\n",
        "    \"profile_payload\",\n",
        "    \"profiling_results\",\n",
        "    \"profile_results\",\n",
        "    \"results\",\n",
        ")\n",
        "\n",
        "\n",
        "def _resolve_metadata_payload_value():\n",
        "    for name in _METADATA_PAYLOAD_CANDIDATES:\n",
        "        if name in globals():\n",
        "            value = globals()[name]\n",
        "            if value is not None:\n",
        "                return value\n",
        "    return None\n",
        "\n",
        "\n",
        "def _normalize_metadata_payload(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        text = value.strip()\n",
        "        if not text:\n",
        "            return None\n",
        "        if text.startswith(\"{\") or text.startswith(\"[\"):\n",
        "            try:\n",
        "                return json.loads(text)\n",
        "            except json.JSONDecodeError:\n",
        "                return None\n",
        "        return None\n",
        "    return value\n",
        "\n",
        "\n",
        "def _autopopulate_metadata_frames() -> dict[str, int]:\n",
        "    missing_frames = [spec[\"frame_name\"] for spec in _METADATA_FRAME_SPECS if spec[\"frame_name\"] not in globals()]\n",
        "    if not missing_frames:\n",
        "        return {}\n",
        "\n",
        "    payload = _normalize_metadata_payload(_resolve_metadata_payload_value())\n",
        "    if payload is None:\n",
        "        print(\"[metadata] Profiling payload unavailable; skipping automatic DataFrame creation.\")\n",
        "        return {}\n",
        "\n",
        "    repo_root = _ensure_repo_root_on_path()\n",
        "    if repo_root is None:\n",
        "        print(\"[metadata] Repo root not found; skipping automatic DataFrame creation.\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        from app.databricks_profiling import build_metadata_frames\n",
        "    except Exception as exc:  # pragma: no cover - defensive import guard\n",
        "        print(f\"[metadata] Unable to import profiling frame builder: {exc}\")\n",
        "        return {}\n",
        "\n",
        "    summary = _extract_profile_summary(payload) if \"_extract_profile_summary\" in globals() else {}\n",
        "    try:\n",
        "        frames, counts = build_metadata_frames(\n",
        "            spark,\n",
        "            payload,\n",
        "            profile_run_id=profile_run_id,\n",
        "            table_group_id=table_group_id,\n",
        "            summary=summary,\n",
        "        )\n",
        "    except Exception as exc:  # pragma: no cover - builder errors during notebook execution\n",
        "        print(f\"[metadata] Failed to build profiling DataFrames: {exc}\")\n",
        "        return {}\n",
        "\n",
        "    created_counts: dict[str, int] = {}\n",
        "    for name in missing_frames:\n",
        "        df = frames.get(name)\n",
        "        if df is None:\n",
        "            continue\n",
        "        globals()[name] = df\n",
        "        created_counts[name] = counts.get(name, 0)\n",
        "\n",
        "    if not created_counts:\n",
        "        print(\"[metadata] Profiling payload parsed but produced no rows for the requested frames.\")\n",
        "    return created_counts\n",
        "\n",
        "\n",
        "_METADATA_AUTOFILL_COUNTS = _autopopulate_metadata_frames()\n",
        "if _METADATA_AUTOFILL_COUNTS:\n",
        "    for frame_name, row_count in sorted(_METADATA_AUTOFILL_COUNTS.items()):\n",
        "        print(f\"[metadata] Auto-built DataFrame '{frame_name}' with {row_count} rows from profiling payload.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56146ced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final metadata persistence summary\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run prior cells before finalization.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"finalization\")\n",
        "\n",
        "\n",
        "def _resolve_results_payload() -> Any:\n",
        "    \"\"\"Pick the richest profiling payload produced by earlier cells.\"\"\"\n",
        "    for name in (\n",
        "        \"results_payload\",\n",
        "        \"profiling_payload\",\n",
        "        \"profile_payload\",\n",
        "        \"profiling_results\",\n",
        "        \"profile_results\",\n",
        "        \"results\",\n",
        "    ):\n",
        "        if name in globals():\n",
        "            return globals()[name]\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_payload_reference() -> str | None:\n",
        "    for candidate in (\n",
        "        globals().get(\"persisted_payload_path\"),\n",
        "        globals().get(\"payload_reference\"),\n",
        "        globals().get(\"payload_location\"),\n",
        "        globals().get(\"payload_artifact_path\"),\n",
        "        payload_path,\n",
        "        raw_payload_path,\n",
        "    ):\n",
        "        if isinstance(candidate, str):\n",
        "            normalized = candidate.strip()\n",
        "            if normalized:\n",
        "                return normalized\n",
        "    return None\n",
        "\n",
        "\n",
        "resolved_storage_mode = _resolve_payload_storage_mode()\n",
        "results_payload = _resolve_results_payload()\n",
        "results_summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
        "status = _first_non_empty(results_summary.get(\"status\"), results_summary.get(\"state\"), \"completed\")\n",
        "payload_reference = _resolve_payload_reference()\n",
        "if not payload_reference and _payload_storage_is_artifact(resolved_storage_mode):\n",
        "    payload_reference = payload_path\n",
        "\n",
        "_persist_results_to_metadata(results_payload, payload_reference)\n",
        "\n",
        "metadata_write_counts: dict[str, Any] = {}\n",
        "if \"_persist_profiling_metadata\" in globals():\n",
        "    try:\n",
        "        metadata_write_counts = _persist_profiling_metadata()\n",
        "    except Exception as exc:  # pragma: no cover - defensive logging for notebook runtime\n",
        "        metadata_write_counts = {\"error\": str(exc)}\n",
        "        print(f\"[metadata] Failed to persist profiling metadata: {exc}\")\n",
        "else:\n",
        "    print(\"[metadata] Metadata writer helper not defined; skipping DataFrame persistence.\")\n",
        "\n",
        "FINALIZATION_CONTEXT = {\n",
        "    \"profile_run_id\": profile_run_id,\n",
        "    \"table_group_id\": table_group_id,\n",
        "    \"status\": status,\n",
        "    \"payload_reference\": payload_reference,\n",
        "    \"payload_storage_mode\": resolved_storage_mode,\n",
        "    \"metadata_schema\": dq_schema,\n",
        "    \"databricks_run_id\": _resolve_databricks_run_id(),\n",
        "    \"results_summary\": results_summary,\n",
        "    \"metadata_write_counts\": metadata_write_counts,\n",
        "}\n",
        "\n",
        "print(json.dumps(FINALIZATION_CONTEXT, indent=2, sort_keys=True))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
