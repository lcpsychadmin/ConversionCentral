{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"payload_storage\", \"\")\n",
    "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
    "payload_path = raw_payload_path or None\n",
    "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from typing import Iterable\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, BinaryType, MapType, StructType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "\n",
    "def _split_identifier(value: str | None) -> list[str]:\n",
    "    cleaned = (value or \"\").replace(\"`\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    return [segment.strip() for segment in cleaned.split(\".\") if segment.strip()]\n",
    "\n",
    "def _catalog_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None\n",
    "\n",
    "def _schema_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if not parts:\n",
    "        return None\n",
    "    return parts[-1]\n",
    "\n",
    "def _qualify(*parts: Iterable[str | None]) -> str:\n",
    "    tokens: list[str] = []\n",
    "    for part in parts:\n",
    "        if isinstance(part, (list, tuple)):\n",
    "            tokens.extend([token for token in part if token])\n",
    "        elif part:\n",
    "            tokens.append(part)\n",
    "    if not tokens:\n",
    "        raise ValueError(\"Cannot build a fully qualified identifier with no parts.\")\n",
    "    return \".\".join(f\"`{token}`\" for token in tokens)\n",
    "\n",
    "metadata_catalog = _catalog_component(dq_schema)\n",
    "metadata_schema = _schema_component(dq_schema)\n",
    "if metadata_schema is None:\n",
    "    raise ValueError(\"Unable to resolve schema portion of the data quality schema setting.\")\n",
    "if metadata_catalog is None:\n",
    "    fallback_catalog = _catalog_component(connection_catalog)\n",
    "    if fallback_catalog:\n",
    "        metadata_catalog = fallback_catalog\n",
    "    else:\n",
    "        with suppress(Exception):\n",
    "            metadata_catalog = spark.catalog.currentCatalog()\n",
    "\n",
    "connection_catalog_clean = _catalog_component(connection_catalog)\n",
    "connection_schema_clean = _schema_component(connection_schema)\n",
    "\n",
    "def _metadata_table(name: str) -> str:\n",
    "    return _qualify(metadata_catalog, metadata_schema, name) if metadata_catalog else _qualify(metadata_schema, name)\n",
    "\n",
    "def _compile_patterns(mask: str | None) -> list[re.Pattern[str]]:\n",
    "    if not mask:\n",
    "        return []\n",
    "    tokens = [token.strip() for token in re.split(r\"[\\n,]+\", mask) if token.strip()]\n",
    "    compiled: list[re.Pattern[str]] = []\n",
    "    for token in tokens:\n",
    "        escaped = re.escape(token).replace(\"\\\\*\", \".*\").replace(\"\\\\%\", \".*\")\n",
    "        compiled.append(re.compile(f\"^{escaped}$\", re.IGNORECASE))\n",
    "    return compiled\n",
    "\n",
    "def _matches_pattern(patterns: list[re.Pattern[str]], schema_name: str | None, table_name: str) -> bool:\n",
    "    if not patterns:\n",
    "        return False\n",
    "    candidate_full = \".\".join(filter(None, [(schema_name or \"\").lower(), table_name.lower()])).strip(\".\")\n",
    "    short_name = table_name.lower()\n",
    "    for pattern in patterns:\n",
    "        if pattern.match(candidate_full) or pattern.match(short_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _qualify_data_table(raw_schema: str | None, table_name: str) -> str:\n",
    "    table_tokens = _split_identifier(table_name)\n",
    "    if len(table_tokens) >= 2:\n",
    "        return _qualify(table_tokens)\n",
    "\n",
    "    schema_tokens = _split_identifier(raw_schema)\n",
    "    if len(schema_tokens) >= 2:\n",
    "        return _qualify(schema_tokens + table_tokens)\n",
    "\n",
    "    catalog_part = connection_catalog_clean\n",
    "    schema_part = connection_schema_clean\n",
    "    if len(schema_tokens) == 1:\n",
    "        schema_part = schema_tokens[0]\n",
    "    elif schema_tokens:\n",
    "        schema_part = schema_tokens[-1]\n",
    "\n",
    "    return _qualify(catalog_part, schema_part, table_tokens[0] if table_tokens else table_name)\n",
    "\n",
    "def _select_profile_columns(df) -> list[str]:\n",
    "    allowed: list[str] = []\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, (BinaryType, MapType, ArrayType, StructType)):\n",
    "            continue\n",
    "        allowed.append(field.name)\n",
    "        if len(allowed) >= MAX_COLUMNS_TO_PROFILE:\n",
    "            break\n",
    "    return allowed\n",
    "\n",
    "def _record_anomaly(buffer: list[dict[str, str]], table_name: str, column_name: str | None, anomaly_type: str, severity: str, description: str, detected_at: str) -> None:\n",
    "    buffer.append(\n",
    "        {\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"anomaly_type\": anomaly_type,\n",
    "            \"severity\": severity,\n",
    "            \"description\": description,\n",
    "            \"detected_at\": detected_at,\n",
    "        }\n",
    "    )\n",
    "\n",
    "metadata_tables_name = _metadata_table(\"dq_tables\")\n",
    "group_table_name = _metadata_table(\"dq_table_groups\")\n",
    "group_rows = (\n",
    "    spark.table(group_table_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"name\", \"profiling_include_mask\", \"profiling_exclude_mask\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "if not group_rows:\n",
    "    raise ValueError(f\"Table group '{table_group_id}' not found in schema '{dq_schema}'.\")\n",
    "group_details = group_rows[0].asDict()\n",
    "include_patterns = _compile_patterns(group_details.get(\"profiling_include_mask\"))\n",
    "exclude_patterns = _compile_patterns(group_details.get(\"profiling_exclude_mask\"))\n",
    "\n",
    "table_rows = (\n",
    "    spark.table(metadata_tables_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"schema_name\", \"table_name\")\n",
    "    .collect()\n",
    ")\n",
    "if not table_rows:\n",
    "    raise ValueError(f\"No dq_tables rows registered for table_group_id '{table_group_id}'.\")\n",
    "\n",
    "table_candidates: list[dict[str, str]] = []\n",
    "for row in table_rows:\n",
    "    schema_value = (row[\"schema_name\"] or connection_schema_clean or \"\").strip() or None\n",
    "    table_value = (row[\"table_name\"] or \"\").strip()\n",
    "    if not table_value:\n",
    "        continue\n",
    "    if include_patterns and not _matches_pattern(include_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    if exclude_patterns and _matches_pattern(exclude_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    label = \".\".join(filter(None, [schema_value, table_value])) or table_value\n",
    "    table_candidates.append({\"schema_name\": schema_value, \"table_name\": table_value, \"label\": label})\n",
    "\n",
    "if not table_candidates:\n",
    "    raise ValueError(\"All candidate tables were filtered out by include/exclude masks.\")\n",
    "\n",
    "generated_at = datetime.utcnow().isoformat() + \"Z\"\n",
    "anomalies: list[dict[str, str]] = []\n",
    "table_profiles: list[dict[str, object]] = []\n",
    "total_rows = 0\n",
    "profiling_failures = 0\n",
    "profiling_successes = 0\n",
    "\n",
    "print(f\"Profiling {len(table_candidates)} tables for group {table_group_id}.\")\n",
    "for candidate in table_candidates:\n",
    "    schema_value = candidate[\"schema_name\"]\n",
    "    table_value = candidate[\"table_name\"]\n",
    "    label = candidate[\"label\"]\n",
    "    qualified_name = _qualify_data_table(schema_value, table_value)\n",
    "    table_result: dict[str, object] = {\n",
    "        \"table_name\": label,\n",
    "        \"qualified_name\": qualified_name,\n",
    "    }\n",
    "    print(f\"-> Scanning {qualified_name}\")\n",
    "    try:\n",
    "        df = spark.read.table(qualified_name)\n",
    "    except AnalysisException as exc:\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"missing_table\", \"high\", f\"Spark could not read {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "    except Exception as exc:\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"profiling_error\", \"high\", f\"Unexpected error while reading {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "\n",
    "    row_count = df.count()\n",
    "    table_result[\"row_count\"] = int(row_count)\n",
    "    total_rows += row_count\n",
    "\n",
    "    if row_count == 0:\n",
    "        profiling_failures += 1\n",
    "        _record_anomaly(anomalies, label, None, \"empty_table\", \"high\", \"Table returned zero rows during profiling.\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "\n",
    "    profiling_successes += 1\n",
    "    profile_columns = _select_profile_columns(df)\n",
    "    table_result[\"profiled_columns\"] = profile_columns\n",
    "    full_column_count = len(df.columns)\n",
    "    if full_column_count > len(profile_columns):\n",
    "        table_result[\"profiled_columns_truncated\"] = full_column_count - len(profile_columns)\n",
    "\n",
    "    if profile_columns:\n",
    "        agg_exprs = [F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0)).alias(col_name) for col_name in profile_columns]\n",
    "        null_counts = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "        column_null_ratios: dict[str, float] = {}\n",
    "        for column in profile_columns:\n",
    "            null_ratio = float((null_counts.get(column, 0) or 0) / row_count)\n",
    "            column_null_ratios[column] = null_ratio\n",
    "            if null_ratio >= NULL_RATIO_ALERT_THRESHOLD:\n",
    "                severity = \"high\" if null_ratio >= HIGH_NULL_RATIO_THRESHOLD else \"medium\"\n",
    "                description = f\"Null ratio {null_ratio:.2%} exceeds {NULL_RATIO_ALERT_THRESHOLD:.0%} threshold.\"\n",
    "                _record_anomaly(anomalies, label, column, \"null_ratio\", severity, description, generated_at)\n",
    "        table_result[\"column_null_ratios\"] = column_null_ratios\n",
    "\n",
    "    table_profiles.append(table_result)\n",
    "\n",
    "status = \"completed\" if profiling_successes else \"failed\"\n",
    "results = {\n",
    "    \"table_group_id\": table_group_id,\n",
    "    \"profile_run_id\": profile_run_id,\n",
    "    \"table_group_name\": group_details.get(\"name\"),\n",
    "    \"status\": status,\n",
    "    \"row_count\": int(total_rows),\n",
    "    \"anomaly_count\": len(anomalies),\n",
    "    \"anomalies\": anomalies,\n",
    "    \"generated_at\": generated_at,\n",
    "    \"table_profiles\": table_profiles,\n",
    "    \"diagnostics\": {\n",
    "        \"tables_requested\": len(table_rows),\n",
    "        \"tables_profiled\": profiling_successes,\n",
    "        \"tables_failed\": profiling_failures,\n",
    "        \"include_mask_applied\": bool(include_patterns),\n",
    "        \"exclude_mask_applied\": bool(exclude_patterns),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Profiling complete: {profiling_successes} succeeded, {profiling_failures} failed, total rows={total_rows}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload and call back into the API\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import socket\n",
    "from contextlib import suppress\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
    "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
    "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
    "\n",
    "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
    "\n",
    "\n",
    "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
    "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
    "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
    "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
    "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
    "\n",
    "\n",
    "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
    "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
    "    current = error\n",
    "    while current:\n",
    "        if isinstance(current, socket.gaierror):\n",
    "            return True\n",
    "        name = current.__class__.__name__.lower()\n",
    "        if \"nameresolution\" in name:\n",
    "            return True\n",
    "        message = str(current).lower()\n",
    "        if \"temporary failure in name resolution\" in message:\n",
    "            return True\n",
    "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
    "    return False\n",
    "\n",
    "\n",
    "def _rewrite_heroku_app_host(url: str | None) -> str | None:\n",
    "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.hostname\n",
    "    if not host:\n",
    "        return None\n",
    "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
    "    if not match:\n",
    "        return None\n",
    "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
    "    netloc = canonical_host\n",
    "    if parsed.port:\n",
    "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
    "    if parsed.username:\n",
    "        auth = parsed.username\n",
    "        if parsed.password:\n",
    "            auth = f\"{auth}:{parsed.password}\"\n",
    "        netloc = f\"{auth}@{netloc}\"\n",
    "    scheme = parsed.scheme or \"https\"\n",
    "    if scheme.lower() == \"http\":\n",
    "        scheme = \"https\"\n",
    "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))\n",
    "\n",
    "\n",
    "def _is_dbfs_path(path: str | None) -> bool:\n",
    "    return bool(path and path.lower().startswith(\"dbfs:/\"))\n",
    "\n",
    "\n",
    "def _has_uri_scheme(value: str | None) -> bool:\n",
    "    return bool(value and URI_SCHEME_PATTERN.match(value.strip()))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _dbfs_root_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/_dbfs_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DBFS_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _driver_fs_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/_driver_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DRIVER_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "def _warn_storage_disabled(message: str) -> None:\n",
    "    global _STORAGE_DISABLED_NOTICE_EMITTED\n",
    "    if not _STORAGE_DISABLED_NOTICE_EMITTED:\n",
    "        print(message)\n",
    "        _STORAGE_DISABLED_NOTICE_EMITTED = True\n",
    "\n",
    "\n",
    "def _redirect_dbfs_path(path: str) -> str | None:\n",
    "    global _DBFS_REDIRECT_NOTICE_EMITTED\n",
    "    if not _is_dbfs_path(path):\n",
    "        return path\n",
    "    if not _dbfs_root_is_disabled():\n",
    "        return path\n",
    "    if _driver_fs_is_disabled():\n",
    "        _warn_storage_disabled(\n",
    "            \"DBFS root access and driver filesystem writes are both disabled; payload artifacts will be skipped unless \"\n",
    "            \"a cloud storage payload_base_path is provided.\"\n",
    "        )\n",
    "        return None\n",
    "    if not _DBFS_REDIRECT_NOTICE_EMITTED:\n",
    "        print(\n",
    "            \"DBFS root access is disabled on this workspace; persisting profiling artifacts to the driver filesystem \"\n",
    "            \"instead.\"\n",
    "        )\n",
    "        _DBFS_REDIRECT_NOTICE_EMITTED = True\n",
    "    suffix = path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    redirected = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/{suffix}\" if suffix else DEFAULT_DRIVER_PAYLOAD_ROOT\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _mkdirs_if_supported(target_path: str) -> None:\n",
    "    lowered = target_path.lower()\n",
    "    if lowered.startswith(\"dbfs:/\") and _dbfs_root_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"file:/\") and _driver_fs_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"dbfs:/\") or lowered.startswith(\"file:/\"):\n",
    "        parent_dir = target_path.rsplit(\"/\", 1)[0]\n",
    "        dbutils.fs.mkdirs(parent_dir)\n",
    "\n",
    "\n",
    "def _ensure_https_base_url(value: str) -> str:\n",
    "    normalized = (value or \"\").strip()\n",
    "    if not normalized:\n",
    "        return normalized\n",
    "    parsed = urlparse(normalized)\n",
    "    if not parsed.scheme:\n",
    "        normalized = f\"https://{normalized.lstrip('/')}\"\n",
    "        parsed = urlparse(normalized)\n",
    "    if parsed.scheme.lower() == \"http\":\n",
    "        parsed = parsed._replace(scheme=\"https\")\n",
    "    normalized = urlunparse(parsed).rstrip(\"/\")\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _lookup_metadata_setting(setting_key: str) -> str | None:\n",
    "    normalized_key = (setting_key or \"\").strip().lower()\n",
    "    if not normalized_key:\n",
    "        return None\n",
    "    try:\n",
    "        settings_table = _metadata_table(\"dq_settings\")\n",
    "    except NameError:\n",
    "        return None\n",
    "    try:\n",
    "        row = (\n",
    "            spark.table(settings_table)\n",
    "            .where(F.lower(F.col(\"key\")) == normalized_key)\n",
    "            .select(\"value\")\n",
    "            .limit(1)\n",
    "            .collect()\n",
    "        )\n",
    "    except AnalysisException:\n",
    "        return None\n",
    "    if not row:\n",
    "        return None\n",
    "    value = row[0].get(\"value\")\n",
    "    return value.strip() if isinstance(value, str) and value.strip() else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_payload_storage_mode(value: str | None) -> str | None:\n",
    "    normalized = (value or \"\").strip().lower()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if normalized in {\"inline\", \"database\", \"db\"}:\n",
    "        return \"inline\"\n",
    "    if normalized in {\"artifact\", \"artifacts\", \"file\", \"files\", \"path\", \"paths\", \"dbfs\", \"cloud\"}:\n",
    "        return \"artifact\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_payload_storage_mode() -> str:\n",
    "    widget_choice = _normalize_payload_storage_mode(dbutils.widgets.get(\"payload_storage\"))\n",
    "    if widget_choice:\n",
    "        return widget_choice\n",
    "    setting_choice = _normalize_payload_storage_mode(_lookup_metadata_setting(\"profile_payload_storage_mode\"))\n",
    "    if setting_choice:\n",
    "        return setting_choice\n",
    "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
    "\n",
    "\n",
    "def _payload_storage_is_artifact(mode: str) -> bool:\n",
    "    return (mode or \"\").strip().lower() == \"artifact\"\n",
    "\n",
    "\n",
    "def _encode_payload_json(payload: dict[str, object]) -> str | None:\n",
    "    try:\n",
    "        return json.dumps(payload, separators=(\",\", \":\"))\n",
    "    except TypeError as exc:\n",
    "        print(f\"Unable to serialize profiling payload: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _resolve_callback_behavior() -> str:\n",
    "    widget_value = (dbutils.widgets.get(\"callback_behavior\") or \"\").strip().lower()\n",
    "    if widget_value:\n",
    "        return widget_value\n",
    "    setting_value = (_lookup_metadata_setting(\"profile_callback_behavior\") or \"\").strip().lower()\n",
    "    if setting_value:\n",
    "        return setting_value\n",
    "    return DEFAULT_CALLBACK_BEHAVIOR\n",
    "\n",
    "\n",
    "def _callbacks_enabled(behavior: str) -> bool:\n",
    "    if behavior in {\"api\", \"callback\", \"legacy\"}:\n",
    "        return True\n",
    "    if behavior in {\"metadata_only\", \"metadata\", \"skip\", \"disabled\", \"off\"}:\n",
    "        return False\n",
    "    print(f\"Unknown callback behavior '{behavior}'; defaulting to metadata_only.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sql_string_literal(value: str | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    escaped = str(value).replace(\"'\", \"''\")\n",
    "    return f\"'{escaped}'\"\n",
    "\n",
    "\n",
    "def _sql_numeric_literal(value: int | float | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        return str(int(value))\n",
    "    except (TypeError, ValueError):\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "def _normalize_temp_view_name(suffix: str | None) -> str:\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", (suffix or \"profile_run\"))\n",
    "    return f\"_profile_anomalies_{cleaned}\"\n",
    "\n",
    "\n",
    "def _parse_anomaly_timestamp(value: str | None) -> datetime | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    candidate = value.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.endswith(\"Z\"):\n",
    "        candidate = f\"{candidate[:-1]}+00:00\"\n",
    "    with suppress(ValueError):\n",
    "        return datetime.fromisoformat(candidate)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _persist_results_to_metadata(results_payload: dict[str, object], payload_location: str | None) -> None:\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id widget is required before persisting profiling metadata.\")\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "    anomalies_table = _metadata_table(\"dq_profile_anomalies\")\n",
    "    assignments = [\n",
    "        f\"status = {_sql_string_literal(results_payload.get('status') or 'unknown')}\",\n",
    "        \"completed_at = current_timestamp()\",\n",
    "        f\"row_count = {_sql_numeric_literal(results_payload.get('row_count'))}\",\n",
    "        f\"anomaly_count = {_sql_numeric_literal(results_payload.get('anomaly_count'))}\",\n",
    "        f\"payload_path = {_sql_string_literal(payload_location)}\",\n",
    "    ]\n",
    "    update_sql = (\n",
    "        f\"UPDATE {profiles_table} \"\n",
    "        f\"SET {', '.join(assignments)} \"\n",
    "        f\"WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    )\n",
    "    spark.sql(update_sql)\n",
    "    print(f\"Updated dq_profiles entry for run {profile_run_id}.\")\n",
    "\n",
    "    anomalies = list(results_payload.get(\"anomalies\") or [])\n",
    "    delete_sql = f\"DELETE FROM {anomalies_table} WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    spark.sql(delete_sql)\n",
    "\n",
    "    if not anomalies:\n",
    "        print(f\"No anomalies to persist for run {profile_run_id}.\")\n",
    "        return\n",
    "\n",
    "    anomaly_rows = []\n",
    "    for anomaly in anomalies:\n",
    "        anomaly_rows.append(\n",
    "            {\n",
    "                \"profile_run_id\": profile_run_id,\n",
    "                \"table_name\": anomaly.get(\"table_name\"),\n",
    "                \"column_name\": anomaly.get(\"column_name\"),\n",
    "                \"anomaly_type\": anomaly.get(\"anomaly_type\"),\n",
    "                \"severity\": anomaly.get(\"severity\"),\n",
    "                \"description\": anomaly.get(\"description\"),\n",
    "                \"detected_at\": _parse_anomaly_timestamp(anomaly.get(\"detected_at\")) or datetime.utcnow(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    anomalies_df = spark.createDataFrame(anomaly_rows)\n",
    "    view_name = _normalize_temp_view_name(profile_run_id)\n",
    "    try:\n",
    "        anomalies_df.createOrReplaceTempView(view_name)\n",
    "        spark.sql(\n",
    "            f\"INSERT INTO {anomalies_table} \"\n",
    "            \"(profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at) \"\n",
    "            f\"SELECT profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at FROM {view_name}\"\n",
    "        )\n",
    "    finally:\n",
    "        with suppress(Exception):\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "\n",
    "    print(f\"Persisted {len(anomalies)} anomalies for run {profile_run_id}.\")\n",
    "\n",
    "\n",
    "payload_base_path = payload_base_path or _lookup_metadata_setting(\"profile_payload_base_path\")\n",
    "callback_base_url = callback_base_url or _lookup_metadata_setting(\"profile_callback_base_url\")\n",
    "callback_token = callback_token or _lookup_metadata_setting(\"profile_callback_token\")\n",
    "\n",
    "\n",
    "def _normalize_payload_base(base_path: str | None) -> str | None:\n",
    "    raw_value = (base_path or \"\").strip()\n",
    "    if not raw_value:\n",
    "        redirected = _redirect_dbfs_path(DEFAULT_PRIVATE_PAYLOAD_ROOT)\n",
    "        if not redirected:\n",
    "            _warn_storage_disabled(\n",
    "                \"No writable default payload location detected; configure profile_payload_base_path to point to cloud \"\n",
    "                \"storage accessible from this workspace.\"\n",
    "            )\n",
    "        return redirected\n",
    "    if \"://\" in raw_value and not raw_value.lower().startswith(\"dbfs:/\"):\n",
    "        return raw_value.rstrip(\"/\")\n",
    "    if raw_value.lower().startswith(\"dbfs:/\"):\n",
    "        normalized = raw_value\n",
    "    elif raw_value.startswith(\"/\"):\n",
    "        normalized = f\"dbfs:{raw_value}\"\n",
    "    else:\n",
    "        normalized = f\"dbfs:/tmp/conversioncentral/{raw_value.lstrip('/')}\"\n",
    "    normalized = normalized.rstrip(\"/\")\n",
    "    if normalized.lower().startswith(\"dbfs:/filestore\"):\n",
    "        print(\"FileStore paths are disabled on this workspace; switching to private tmp storage.\")\n",
    "        normalized = DEFAULT_PRIVATE_PAYLOAD_ROOT\n",
    "    redirected = _redirect_dbfs_path(normalized)\n",
    "    if not redirected:\n",
    "        _warn_storage_disabled(\n",
    "            \"The configured payload base path resolves to a blocked filesystem; provide a supported cloud URI instead.\"\n",
    "        )\n",
    "    return redirected\n",
    "\n",
    "\n",
    "def _derive_payload_path(base_path: str | None, group_id: str, run_id: str) -> str | None:\n",
    "    normalized_base = _normalize_payload_base(base_path)\n",
    "    if not normalized_base:\n",
    "        return None\n",
    "    safe_group = (group_id or \"default\").replace(\":\", \"_\")\n",
    "    safe_run = (run_id or \"unknown\").replace(\":\", \"_\")\n",
    "    return f\"{normalized_base}/{safe_group}/{safe_run}.json\"\n",
    "\n",
    "\n",
    "def _normalize_payload_target(path: str | None, group_id: str, run_id: str, base_path: str | None) -> str | None:\n",
    "    candidate = (path or \"\").strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.lower().startswith(\"dbfs:/\"):\n",
    "        normalized = candidate\n",
    "    elif _has_uri_scheme(candidate):\n",
    "        normalized = candidate.rstrip(\"/\")\n",
    "    elif candidate.startswith(\"/\"):\n",
    "        normalized = f\"dbfs:{candidate}\"\n",
    "    else:\n",
    "        normalized = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/{candidate.lstrip('/')}\"\n",
    "    normalized = normalized.rstrip(\"/\")\n",
    "    lowered = normalized.lower()\n",
    "    if lowered.startswith(\"dbfs:/filestore\"):\n",
    "        print(\"FileStore paths are disabled on this workspace; switching to private tmp storage.\")\n",
    "        derived = _derive_payload_path(base_path, group_id, run_id)\n",
    "        normalized = derived or \"\"\n",
    "    redirected = _redirect_dbfs_path(normalized)\n",
    "    if not redirected:\n",
    "        return None\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _resolve_callback_target(base_url: str | None, run_id: str) -> str | None:\n",
    "    if not base_url:\n",
    "        return None\n",
    "    normalized = base_url.strip()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if \"{profile_run_id}\" in normalized:\n",
    "        try:\n",
    "            normalized = normalized.format(profile_run_id=run_id)\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "    normalized = _ensure_https_base_url(normalized)\n",
    "    if normalized.endswith(\"/complete\"):\n",
    "        return normalized\n",
    "    return f\"{normalized}/{run_id}/complete\"\n",
    "\n",
    "\n",
    "payload_storage_mode = _resolve_payload_storage_mode()\n",
    "payload_reference: str | None = None\n",
    "payload_json_value = _encode_payload_json(results)\n",
    "\n",
    "if _payload_storage_is_artifact(payload_storage_mode):\n",
    "    artifact_path = payload_path\n",
    "    payload_was_derived = False\n",
    "    if not artifact_path:\n",
    "        artifact_path = _derive_payload_path(payload_base_path, table_group_id, profile_run_id)\n",
    "        payload_was_derived = bool(artifact_path)\n",
    "\n",
    "    normalized_payload_path = _normalize_payload_target(artifact_path, table_group_id, profile_run_id, payload_base_path)\n",
    "    if normalized_payload_path:\n",
    "        if artifact_path and normalized_payload_path != artifact_path:\n",
    "            print(f\"Normalized payload path: {normalized_payload_path}\")\n",
    "        artifact_path = normalized_payload_path\n",
    "    elif not artifact_path:\n",
    "        print(\"Payload storage mode is 'artifact' but no valid path was supplied; inline fallback will be used.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Resolved payload path is blocked by workspace filesystem restrictions; skipping artifact export unless a \"\n",
    "            \"cloud path is provided.\"\n",
    "        )\n",
    "        artifact_path = None\n",
    "\n",
    "    if payload_was_derived and artifact_path:\n",
    "        print(f\"Derived payload artifact path: {artifact_path}\")\n",
    "\n",
    "    if artifact_path:\n",
    "        try:\n",
    "            _mkdirs_if_supported(artifact_path)\n",
    "            dbutils.fs.put(artifact_path, json.dumps(results, indent=2), overwrite=True)\n",
    "            print(f\"Wrote profiling payload to {artifact_path}\")\n",
    "            payload_reference = artifact_path\n",
    "        except Exception as exc:  # noqa: BLE001 - surface full failure for Databricks\n",
    "            print(f\"Failed to write profiling payload to {artifact_path}: {exc}\")\n",
    "            artifact_path = None\n",
    "            print(\"Falling back to inline payload storage.\")\n",
    "    else:\n",
    "        print(\"Skipping artifact export; inline payload will be stored instead.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Payload storage mode set to 'inline'; profiling results will be written directly to dq_profiles.payload_path.\"\n",
    "    )\n",
    "    artifact_path = None\n",
    "\n",
    "if artifact_path and not payload_reference:\n",
    "    payload_reference = artifact_path\n",
    "\n",
    "if not payload_reference:\n",
    "    if payload_json_value is None:\n",
    "        print(\"Unable to capture profiling payload; payload_path column will remain NULL.\")\n",
    "    else:\n",
    "        payload_reference = payload_json_value\n",
    "        print(\"Stored profiling payload inline with the profile metadata entry.\")\n",
    "\n",
    "_persist_results_to_metadata(results, payload_reference)\n",
    "\n",
    "\n",
    "callback_behavior = _resolve_callback_behavior()\n",
    "callback_source_url = callback_url or callback_base_url or _lookup_metadata_setting(\"profile_callback_base_url\")\n",
    "callback_target = _resolve_callback_target(callback_source_url, profile_run_id)\n",
    "if not _callbacks_enabled(callback_behavior):\n",
    "    print(\n",
    "        \"Skipping completion callback; profiling results were written directly to metadata tables. \"\n",
    "        \"Set callback_behavior='api' to re-enable HTTP callbacks.\"\n",
    "    )\n",
    "elif callback_target:\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    token_value = callback_token or _lookup_metadata_setting(\"profile_callback_token\")\n",
    "    if token_value:\n",
    "        headers[\"Authorization\"] = f\"Bearer {token_value}\"\n",
    "    callback_body = {\n",
    "        \"status\": results[\"status\"],\n",
    "        \"row_count\": results[\"row_count\"],\n",
    "        \"anomaly_count\": results[\"anomaly_count\"],\n",
    "        \"anomalies\": results[\"anomalies\"],\n",
    "    }\n",
    "    canonical_fallback = _rewrite_heroku_app_host(callback_target)\n",
    "    callback_candidates = [callback_target]\n",
    "    if canonical_fallback and canonical_fallback not in callback_candidates:\n",
    "        callback_candidates.append(canonical_fallback)\n",
    "    response = None\n",
    "    last_error: Exception | None = None\n",
    "    for idx, candidate in enumerate(callback_candidates):\n",
    "        try:\n",
    "            response = requests.post(candidate, headers=headers, json=callback_body, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Callback succeeded: {candidate} ({response.status_code})\")\n",
    "            break\n",
    "        except requests.exceptions.RequestException as exc:\n",
    "            last_error = exc\n",
    "            should_retry = idx == 0 and canonical_fallback and _looks_like_dns_failure(exc)\n",
    "            if should_retry:\n",
    "                print(\n",
    "                    f\"Callback host failed DNS lookup ({exc}); retrying canonical domain {canonical_fallback}.\"\n",
    "                )\n",
    "                continue\n",
    "            raise\n",
    "    if response is None:\n",
    "        raise last_error or RuntimeError(\"Callback failed without an HTTP response.\")\n",
    "else:\n",
    "    print(\"Callback URL not provided; skipping completion POST.\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}