{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"payload_storage\", \"\")\n",
    "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
    "payload_path = raw_payload_path or None\n",
    "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "from datetime import datetime\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from typing import Iterable\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, BinaryType, MapType, StructType\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "VALUE_DISTRIBUTION_LIMIT = 25\n",
    "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
    "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
    "MAX_VALUE_DISPLAY_LENGTH = 256\n",
    "\n",
    "PROFILE_COLUMN_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"qualified_name\",\n",
    "    \"data_type\",\n",
    "    \"general_type\",\n",
    "    \"ordinal_position\",\n",
    "    \"row_count\",\n",
    "    \"null_count\",\n",
    "    \"non_null_count\",\n",
    "    \"distinct_count\",\n",
    "    \"min_value\",\n",
    "    \"max_value\",\n",
    "    \"avg_value\",\n",
    "    \"stddev_value\",\n",
    "    \"median_value\",\n",
    "    \"p95_value\",\n",
    "    \"true_count\",\n",
    "    \"false_count\",\n",
    "    \"min_length\",\n",
    "    \"max_length\",\n",
    "    \"avg_length\",\n",
    "    \"non_ascii_ratio\",\n",
    "    \"min_date\",\n",
    "    \"max_date\",\n",
    "    \"date_span_days\",\n",
    "    \"metrics_json\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMNS_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"qualified_name\", T.StringType(), True),\n",
    "        T.StructField(\"data_type\", T.StringType(), True),\n",
    "        T.StructField(\"general_type\", T.StringType(), True),\n",
    "        T.StructField(\"ordinal_position\", T.IntegerType(), True),\n",
    "        T.StructField(\"row_count\", T.LongType(), True),\n",
    "        T.StructField(\"null_count\", T.LongType(), True),\n",
    "        T.StructField(\"non_null_count\", T.LongType(), True),\n",
    "        T.StructField(\"distinct_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_value\", T.StringType(), True),\n",
    "        T.StructField(\"max_value\", T.StringType(), True),\n",
    "        T.StructField(\"avg_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"stddev_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"median_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"p95_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"true_count\", T.LongType(), True),\n",
    "        T.StructField(\"false_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"max_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"avg_length\", T.DoubleType(), True),\n",
    "        T.StructField(\"non_ascii_ratio\", T.DoubleType(), True),\n",
    "        T.StructField(\"min_date\", T.DateType(), True),\n",
    "        T.StructField(\"max_date\", T.DateType(), True),\n",
    "        T.StructField(\"date_span_days\", T.IntegerType(), True),\n",
    "        T.StructField(\"metrics_json\", T.StringType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PROFILE_COLUMN_VALUES_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"value\",\n",
    "    \"value_hash\",\n",
    "    \"frequency\",\n",
    "    \"relative_freq\",\n",
    "    \"rank\",\n",
    "    \"bucket_label\",\n",
    "    \"bucket_lower_bound\",\n",
    "    \"bucket_upper_bound\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMN_VALUES_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"value\", T.StringType(), True),\n",
    "        T.StructField(\"value_hash\", T.StringType(), True),\n",
    "        T.StructField(\"frequency\", T.LongType(), True),\n",
    "        T.StructField(\"relative_freq\", T.DoubleType(), True),\n",
    "        T.StructField(\"rank\", T.IntegerType(), True),\n",
    "        T.StructField(\"bucket_label\", T.StringType(), True),\n",
    "        T.StructField(\"bucket_lower_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"bucket_upper_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PROFILE_ANOMALIES_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"anomaly_type\",\n",
    "    \"severity\",\n",
    "    \"description\",\n",
    "    \"detected_at\",\n",
    "]\n",
    "\n",
    "PROFILE_ANOMALIES_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"table_name\", T.StringType(), True),\n",
    "        T.StructField(\"column_name\", T.StringType(), True),\n",
    "        T.StructField(\"anomaly_type\", T.StringType(), True),\n",
    "        T.StructField(\"severity\", T.StringType(), True),\n",
    "        T.StructField(\"description\", T.StringType(), True),\n",
    "        T.StructField(\"detected_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def _split_identifier(value: str | None) -> list[str]:\n",
    "    cleaned = (value or \"\").replace(\"`\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    return [segment.strip() for segment in cleaned.split(\".\") if segment.strip()]\n",
    "\n",
    "\n",
    "def _catalog_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload and call back into the API\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import socket\n",
    "from contextlib import suppress\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
    "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
    "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
    "\n",
    "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
    "\n",
    "\n",
    "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
    "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
    "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
    "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
    "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
    "\n",
    "\n",
    "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
    "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
    "    current = error\n",
    "    while current:\n",
    "        if isinstance(current, socket.gaierror):\n",
    "            return True\n",
    "        name = current.__class__.__name__.lower()\n",
    "        if \"nameresolution\" in name:\n",
    "            return True\n",
    "        message = str(current).lower()\n",
    "        if \"temporary failure in name resolution\" in message:\n",
    "            return True\n",
    "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
    "    return False\n",
    "\n",
    "\n",
    "def _rewrite_heroku_app_host(url: str | None) -> str | None:\n",
    "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.hostname\n",
    "    if not host:\n",
    "        return None\n",
    "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
    "    if not match:\n",
    "        return None\n",
    "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
    "    netloc = canonical_host\n",
    "    if parsed.port:\n",
    "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
    "    if parsed.username:\n",
    "        auth = parsed.username\n",
    "        if parsed.password:\n",
    "            auth = f\"{auth}:{parsed.password}\"\n",
    "        netloc = f\"{auth}@{netloc}\"\n",
    "    scheme = parsed.scheme or \"https\"\n",
    "    if scheme.lower() == \"http\":\n",
    "        scheme = \"https\"\n",
    "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))\n",
    "\n",
    "\n",
    "def _is_dbfs_path(path: str | None) -> bool:\n",
    "    return bool(path and path.lower().startswith(\"dbfs:/\"))\n",
    "\n",
    "\n",
    "def _has_uri_scheme(value: str | None) -> bool:\n",
    "    return bool(value and URI_SCHEME_PATTERN.match(value.strip()))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _dbfs_root_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/_dbfs_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DBFS_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _driver_fs_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/_driver_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DRIVER_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "def _warn_storage_disabled(message: str) -> None:\n",
    "    global _STORAGE_DISABLED_NOTICE_EMITTED\n",
    "    if not _STORAGE_DISABLED_NOTICE_EMITTED:\n",
    "        print(message)\n",
    "        _STORAGE_DISABLED_NOTICE_EMITTED = True\n",
    "\n",
    "\n",
    "def _redirect_dbfs_path(path: str) -> str | None:\n",
    "    global _DBFS_REDIRECT_NOTICE_EMITTED\n",
    "    if not _is_dbfs_path(path):\n",
    "        return path\n",
    "    if not _dbfs_root_is_disabled():\n",
    "        return path\n",
    "    if _driver_fs_is_disabled():\n",
    "        _warn_storage_disabled(\n",
    "            \"DBFS root access and driver filesystem writes are both disabled; payload artifacts will be skipped unless \"\n",
    "            \"a cloud storage payload_base_path is provided.\"\n",
    "        )\n",
    "        return None\n",
    "    if not _DBFS_REDIRECT_NOTICE_EMITTED:\n",
    "        print(\n",
    "            \"DBFS root access is disabled on this workspace; persisting profiling artifacts to the driver filesystem \"\n",
    "            \"instead.\"\n",
    "        )\n",
    "        _DBFS_REDIRECT_NOTICE_EMITTED = True\n",
    "    suffix = path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    redirected = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/{suffix}\" if suffix else DEFAULT_DRIVER_PAYLOAD_ROOT\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _mkdirs_if_supported(target_path: str) -> None:\n",
    "    lowered = target_path.lower()\n",
    "    if lowered.startswith(\"dbfs:/\") and _dbfs_root_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"file:/\") and _driver_fs_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"dbfs:/\") or lowered.startswith(\"file:/\"):\n",
    "        parent_dir = target_path.rsplit(\"/\", 1)[0]\n",
    "        dbutils.fs.mkdirs(parent_dir)\n",
    "\n",
    "\n",
    "def _ensure_https_base_url(value: str) -> str:\n",
    "    normalized = (value or \"\").strip()\n",
    "    if not normalized:\n",
    "        return normalized\n",
    "    parsed = urlparse(normalized)\n",
    "    if not parsed.scheme:\n",
    "        normalized = f\"https://{normalized.lstrip('/')}\"\n",
    "        parsed = urlparse(normalized)\n",
    "    if parsed.scheme.lower() == \"http\":\n",
    "        parsed = parsed._replace(scheme=\"https\")\n",
    "    normalized = urlunparse(parsed).rstrip(\"/\")\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _lookup_metadata_setting(setting_key: str) -> str | None:\n",
    "    normalized_key = (setting_key or \"\").strip().lower()\n",
    "    if not normalized_key:\n",
    "        return None\n",
    "    try:\n",
    "        settings_table = _metadata_table(\"dq_settings\")\n",
    "    except NameError:\n",
    "        return None\n",
    "    try:\n",
    "        row = (\n",
    "            spark.table(settings_table)\n",
    "            .where(F.lower(F.col(\"key\")) == normalized_key)\n",
    "            .select(\"value\")\n",
    "            .limit(1)\n",
    "            .collect()\n",
    "        )\n",
    "    except AnalysisException:\n",
    "        return None\n",
    "    if not row:\n",
    "        return None\n",
    "    value = row[0].get(\"value\")\n",
    "    return value.strip() if isinstance(value, str) and value.strip() else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_payload_storage_mode(value: str | None) -> str | None:\n",
    "    normalized = (value or \"\").strip().lower()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if normalized in {\"inline\", \"database\", \"db\"}:\n",
    "        return \"inline\"\n",
    "    if normalized in {\"artifact\", \"artifacts\", \"file\", \"files\", \"path\", \"paths\", \"dbfs\", \"cloud\"}:\n",
    "        return \"artifact\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_payload_storage_mode() -> str:\n",
    "    widget_choice = _normalize_payload_storage_mode(dbutils.widgets.get(\"payload_storage\"))\n",
    "    if widget_choice:\n",
    "        return widget_choice\n",
    "    setting_choice = _normalize_payload_storage_mode(_lookup_metadata_setting(\"profile_payload_storage_mode\"))\n",
    "    if setting_choice:\n",
    "        return setting_choice\n",
    "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
    "\n",
    "\n",
    "def _payload_storage_is_artifact(mode: str) -> bool:\n",
    "    return (mode or \"\").strip().lower() == \"artifact\"\n",
    "\n",
    "\n",
    "def _encode_payload_json(payload: dict[str, object]) -> str | None:\n",
    "    try:\n",
    "        return json.dumps(payload, separators=(\",\", \":\"))\n",
    "    except TypeError as exc:\n",
    "        print(f\"Unable to serialize profiling payload: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _resolve_callback_behavior() -> str:\n",
    "    widget_value = (dbutils.widgets.get(\"callback_behavior\") or \"\").strip().lower()\n",
    "    if widget_value:\n",
    "        return widget_value\n",
    "    setting_value = (_lookup_metadata_setting(\"profile_callback_behavior\") or \"\").strip().lower()\n",
    "    if setting_value:\n",
    "        return setting_value\n",
    "    return DEFAULT_CALLBACK_BEHAVIOR\n",
    "\n",
    "\n",
    "def _callbacks_enabled(behavior: str) -> bool:\n",
    "    if behavior in {\"api\", \"callback\", \"legacy\"}:\n",
    "        return True\n",
    "    if behavior in {\"metadata_only\", \"metadata\", \"skip\", \"disabled\", \"off\"}:\n",
    "        return False\n",
    "    print(f\"Unknown callback behavior '{behavior}'; defaulting to metadata_only.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sql_string_literal(value: str | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    escaped = str(value).replace(\"'\", \"''\")\n",
    "    return f\"'{escaped}'\"\n",
    "\n",
    "\n",
    "def _sql_numeric_literal(value: int | float | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        return str(int(value))\n",
    "    except (TypeError, ValueError):\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "def _normalize_temp_view_name(suffix: str | None) -> str:\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", (suffix or \"profile_run\"))\n",
    "    return f\"_profile_anomalies_{cleaned}\"\n",
    "\n",
    "\n",
    "def _parse_anomaly_timestamp(value: str | None) -> datetime | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    candidate = value.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.endswith(\"Z\"):\n",
    "        candidate = f\"{candidate[:-1]}+00:00\"\n",
    "    with suppress(ValueError):\n",
    "        return datetime.fromisoformat(candidate)\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column/value persistence helpers and overrides\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "def _escape_identifier(identifier: str) -> str:\n",
    "    cleaned = (identifier or \"\").strip().replace(\"`\", \"\")\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"Metadata identifiers cannot be empty.\")\n",
    "    return f\"`{cleaned}`\"\n",
    "\n",
    "def _metadata_schema_reference() -> str:\n",
    "    if not dq_schema:\n",
    "        raise ValueError(\"data_quality_schema widget must be set before resolving metadata tables.\")\n",
    "    catalog = (connection_catalog or \"\").strip()\n",
    "    if catalog:\n",
    "        return f\"{_escape_identifier(catalog)}.{_escape_identifier(dq_schema)}\"\n",
    "    return _escape_identifier(dq_schema)\n",
    "\n",
    "def _metadata_table(table_name: str) -> str:\n",
    "    return f\"{_metadata_schema_reference()}.{_escape_identifier(table_name)}\"\n",
    "\n",
    "def _first_non_empty(*values):\n",
    "    for value in values:\n",
    "        if isinstance(value, str):\n",
    "            candidate = value.strip()\n",
    "            if candidate:\n",
    "                return candidate\n",
    "        elif value is not None:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def _coerce_int(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bool):\n",
    "        return int(value)\n",
    "    if isinstance(value, int):\n",
    "        return value\n",
    "    if isinstance(value, float):\n",
    "        if not math.isfinite(value):\n",
    "            return None\n",
    "        return int(round(value))\n",
    "    if isinstance(value, str):\n",
    "        candidate = value.strip().replace(\",\", \"\")\n",
    "        if not candidate:\n",
    "            return None\n",
    "        try:\n",
    "            if \".\" in candidate:\n",
    "                return int(float(candidate))\n",
    "            return int(candidate)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _coerce_float(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bool):\n",
    "        return float(value)\n",
    "    if isinstance(value, (int, float)):\n",
    "        numeric = float(value)\n",
    "        if math.isfinite(numeric):\n",
    "            return numeric\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        candidate = value.strip().replace(\",\", \"\")\n",
    "        if not candidate:\n",
    "            return None\n",
    "        try:\n",
    "            numeric = float(candidate)\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return numeric if math.isfinite(numeric) else None\n",
    "    return None\n",
    "\n",
    "def _coerce_date(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, dt.date):\n",
    "        return value\n",
    "    if isinstance(value, datetime):\n",
    "        return value.date()\n",
    "    if isinstance(value, str):\n",
    "        candidate = value.strip()\n",
    "        if not candidate:\n",
    "            return None\n",
    "        normalized = f\"{candidate[:-1]}+00:00\" if candidate.endswith(\"Z\") else candidate\n",
    "        try:\n",
    "            parsed = datetime.fromisoformat(normalized)\n",
    "            return parsed.date()\n",
    "        except ValueError:\n",
    "            pass\n",
    "        with suppress(ValueError):\n",
    "            return datetime.strptime(candidate, \"%Y-%m-%d\").date()\n",
    "    return None\n",
    "\n",
    "def _stringify_value(value, limit: int | None = MAX_VALUE_DISPLAY_LENGTH):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bytes):\n",
    "        candidate = value.decode(\"utf-8\", errors=\"replace\")\n",
    "    elif isinstance(value, (datetime, dt.date)):\n",
    "        candidate = value.isoformat()\n",
    "    else:\n",
    "        candidate = str(value)\n",
    "    candidate = candidate.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if limit is not None and len(candidate) > limit:\n",
    "        return candidate[:limit]\n",
    "    return candidate\n",
    "\n",
    "def _hash_value(value: str | None) -> str | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    return hashlib.sha1(value.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _collect_metrics(column_entry: dict[str, object]) -> dict[str, object]:\n",
    "    metrics: dict[str, object] = {}\n",
    "    for key in (\"metrics\", \"summary\"):\n",
    "        nested = column_entry.get(key)\n",
    "        if isinstance(nested, dict):\n",
    "            for nested_key, nested_value in nested.items():\n",
    "                if nested_value is None or nested_key in metrics:\n",
    "                    continue\n",
    "                metrics[nested_key] = nested_value\n",
    "    return metrics\n",
    "\n",
    "def _metric_lookup(column_entry: dict[str, object], metrics_map: dict[str, object], *keys):\n",
    "    for key in keys:\n",
    "        if key in metrics_map and metrics_map[key] is not None:\n",
    "            return metrics_map[key]\n",
    "        value = column_entry.get(key)\n",
    "        if value is not None:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def _infer_general_type(data_type: str | None) -> str | None:\n",
    "    if not data_type:\n",
    "        return None\n",
    "    lowered = data_type.lower()\n",
    "    if any(token in lowered for token in (\"int\", \"decimal\", \"number\", \"double\", \"float\")):\n",
    "        return \"N\"\n",
    "    if any(token in lowered for token in (\"date\", \"time\", \"timestamp\")):\n",
    "        return \"D\"\n",
    "    if \"bool\" in lowered or \"bit\" in lowered:\n",
    "        return \"B\"\n",
    "    if any(token in lowered for token in (\"char\", \"string\", \"text\")):\n",
    "        return \"A\"\n",
    "    return \"X\"\n",
    "\n",
    "def _encode_metrics_blob(metrics_map: dict[str, object]) -> str | None:\n",
    "    if not metrics_map:\n",
    "        return None\n",
    "    normalized: dict[str, object] = {}\n",
    "    for key, value in metrics_map.items():\n",
    "        if isinstance(value, (datetime, dt.date)):\n",
    "            normalized[key] = value.isoformat()\n",
    "        elif isinstance(value, (int, float, bool)) or value is None:\n",
    "            normalized[key] = value\n",
    "        else:\n",
    "            normalized[key] = _stringify_value(value, limit=None)\n",
    "    try:\n",
    "        return json.dumps(normalized, separators=(\",\", \":\"))\n",
    "    except TypeError:\n",
    "        return None\n",
    "\n",
    "def _extract_column_name(column_entry: dict[str, object]) -> str | None:\n",
    "    return _first_non_empty(\n",
    "        column_entry.get(\"column_name\"),\n",
    "        column_entry.get(\"column\"),\n",
    "        column_entry.get(\"name\"),\n",
    "        column_entry.get(\"columnName\"),\n",
    "    )\n",
    "\n",
    "def _extract_table_entries(results_payload: dict[str, object] | list) -> list[dict[str, object]]:\n",
    "    if isinstance(results_payload, dict):\n",
    "        for key in (\"tables\", \"table_profiles\", \"tablesProfiled\"):\n",
    "            candidate = results_payload.get(key)\n",
    "            if isinstance(candidate, list):\n",
    "                return [entry for entry in candidate if isinstance(entry, dict)]\n",
    "        single = results_payload.get(\"table\")\n",
    "        if isinstance(single, dict):\n",
    "            return [single]\n",
    "    if isinstance(results_payload, list):\n",
    "        return [entry for entry in results_payload if isinstance(entry, dict)]\n",
    "    return []\n",
    "\n",
    "def _collect_column_entries(table_entry: dict[str, object]) -> list[dict[str, object]]:\n",
    "    for key in (\"columns\", \"column_profiles\", \"columnsProfiled\"):\n",
    "        value = table_entry.get(key)\n",
    "        if isinstance(value, list):\n",
    "            return [entry for entry in value if isinstance(entry, dict)]\n",
    "        if isinstance(value, dict):\n",
    "            return [entry for entry in value.values() if isinstance(entry, dict)]\n",
    "    return []\n",
    "\n",
    "def _extract_table_context(table_entry: dict[str, object]) -> dict[str, object]:\n",
    "    schema_name = _first_non_empty(\n",
    "        table_entry.get(\"schema_name\"),\n",
    "        table_entry.get(\"schema\"),\n",
    "        table_entry.get(\"schemaName\"),\n",
    "        connection_schema,\n",
    "    )\n",
    "    table_name = _first_non_empty(\n",
    "        table_entry.get(\"table_name\"),\n",
    "        table_entry.get(\"table\"),\n",
    "        table_entry.get(\"name\"),\n",
    "        table_entry.get(\"tableName\"),\n",
    "        table_entry.get(\"physical_name\"),\n",
    "        table_entry.get(\"physicalName\"),\n",
    "    )\n",
    "    qualified_name = _first_non_empty(\n",
    "        table_entry.get(\"qualified_name\"),\n",
    "        table_entry.get(\"qualifiedName\"),\n",
    "        table_entry.get(\"physical_name\"),\n",
    "        table_entry.get(\"physicalName\"),\n",
    "    )\n",
    "    if not qualified_name and schema_name and table_name:\n",
    "        qualified_name = f\"{schema_name}.{table_name}\"\n",
    "    return {\n",
    "        \"schema_name\": schema_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"qualified_name\": qualified_name,\n",
    "    }\n",
    "\n",
    "def _build_column_row(\n",
    "    context: dict[str, object],\n",
    "    column_entry: dict[str, object],\n",
    "    generated_at: datetime,\n",
    "    ordinal_fallback: int,\n",
    " ):\n",
    "    column_name = _extract_column_name(column_entry)\n",
    "    table_name = context.get(\"table_name\")\n",
    "    if not column_name or not table_name:\n",
    "        return None\n",
    "\n",
    "    schema_name = context.get(\"schema_name\") or connection_schema or dq_schema\n",
    "    metrics_map = _collect_metrics(column_entry)\n",
    "\n",
    "    row_count = _coerce_int(_metric_lookup(column_entry, metrics_map, \"row_count\", \"rows\", \"total_rows\"))\n",
    "    null_count = _coerce_int(_metric_lookup(column_entry, metrics_map, \"null_count\"))\n",
    "    non_null_count = _coerce_int(\n",
    "        _metric_lookup(column_entry, metrics_map, \"non_null_count\", \"nonnull_count\", \"valid_count\")\n",
    "    )\n",
    "    if non_null_count is None and row_count is not None and null_count is not None:\n",
    "        non_null_count = max(row_count - null_count, 0)\n",
    "    distinct_count = _coerce_int(_metric_lookup(column_entry, metrics_map, \"distinct_count\", \"cardinality\"))\n",
    "    min_value = _stringify_value(_metric_lookup(column_entry, metrics_map, \"min_value\", \"min\"))\n",
    "    max_value = _stringify_value(_metric_lookup(column_entry, metrics_map, \"max_value\", \"max\"))\n",
    "    avg_value = _coerce_float(_metric_lookup(column_entry, metrics_map, \"avg_value\", \"avg\", \"mean\"))\n",
    "    stddev_value = _coerce_float(_metric_lookup(column_entry, metrics_map, \"stddev_value\", \"std_dev\", \"stddev\"))\n",
    "    median_value = _coerce_float(_metric_lookup(column_entry, metrics_map, \"median_value\", \"median\", \"p50\"))\n",
    "    p95_value = _coerce_float(_metric_lookup(column_entry, metrics_map, \"p95_value\", \"p95\"))\n",
    "    true_count = _coerce_int(_metric_lookup(column_entry, metrics_map, \"true_count\", \"trues\"))\n",
    "    false_count = _coerce_int(_metric_lookup(column_entry, metrics_map, \"false_count\", \"falses\"))\n",
    "    min_length = _coerce_int(_metric_lookup(column_entry, metrics_map, \"min_length\", \"length_min\"))\n",
    "    max_length = _coerce_int(_metric_lookup(column_entry, metrics_map, \"max_length\", \"length_max\"))\n",
    "    avg_length = _coerce_float(_metric_lookup(column_entry, metrics_map, \"avg_length\", \"length_avg\"))\n",
    "    non_ascii_ratio = _coerce_float(_metric_lookup(column_entry, metrics_map, \"non_ascii_ratio\", \"nonAsciiPercent\"))\n",
    "    min_date = _coerce_date(_metric_lookup(column_entry, metrics_map, \"min_date\", \"minDate\"))\n",
    "    max_date = _coerce_date(_metric_lookup(column_entry, metrics_map, \"max_date\", \"maxDate\"))\n",
    "    date_span_days = _coerce_int(_metric_lookup(column_entry, metrics_map, \"date_span_days\", \"dateSpanDays\"))\n",
    "    if date_span_days is None and min_date and max_date:\n",
    "        date_span_days = (max_date - min_date).days\n",
    "\n",
    "    data_type = _first_non_empty(\n",
    "        column_entry.get(\"data_type\"),\n",
    "        column_entry.get(\"type\"),\n",
    "        column_entry.get(\"dataType\"),\n",
    "    )\n",
    "    general_type = _first_non_empty(\n",
    "        column_entry.get(\"general_type\"),\n",
    "        column_entry.get(\"type_category\"),\n",
    "        column_entry.get(\"typeCategory\"),\n",
    "        _infer_general_type(data_type),\n",
    "    )\n",
    "    qualified_name = _first_non_empty(\n",
    "        column_entry.get(\"qualified_name\"),\n",
    "        column_entry.get(\"qualifiedName\"),\n",
    "        context.get(\"qualified_name\"),\n",
    "    )\n",
    "    ordinal_position = _coerce_int(\n",
    "        _metric_lookup(column_entry, metrics_map, \"ordinal_position\", \"position\", \"index\", \"ordinal\")\n",
    "    )\n",
    "    if ordinal_position is None:\n",
    "        ordinal_position = ordinal_fallback\n",
    "\n",
    "    generated_at_value = column_entry.get(\"generated_at\")\n",
    "    row_generated_at = (\n",
    "        generated_at_value\n",
    "        if isinstance(generated_at_value, datetime)\n",
    "        else _parse_anomaly_timestamp(generated_at_value)\n",
    "    ) or generated_at\n",
    "\n",
    "    metrics_json = column_entry.get(\"metrics_json\") or _encode_metrics_blob(metrics_map)\n",
    "\n",
    "    return {\n",
    "        \"profile_run_id\": profile_run_id,\n",
    "        \"schema_name\": schema_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"column_name\": column_name,\n",
    "        \"qualified_name\": qualified_name,\n",
    "        \"data_type\": data_type,\n",
    "        \"general_type\": general_type,\n",
    "        \"ordinal_position\": ordinal_position,\n",
    "        \"row_count\": row_count,\n",
    "        \"null_count\": null_count,\n",
    "        \"non_null_count\": non_null_count,\n",
    "        \"distinct_count\": distinct_count,\n",
    "        \"min_value\": min_value,\n",
    "        \"max_value\": max_value,\n",
    "        \"avg_value\": avg_value,\n",
    "        \"stddev_value\": stddev_value,\n",
    "        \"median_value\": median_value,\n",
    "        \"p95_value\": p95_value,\n",
    "        \"true_count\": true_count,\n",
    "        \"false_count\": false_count,\n",
    "        \"min_length\": min_length,\n",
    "        \"max_length\": max_length,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"non_ascii_ratio\": non_ascii_ratio,\n",
    "        \"min_date\": min_date,\n",
    "        \"max_date\": max_date,\n",
    "        \"date_span_days\": date_span_days,\n",
    "        \"metrics_json\": metrics_json,\n",
    "        \"generated_at\": row_generated_at,\n",
    "    }\n",
    "\n",
    "def _normalize_ratio(value):\n",
    "    numeric = _coerce_float(value)\n",
    "    if numeric is None:\n",
    "        return None\n",
    "    if numeric > 1.0:\n",
    "        return numeric / 100.0\n",
    "    return numeric\n",
    "\n",
    "def _extract_top_values(column_entry: dict[str, object]) -> list[dict[str, object]]:\n",
    "    for key in (\"top_values\", \"frequencies\", \"topValues\"):\n",
    "        value = column_entry.get(key)\n",
    "        if isinstance(value, list):\n",
    "            return [entry for entry in value if isinstance(entry, dict)]\n",
    "    return []\n",
    "\n",
    "def _extract_histogram_entries(column_entry: dict[str, object]) -> list[dict[str, object]]:\n",
    "    for key in (\"histogram\", \"bins\"):\n",
    "        value = column_entry.get(key)\n",
    "        if isinstance(value, list):\n",
    "            return [entry for entry in value if isinstance(entry, dict)]\n",
    "    return []\n",
    "\n",
    "def _extract_bucket_bound(item: dict[str, object], *keys):\n",
    "    for key in keys:\n",
    "        bound = _coerce_float(item.get(key))\n",
    "        if bound is not None:\n",
    "            return bound\n",
    "    return None\n",
    "\n",
    "def _build_value_rows(\n",
    "    context: dict[str, object],\n",
    "    column_entry: dict[str, object],\n",
    "    generated_at: datetime,\n",
    "    row_count: int | None,\n",
    " ):\n",
    "    column_name = _extract_column_name(column_entry)\n",
    "    table_name = context.get(\"table_name\")\n",
    "    if not column_name or not table_name:\n",
    "        return []\n",
    "\n",
    "    schema_name = context.get(\"schema_name\") or connection_schema or dq_schema\n",
    "    rows: list[dict[str, object]] = []\n",
    "\n",
    "    top_values = _extract_top_values(column_entry)[:VALUE_DISTRIBUTION_LIMIT]\n",
    "    for idx, item in enumerate(top_values, start=1):\n",
    "        value_text = _stringify_value(item.get(\"value\") or item.get(\"label\"))\n",
    "        frequency = _coerce_int(item.get(\"frequency\") or item.get(\"count\") or item.get(\"value_count\"))\n",
    "        rank = _coerce_int(item.get(\"rank\")) or idx\n",
    "        relative_freq = _normalize_ratio(\n",
    "            item.get(\"relative_freq\") or item.get(\"ratio\") or item.get(\"percentage\") or item.get(\"percent\")\n",
    "        )\n",
    "        if relative_freq is None and row_count and row_count > 0 and frequency is not None:\n",
    "            relative_freq = frequency / float(row_count)\n",
    "        if value_text is None and frequency is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"profile_run_id\": profile_run_id,\n",
    "            \"schema_name\": schema_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"value\": value_text,\n",
    "            \"value_hash\": _hash_value(value_text),\n",
    "            \"frequency\": frequency,\n",
    "            \"relative_freq\": relative_freq,\n",
    "            \"rank\": rank,\n",
    "            \"bucket_label\": None,\n",
    "            \"bucket_lower_bound\": None,\n",
    "            \"bucket_upper_bound\": None,\n",
    "            \"generated_at\": generated_at,\n",
    "        })\n",
    "\n",
    "    histogram_entries = _extract_histogram_entries(column_entry)\n",
    "    for item in histogram_entries:\n",
    "        bucket_label = _stringify_value(\n",
    "            _first_non_empty(\n",
    "                item.get(\"bucket_label\"),\n",
    "                item.get(\"bucketLabel\"),\n",
    "                item.get(\"label\"),\n",
    "                item.get(\"range\"),\n",
    "            )\n",
    "        )\n",
    "        value_text = _stringify_value(item.get(\"value\") or bucket_label)\n",
    "        frequency = _coerce_int(item.get(\"frequency\") or item.get(\"count\"))\n",
    "        relative_freq = _normalize_ratio(\n",
    "            item.get(\"relative_freq\") or item.get(\"ratio\") or item.get(\"percentage\") or item.get(\"percent\")\n",
    "        )\n",
    "        if relative_freq is None and row_count and row_count > 0 and frequency is not None:\n",
    "            relative_freq = frequency / float(row_count)\n",
    "        if bucket_label is None and value_text is None and frequency is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"profile_run_id\": profile_run_id,\n",
    "            \"schema_name\": schema_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"value\": value_text,\n",
    "            \"value_hash\": _hash_value(value_text or bucket_label),\n",
    "            \"frequency\": frequency,\n",
    "            \"relative_freq\": relative_freq,\n",
    "            \"rank\": None,\n",
    "            \"bucket_label\": bucket_label,\n",
    "            \"bucket_lower_bound\": _extract_bucket_bound(item, \"bucket_lower_bound\", \"lower\", \"min\"),\n",
    "            \"bucket_upper_bound\": _extract_bucket_bound(item, \"bucket_upper_bound\", \"upper\", \"max\"),\n",
    "            \"generated_at\": generated_at,\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "def _build_profile_detail_rows(results_payload: dict[str, object] | list):\n",
    "    tables = _extract_table_entries(results_payload)\n",
    "    if not tables:\n",
    "        return [], []\n",
    "\n",
    "    generated_at = datetime.utcnow()\n",
    "    column_rows: list[dict[str, object]] = []\n",
    "    value_rows: list[dict[str, object]] = []\n",
    "\n",
    "    for table_entry in tables:\n",
    "        context = _extract_table_context(table_entry)\n",
    "        if not context.get(\"table_name\"):\n",
    "            continue\n",
    "        columns = _collect_column_entries(table_entry)\n",
    "        if not columns:\n",
    "            continue\n",
    "        for idx, column_entry in enumerate(columns, start=1):\n",
    "            column_row = _build_column_row(context, column_entry, generated_at, ordinal_fallback=idx)\n",
    "            if not column_row:\n",
    "                continue\n",
    "            column_rows.append(column_row)\n",
    "            value_rows.extend(_build_value_rows(context, column_entry, generated_at, column_row.get(\"row_count\")))\n",
    "\n",
    "    return column_rows, value_rows\n",
    "\n",
    "def _normalize_temp_view_name(suffix: str | None, *, prefix: str = \"profile_anomalies\") -> str:\n",
    "    cleaned_suffix = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", (suffix or \"profile_run\"))\n",
    "    cleaned_prefix = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", prefix)\n",
    "    return f\"_{cleaned_prefix}_{cleaned_suffix}\"\n",
    "\n",
    "def _coerce_payload_structure(payload_candidate: object | None) -> dict[str, object] | list | None:\n",
    "    if isinstance(payload_candidate, (dict, list)):\n",
    "        return payload_candidate\n",
    "    if isinstance(payload_candidate, (bytes, bytearray)):\n",
    "        payload_candidate = payload_candidate.decode(\"utf-8\", errors=\"replace\")\n",
    "    if isinstance(payload_candidate, str):\n",
    "        candidate = payload_candidate.strip()\n",
    "        if candidate:\n",
    "            with suppress(json.JSONDecodeError):\n",
    "                decoded = json.loads(candidate)\n",
    "                if isinstance(decoded, (dict, list)):\n",
    "                    return decoded\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_payload_from_location(payload_location: str | None) -> dict[str, object] | list | None:\n",
    "    trimmed = (payload_location or \"\").strip()\n",
    "    if not trimmed:\n",
    "        return None\n",
    "    try:\n",
    "        if trimmed.startswith(\"/dbfs/\"):\n",
    "            with open(trimmed, \"r\", encoding=\"utf-8\") as handle:\n",
    "                return _coerce_payload_structure(handle.read())\n",
    "        if trimmed.startswith(\"dbfs:/\") or trimmed.startswith(\"file:/\") or trimmed.startswith(\"abfss:/\") or trimmed.startswith(\"s3:/\"):\n",
    "            with dbutils.fs.open(trimmed, \"r\") as handle:\n",
    "                return _coerce_payload_structure(handle.read())\n",
    "        if _has_uri_scheme(trimmed):\n",
    "            response = requests.get(trimmed, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return _coerce_payload_structure(response.text)\n",
    "    except Exception as exc:\n",
    "        print(f\"Unable to load profiling payload from {trimmed}: {exc}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _persist_profile_detail_tables(results_payload: dict[str, object] | list | None, payload_location: str | None = None) -> None:\n",
    "    payload = _coerce_payload_structure(results_payload)\n",
    "    if payload is None:\n",
    "        payload = _load_payload_from_location(payload_location)\n",
    "    if payload is None:\n",
    "        print(\"No profiling payload available for detail persistence; skipping column/value inserts.\")\n",
    "        return\n",
    "    columns_table = _metadata_table(\"dq_profile_columns\")\n",
    "    values_table = _metadata_table(\"dq_profile_column_values\")\n",
    "\n",
    "    column_rows, value_rows = _build_profile_detail_rows(payload)\n",
    "    delete_clause = f\"WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "\n",
    "    for target_table in (columns_table, values_table):\n",
    "        try:\n",
    "            spark.sql(f\"DELETE FROM {target_table} {delete_clause}\")\n",
    "        except AnalysisException as exc:\n",
    "            print(f\"Unable to delete existing rows from {target_table}: {exc}\")\n",
    "\n",
    "    if column_rows:\n",
    "        columns_df = spark.createDataFrame(column_rows, PROFILE_COLUMNS_SCHEMA).select(*PROFILE_COLUMN_FIELDS)\n",
    "        view_name = _normalize_temp_view_name(profile_run_id, prefix=\"profile_columns\")\n",
    "        try:\n",
    "            columns_df.createOrReplaceTempView(view_name)\n",
    "            select_list = \", \".join(PROFILE_COLUMN_FIELDS)\n",
    "            spark.sql(\n",
    "                f\"INSERT INTO {columns_table} ({select_list}) SELECT {select_list} FROM {view_name}\"\n",
    "            )\n",
    "            print(f\"Persisted {len(column_rows)} column metrics for run {profile_run_id}.\")\n",
    "        finally:\n",
    "            with suppress(Exception):\n",
    "                spark.catalog.dropTempView(view_name)\n",
    "    else:\n",
    "        print(f\"No column metrics extracted for run {profile_run_id}.\")\n",
    "\n",
    "    if value_rows:\n",
    "        values_df = spark.createDataFrame(value_rows, PROFILE_COLUMN_VALUES_SCHEMA).select(*PROFILE_COLUMN_VALUES_FIELDS)\n",
    "        view_name = _normalize_temp_view_name(profile_run_id, prefix=\"profile_column_values\")\n",
    "        try:\n",
    "            values_df.createOrReplaceTempView(view_name)\n",
    "            select_list = \", \".join(PROFILE_COLUMN_VALUES_FIELDS)\n",
    "            spark.sql(\n",
    "                f\"INSERT INTO {values_table} ({select_list}) SELECT {select_list} FROM {view_name}\"\n",
    "            )\n",
    "            print(f\"Persisted {len(value_rows)} column value rows for run {profile_run_id}.\")\n",
    "        finally:\n",
    "            with suppress(Exception):\n",
    "                spark.catalog.dropTempView(view_name)\n",
    "    else:\n",
    "        print(f\"No column value distributions extracted for run {profile_run_id}.\")\n",
    "\n",
    "def _persist_results_to_metadata(results_payload: dict[str, object] | None, payload_location: str | None) -> None:\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id widget is required before persisting profiling metadata.\")\n",
    "\n",
    "    raw_payload = results_payload\n",
    "    payload = raw_payload or {}\n",
    "    if not isinstance(payload, dict):\n",
    "        payload = {}\n",
    "\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "    anomalies_table = _metadata_table(\"dq_profile_anomalies\")\n",
    "\n",
    "    assignments = [\n",
    "        f\"status = {_sql_string_literal(payload.get('status') or 'unknown')}\",\n",
    "        \"completed_at = current_timestamp()\",\n",
    "        f\"row_count = {_sql_numeric_literal(payload.get('row_count'))}\",\n",
    "        f\"anomaly_count = {_sql_numeric_literal(payload.get('anomaly_count'))}\",\n",
    "        f\"payload_path = {_sql_string_literal(payload_location)}\",\n",
    "    ]\n",
    "\n",
    "    update_sql = (\n",
    "        f\"UPDATE {profiles_table} \"\n",
    "        f\"SET {', '.join(assignments)} \"\n",
    "        f\"WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    )\n",
    "    spark.sql(update_sql)\n",
    "    print(f\"Updated dq_profiles entry for run {profile_run_id}.\")\n",
    "\n",
    "    _persist_profile_detail_tables(raw_payload, payload_location)\n",
    "\n",
    "    anomalies = list(payload.get(\"anomalies\") or [])\n",
    "    delete_sql = f\"DELETE FROM {anomalies_table} WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    spark.sql(delete_sql)\n",
    "\n",
    "    if not anomalies:\n",
    "        print(f\"No anomalies to persist for run {profile_run_id}.\")\n",
    "        return\n",
    "\n",
    "    anomaly_rows = []\n",
    "    for anomaly in anomalies:\n",
    "        anomaly_rows.append(\n",
    "            {\n",
    "                \"profile_run_id\": profile_run_id,\n",
    "                \"table_name\": anomaly.get(\"table_name\"),\n",
    "                \"column_name\": anomaly.get(\"column_name\"),\n",
    "                \"anomaly_type\": anomaly.get(\"anomaly_type\"),\n",
    "                \"severity\": anomaly.get(\"severity\"),\n",
    "                \"description\": anomaly.get(\"description\"),\n",
    "                \"detected_at\": _parse_anomaly_timestamp(anomaly.get(\"detected_at\")) or datetime.utcnow(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    anomalies_df = spark.createDataFrame(anomaly_rows, PROFILE_ANOMALIES_SCHEMA).select(*PROFILE_ANOMALIES_FIELDS)\n",
    "    view_name = _normalize_temp_view_name(profile_run_id)\n",
    "    try:\n",
    "        anomalies_df.createOrReplaceTempView(view_name)\n",
    "        spark.sql(\n",
    "            f\"INSERT INTO {anomalies_table} \"\n",
    "            \"(profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at) \"\n",
    "            f\"SELECT profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at FROM {view_name}\"\n",
    "        )\n",
    "    finally:\n",
    "        with suppress(Exception):\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "\n",
    "    print(f\"Persisted {len(anomalies)} anomalies for run {profile_run_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the profiling job by persisting payload artifacts and metadata updates.\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(f\"Finalizing profiling run {profile_run_id}...\")\n",
    "\n",
    "\n",
    "def _guess_profiling_payload():\n",
    "    for key in (\"profiling_payload\", \"profile_payload\", \"results_payload\", \"payload\", \"raw_payload\"):\n",
    "        candidate = globals().get(key)\n",
    "        if isinstance(candidate, (dict, list)):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def _sanitize_segment(value, fallback):\n",
    "    cleaned = (value or fallback or \"\").strip() or fallback\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.-]\", \"_\", cleaned)\n",
    "\n",
    "\n",
    "def _default_artifact_path():\n",
    "    base_root = (payload_base_path or DEFAULT_PRIVATE_PAYLOAD_ROOT or \"dbfs:/tmp/conversioncentral/profiles\").rstrip(\"/\")\n",
    "    group_segment = _sanitize_segment(table_group_id, \"table_group\")\n",
    "    run_segment = _sanitize_segment(profile_run_id, \"profile_run\")\n",
    "    return f\"{base_root}/{group_segment}/{run_segment}.json\"\n",
    "\n",
    "\n",
    "def _materialize_payload_artifact(payload_obj, target_path):\n",
    "    destination = target_path\n",
    "    if destination.startswith(\"/dbfs/\"):\n",
    "        destination = f\"dbfs:/{destination[6:]}\"\n",
    "    if destination.startswith(\"dbfs:/\"):\n",
    "        redirected = _redirect_dbfs_path(destination)\n",
    "        if not redirected:\n",
    "            return None\n",
    "        destination = redirected\n",
    "    encoded = _encode_payload_json(payload_obj) or json.dumps(payload_obj, default=str)\n",
    "    if destination.startswith(\"dbfs:/\") or destination.startswith(\"file:/\"):\n",
    "        _mkdirs_if_supported(destination)\n",
    "        dbutils.fs.put(destination, encoded, True)\n",
    "        return destination\n",
    "    directory = os.path.dirname(destination)\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    with open(destination, \"w\", encoding=\"utf-8\") as handle:\n",
    "        handle.write(encoded)\n",
    "    return destination\n",
    "\n",
    "\n",
    "results_payload = _guess_profiling_payload()\n",
    "payload_location = payload_path or None\n",
    "storage_mode = _resolve_payload_storage_mode()\n",
    "\n",
    "if not payload_location and results_payload is not None and _payload_storage_is_artifact(storage_mode):\n",
    "    target_path = _default_artifact_path()\n",
    "    resolved_location = _materialize_payload_artifact(results_payload, target_path)\n",
    "    if resolved_location:\n",
    "        payload_location = resolved_location\n",
    "        print(f\"Persisted profiling payload artifact to {payload_location}.\")\n",
    "    else:\n",
    "        print(\"Unable to persist payload artifact; continuing with inline payload only.\")\n",
    "\n",
    "if payload_location and payload_location.startswith(\"/dbfs/\"):\n",
    "    payload_location = f\"dbfs:/{payload_location[6:]}\"\n",
    "\n",
    "if results_payload is None and not payload_location:\n",
    "    print(\"WARNING: no profiling payload or artifact path detected; detail tables will remain empty.\")\n",
    "\n",
    "_persist_results_to_metadata(results_payload, payload_location)\n",
    "print(\"Profiling metadata persistence completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
