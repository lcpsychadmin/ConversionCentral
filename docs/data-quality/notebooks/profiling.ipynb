{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"payload_storage\", \"\")\n",
    "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
    "payload_path = raw_payload_path or None\n",
    "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "from datetime import datetime\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from typing import Iterable\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, BinaryType, MapType, StructType\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "VALUE_DISTRIBUTION_LIMIT = 25\n",
    "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
    "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
    "MAX_VALUE_DISPLAY_LENGTH = 256\n",
    "\n",
    "PROFILE_COLUMN_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"qualified_name\",\n",
    "    \"data_type\",\n",
    "    \"general_type\",\n",
    "    \"ordinal_position\",\n",
    "    \"row_count\",\n",
    "    \"null_count\",\n",
    "    \"non_null_count\",\n",
    "    \"distinct_count\",\n",
    "    \"min_value\",\n",
    "    \"max_value\",\n",
    "    \"avg_value\",\n",
    "    \"stddev_value\",\n",
    "    \"median_value\",\n",
    "    \"p95_value\",\n",
    "    \"true_count\",\n",
    "    \"false_count\",\n",
    "    \"min_length\",\n",
    "    \"max_length\",\n",
    "    \"avg_length\",\n",
    "    \"non_ascii_ratio\",\n",
    "    \"min_date\",\n",
    "    \"max_date\",\n",
    "    \"date_span_days\",\n",
    "    \"metrics_json\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMNS_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"qualified_name\", T.StringType(), True),\n",
    "        T.StructField(\"data_type\", T.StringType(), True),\n",
    "        T.StructField(\"general_type\", T.StringType(), True),\n",
    "        T.StructField(\"ordinal_position\", T.IntegerType(), True),\n",
    "        T.StructField(\"row_count\", T.LongType(), True),\n",
    "        T.StructField(\"null_count\", T.LongType(), True),\n",
    "        T.StructField(\"non_null_count\", T.LongType(), True),\n",
    "        T.StructField(\"distinct_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_value\", T.StringType(), True),\n",
    "        T.StructField(\"max_value\", T.StringType(), True),\n",
    "        T.StructField(\"avg_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"stddev_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"median_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"p95_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"true_count\", T.LongType(), True),\n",
    "        T.StructField(\"false_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"max_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"avg_length\", T.DoubleType(), True),\n",
    "        T.StructField(\"non_ascii_ratio\", T.DoubleType(), True),\n",
    "        T.StructField(\"min_date\", T.DateType(), True),\n",
    "        T.StructField(\"max_date\", T.DateType(), True),\n",
    "        T.StructField(\"date_span_days\", T.IntegerType(), True),\n",
    "        T.StructField(\"metrics_json\", T.StringType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PROFILE_COLUMN_VALUES_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"value\",\n",
    "    \"value_hash\",\n",
    "    \"frequency\",\n",
    "    \"relative_freq\",\n",
    "    \"rank\",\n",
    "    \"bucket_label\",\n",
    "    \"bucket_lower_bound\",\n",
    "    \"bucket_upper_bound\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMN_VALUES_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"value\", T.StringType(), True),\n",
    "        T.StructField(\"value_hash\", T.StringType(), True),\n",
    "        T.StructField(\"frequency\", T.LongType(), True),\n",
    "        T.StructField(\"relative_freq\", T.DoubleType(), True),\n",
    "        T.StructField(\"rank\", T.IntegerType(), True),\n",
    "        T.StructField(\"bucket_label\", T.StringType(), True),\n",
    "        T.StructField(\"bucket_lower_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"bucket_upper_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PROFILE_ANOMALIES_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"anomaly_type\",\n",
    "    \"severity\",\n",
    "    \"description\",\n",
    "    \"detected_at\",\n",
    "]\n",
    "\n",
    "PROFILE_ANOMALIES_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"table_name\", T.StringType(), True),\n",
    "        T.StructField(\"column_name\", T.StringType(), True),\n",
    "        T.StructField(\"anomaly_type\", T.StringType(), True),\n",
    "        T.StructField(\"severity\", T.StringType(), True),\n",
    "        T.StructField(\"description\", T.StringType(), True),\n",
    "        T.StructField(\"detected_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def _split_identifier(value: str | None) -> list[str]:\n",
    "    cleaned = (value or \"\").replace(\"`\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    return [segment.strip() for segment in cleaned.split(\".\") if segment.strip()]\n",
    "\n",
    "\n",
    "def _catalog_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload and call back into the API\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import socket\n",
    "from contextlib import suppress\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
    "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
    "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
    "\n",
    "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
    "\n",
    "\n",
    "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
    "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
    "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
    "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
    "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
    "\n",
    "\n",
    "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
    "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
    "    current = error\n",
    "    while current:\n",
    "        if isinstance(current, socket.gaierror):\n",
    "            return True\n",
    "        name = current.__class__.__name__.lower()\n",
    "        if \"nameresolution\" in name:\n",
    "            return True\n",
    "        message = str(current).lower()\n",
    "        if \"temporary failure in name resolution\" in message:\n",
    "            return True\n",
    "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
    "    return False\n",
    "\n",
    "\n",
    "def _rewrite_heroku_app_host(url: str | None) -> str | None:\n",
    "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.hostname\n",
    "    if not host:\n",
    "        return None\n",
    "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
    "    if not match:\n",
    "        return None\n",
    "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
    "    netloc = canonical_host\n",
    "    if parsed.port:\n",
    "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
    "    if parsed.username:\n",
    "        auth = parsed.username\n",
    "        if parsed.password:\n",
    "            auth = f\"{auth}:{parsed.password}\"\n",
    "        netloc = f\"{auth}@{netloc}\"\n",
    "    scheme = parsed.scheme or \"https\"\n",
    "    if scheme.lower() == \"http\":\n",
    "        scheme = \"https\"\n",
    "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))\n",
    "\n",
    "\n",
    "def _is_dbfs_path(path: str | None) -> bool:\n",
    "    return bool(path and path.lower().startswith(\"dbfs:/\"))\n",
    "\n",
    "\n",
    "def _has_uri_scheme(value: str | None) -> bool:\n",
    "    return bool(value and URI_SCHEME_PATTERN.match(value.strip()))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _dbfs_root_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/_dbfs_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DBFS_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _driver_fs_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/_driver_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DRIVER_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "def _warn_storage_disabled(message: str) -> None:\n",
    "    global _STORAGE_DISABLED_NOTICE_EMITTED\n",
    "    if not _STORAGE_DISABLED_NOTICE_EMITTED:\n",
    "        print(message)\n",
    "        _STORAGE_DISABLED_NOTICE_EMITTED = True\n",
    "\n",
    "\n",
    "def _redirect_dbfs_path(path: str) -> str | None:\n",
    "    global _DBFS_REDIRECT_NOTICE_EMITTED\n",
    "    if not _is_dbfs_path(path):\n",
    "        return path\n",
    "    if not _dbfs_root_is_disabled():\n",
    "        return path\n",
    "    if _driver_fs_is_disabled():\n",
    "        _warn_storage_disabled(\n",
    "            \"DBFS root access and driver filesystem writes are both disabled; payload artifacts will be skipped unless \"\n",
    "            \"a cloud storage payload_base_path is provided.\"\n",
    "        )\n",
    "        return None\n",
    "    if not _DBFS_REDIRECT_NOTICE_EMITTED:\n",
    "        print(\n",
    "            \"DBFS root access is disabled on this workspace; persisting profiling artifacts to the driver filesystem \"\n",
    "            \"instead.\"\n",
    "        )\n",
    "        _DBFS_REDIRECT_NOTICE_EMITTED = True\n",
    "    suffix = path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    redirected = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/{suffix}\" if suffix else DEFAULT_DRIVER_PAYLOAD_ROOT\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _mkdirs_if_supported(target_path: str) -> None:\n",
    "    lowered = target_path.lower()\n",
    "    if lowered.startswith(\"dbfs:/\") and _dbfs_root_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"file:/\") and _driver_fs_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"dbfs:/\") or lowered.startswith(\"file:/\"):\n",
    "        parent_dir = target_path.rsplit(\"/\", 1)[0]\n",
    "        dbutils.fs.mkdirs(parent_dir)\n",
    "\n",
    "\n",
    "def _ensure_https_base_url(value: str) -> str:\n",
    "    normalized = (value or \"\").strip()\n",
    "    if not normalized:\n",
    "        return normalized\n",
    "    parsed = urlparse(normalized)\n",
    "    if not parsed.scheme:\n",
    "        normalized = f\"https://{normalized.lstrip('/')}\"\n",
    "        parsed = urlparse(normalized)\n",
    "    if parsed.scheme.lower() == \"http\":\n",
    "        parsed = parsed._replace(scheme=\"https\")\n",
    "    normalized = urlunparse(parsed).rstrip(\"/\")\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _lookup_metadata_setting(setting_key: str) -> str | None:\n",
    "    normalized_key = (setting_key or \"\").strip().lower()\n",
    "    if not normalized_key:\n",
    "        return None\n",
    "    try:\n",
    "        settings_table = _metadata_table(\"dq_settings\")\n",
    "    except NameError:\n",
    "        return None\n",
    "    try:\n",
    "        row = (\n",
    "            spark.table(settings_table)\n",
    "            .where(F.lower(F.col(\"key\")) == normalized_key)\n",
    "            .select(\"value\")\n",
    "            .limit(1)\n",
    "            .collect()\n",
    "        )\n",
    "    except AnalysisException:\n",
    "        return None\n",
    "    if not row:\n",
    "        return None\n",
    "    value = row[0].get(\"value\")\n",
    "    return value.strip() if isinstance(value, str) and value.strip() else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_payload_storage_mode(value: str | None) -> str | None:\n",
    "    normalized = (value or \"\").strip().lower()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if normalized in {\"inline\", \"database\", \"db\"}:\n",
    "        return \"inline\"\n",
    "    if normalized in {\"artifact\", \"artifacts\", \"file\", \"files\", \"path\", \"paths\", \"dbfs\", \"cloud\"}:\n",
    "        return \"artifact\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_payload_storage_mode() -> str:\n",
    "    widget_choice = _normalize_payload_storage_mode(dbutils.widgets.get(\"payload_storage\"))\n",
    "    if widget_choice:\n",
    "        return widget_choice\n",
    "    setting_choice = _normalize_payload_storage_mode(_lookup_metadata_setting(\"profile_payload_storage_mode\"))\n",
    "    if setting_choice:\n",
    "        return setting_choice\n",
    "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
    "\n",
    "\n",
    "def _payload_storage_is_artifact(mode: str) -> bool:\n",
    "    return (mode or \"\").strip().lower() == \"artifact\"\n",
    "\n",
    "\n",
    "def _encode_payload_json(payload: dict[str, object]) -> str | None:\n",
    "    try:\n",
    "        return json.dumps(payload, separators=(\",\", \":\"))\n",
    "    except TypeError as exc:\n",
    "        print(f\"Unable to serialize profiling payload: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _resolve_callback_behavior() -> str:\n",
    "    widget_value = (dbutils.widgets.get(\"callback_behavior\") or \"\").strip().lower()\n",
    "    if widget_value:\n",
    "        return widget_value\n",
    "    setting_value = (_lookup_metadata_setting(\"profile_callback_behavior\") or \"\").strip().lower()\n",
    "    if setting_value:\n",
    "        return setting_value\n",
    "    return DEFAULT_CALLBACK_BEHAVIOR\n",
    "\n",
    "\n",
    "def _callbacks_enabled(behavior: str) -> bool:\n",
    "    if behavior in {\"api\", \"callback\", \"legacy\"}:\n",
    "        return True\n",
    "    if behavior in {\"metadata_only\", \"metadata\", \"skip\", \"disabled\", \"off\"}:\n",
    "        return False\n",
    "    print(f\"Unknown callback behavior '{behavior}'; defaulting to metadata_only.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sql_string_literal(value: str | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    escaped = str(value).replace(\"'\", \"''\")\n",
    "    return f\"'{escaped}'\"\n",
    "\n",
    "\n",
    "def _sql_numeric_literal(value: int | float | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        return str(int(value))\n",
    "    except (TypeError, ValueError):\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "def _normalize_temp_view_name(suffix: str | None) -> str:\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", (suffix or \"profile_run\"))\n",
    "    return f\"_profile_anomalies_{cleaned}\"\n",
    "\n",
    "\n",
    "def _parse_anomaly_timestamp(value: str | None) -> datetime | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    candidate = value.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.endswith(\"Z\"):\n",
    "        candidate = f\"{candidate[:-1]}+00:00\"\n",
    "    with suppress(ValueError):\n",
    "        return datetime.fromisoformat(candidate)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _persist_results_to_metadata(results_payload: dict[str, object], payload_location: str | None) -> None:\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id widget is required before persisting profiling metadata.\")\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "    anomalies_table = _metadata_table(\"dq_profile_anomalies\")\n",
    "    assignments = [\n",
    "        f\"status = {_sql_string_literal(results_payload.get('status') or 'unknown')}\",\n",
    "        \"completed_at = current_timestamp()\",\n",
    "        f\"row_count = {_sql_numeric_literal(results_payload.get('row_count'))}\",\n",
    "        f\"anomaly_count = {_sql_numeric_literal(results_payload.get('anomaly_count'))}\",\n",
    "        f\"payload_path = {_sql_string_literal(payload_location)}\",\n",
    "    ]\n",
    "    update_sql = (\n",
    "        f\"UPDATE {profiles_table} \"\n",
    "        f\"SET {', '.join(assignments)} \"\n",
    "        f\"WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    )\n",
    "    spark.sql(update_sql)\n",
    "    print(f\"Updated dq_profiles entry for run {profile_run_id}.\")\n",
    "\n",
    "    anomalies = list(results_payload.get(\"anomalies\") or [])\n",
    "    delete_sql = f\"DELETE FROM {anomalies_table} WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    spark.sql(delete_sql)\n",
    "\n",
    "    if not anomalies:\n",
    "        print(f\"No anomalies to persist for run {profile_run_id}.\")\n",
    "        return\n",
    "\n",
    "    anomaly_rows = []\n",
    "    for anomaly in anomalies:\n",
    "        anomaly_rows.append(\n",
    "            {\n",
    "                \"profile_run_id\": profile_run_id,\n",
    "                \"table_name\": anomaly.get(\"table_name\"),\n",
    "                \"column_name\": anomaly.get(\"column_name\"),\n",
    "                \"anomaly_type\": anomaly.get(\"anomaly_type\"),\n",
    "                \"severity\": anomaly.get(\"severity\"),\n",
    "                \"description\": anomaly.get(\"description\"),\n",
    "                \"detected_at\": _parse_anomaly_timestamp(anomaly.get(\"detected_at\")) or datetime.utcnow(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    anomalies_df = spark.createDataFrame(anomaly_rows, PROFILE_ANOMALIES_SCHEMA).select(*PROFILE_ANOMALIES_FIELDS)\n",
    "    view_name = _normalize_temp_view_name(profile_run_id)\n",
    "    try:\n",
    "        anomalies_df.createOrReplaceTempView(view_name)\n",
    "        spark.sql(\n",
    "            f\"INSERT INTO {anomalies_table} \"\n",
    "            \"(profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at) \"\n",
    "            f\"SELECT profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at FROM {view_name}\"\n",
    "        )\n",
    "    finally:\n",
    "        with suppress(Exception):\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "\n",
    "    print(f\"Persisted {len(anomalies)} anomalies for run {profile_run_id}.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
