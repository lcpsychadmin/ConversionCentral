{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from typing import Iterable\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, BinaryType, MapType, StructType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "\n",
    "def _split_identifier(value: str | None) -> list[str]:\n",
    "    cleaned = (value or \"\").replace(\"`\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    return [segment.strip() for segment in cleaned.split(\".\") if segment.strip()]\n",
    "\n",
    "def _catalog_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None\n",
    "\n",
    "def _schema_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if not parts:\n",
    "        return None\n",
    "    return parts[-1]\n",
    "\n",
    "def _qualify(*parts: Iterable[str | None]) -> str:\n",
    "    tokens: list[str] = []\n",
    "    for part in parts:\n",
    "        if isinstance(part, (list, tuple)):\n",
    "            tokens.extend([token for token in part if token])\n",
    "        elif part:\n",
    "            tokens.append(part)\n",
    "    if not tokens:\n",
    "        raise ValueError(\"Cannot build a fully qualified identifier with no parts.\")\n",
    "    return \".\".join(f\"`{token}`\" for token in tokens)\n",
    "\n",
    "metadata_catalog = _catalog_component(dq_schema)\n",
    "metadata_schema = _schema_component(dq_schema)\n",
    "if metadata_schema is None:\n",
    "    raise ValueError(\"Unable to resolve schema portion of the data quality schema setting.\")\n",
    "if metadata_catalog is None:\n",
    "    fallback_catalog = _catalog_component(connection_catalog)\n",
    "    if fallback_catalog:\n",
    "        metadata_catalog = fallback_catalog\n",
    "    else:\n",
    "        with suppress(Exception):\n",
    "            metadata_catalog = spark.catalog.currentCatalog()\n",
    "\n",
    "connection_catalog_clean = _catalog_component(connection_catalog)\n",
    "connection_schema_clean = _schema_component(connection_schema)\n",
    "\n",
    "def _metadata_table(name: str) -> str:\n",
    "    return _qualify(metadata_catalog, metadata_schema, name) if metadata_catalog else _qualify(metadata_schema, name)\n",
    "\n",
    "def _compile_patterns(mask: str | None) -> list[re.Pattern[str]]:\n",
    "    if not mask:\n",
    "        return []\n",
    "    tokens = [token.strip() for token in re.split(r\"[\\n,]+\", mask) if token.strip()]\n",
    "    compiled: list[re.Pattern[str]] = []\n",
    "    for token in tokens:\n",
    "        escaped = re.escape(token).replace(\"\\\\*\", \".*\").replace(\"\\\\%\", \".*\")\n",
    "        compiled.append(re.compile(f\"^{escaped}$\", re.IGNORECASE))\n",
    "    return compiled\n",
    "\n",
    "def _matches_pattern(patterns: list[re.Pattern[str]], schema_name: str | None, table_name: str) -> bool:\n",
    "    if not patterns:\n",
    "        return False\n",
    "    candidate_full = \".\".join(filter(None, [(schema_name or \"\").lower(), table_name.lower()])).strip(\".\")\n",
    "    short_name = table_name.lower()\n",
    "    for pattern in patterns:\n",
    "        if pattern.match(candidate_full) or pattern.match(short_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _qualify_data_table(raw_schema: str | None, table_name: str) -> str:\n",
    "    table_tokens = _split_identifier(table_name)\n",
    "    if len(table_tokens) >= 2:\n",
    "        return _qualify(table_tokens)\n",
    "    schema_tokens = _split_identifier(raw_schema)\n",
    "    if len(schema_tokens) >= 2:\n",
    "        return _qualify(schema_tokens + table_tokens)\n",
    "    catalog_part = schema_tokens[0] if schema_tokens else connection_catalog_clean\n",
    "    schema_part = schema_tokens[-1] if schema_tokens else connection_schema_clean\n",
    "    return _qualify(catalog_part, schema_part, table_tokens[0] if table_tokens else table_name)\n",
    "\n",
    "def _select_profile_columns(df) -> list[str]:\n",
    "    allowed: list[str] = []\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, (BinaryType, MapType, ArrayType, StructType)):\n",
    "            continue\n",
    "        allowed.append(field.name)\n",
    "        if len(allowed) >= MAX_COLUMNS_TO_PROFILE:\n",
    "            break\n",
    "    return allowed\n",
    "\n",
    "def _record_anomaly(buffer: list[dict[str, str]], table_name: str, column_name: str | None, anomaly_type: str, severity: str, description: str, detected_at: str) -> None:\n",
    "    buffer.append(\n",
    "        {\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"anomaly_type\": anomaly_type,\n",
    "            \"severity\": severity,\n",
    "            \"description\": description,\n",
    "            \"detected_at\": detected_at,\n",
    "        }\n",
    "    )\n",
    "\n",
    "metadata_tables_name = _metadata_table(\"dq_tables\")\n",
    "group_table_name = _metadata_table(\"dq_table_groups\")\n",
    "group_rows = (\n",
    "    spark.table(group_table_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"name\", \"profiling_include_mask\", \"profiling_exclude_mask\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "if not group_rows:\n",
    "    raise ValueError(f\"Table group '{table_group_id}' not found in schema '{dq_schema}'.\")\n",
    "group_details = group_rows[0].asDict()\n",
    "include_patterns = _compile_patterns(group_details.get(\"profiling_include_mask\"))\n",
    "exclude_patterns = _compile_patterns(group_details.get(\"profiling_exclude_mask\"))\n",
    "\n",
    "table_rows = (\n",
    "    spark.table(metadata_tables_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"schema_name\", \"table_name\")\n",
    "    .collect()\n",
    ")\n",
    "if not table_rows:\n",
    "    raise ValueError(f\"No dq_tables rows registered for table_group_id '{table_group_id}'.\")\n",
    "\n",
    "table_candidates: list[dict[str, str]] = []\n",
    "for row in table_rows:\n",
    "    schema_value = (row[\"schema_name\"] or connection_schema_clean or \"\").strip() or None\n",
    "    table_value = (row[\"table_name\"] or \"\").strip()\n",
    "    if not table_value:\n",
    "        continue\n",
    "    if include_patterns and not _matches_pattern(include_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    if exclude_patterns and _matches_pattern(exclude_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    label = \".\".join(filter(None, [schema_value, table_value])) or table_value\n",
    "    table_candidates.append({\"schema_name\": schema_value, \"table_name\": table_value, \"label\": label})\n",
    "\n",
    "if not table_candidates:\n",
    "    raise ValueError(\"All candidate tables were filtered out by include/exclude masks.\")\n",
    "\n",
    "generated_at = datetime.utcnow().isoformat() + \"Z\"\n",
    "anomalies: list[dict[str, str]] = []\n",
    "table_profiles: list[dict[str, object]] = []\n",
    "total_rows = 0\n",
    "profiling_failures = 0\n",
    "profiling_successes = 0\n",
    "\n",
    "print(f\"Profiling {len(table_candidates)} tables for group {table_group_id}.\")\n",
    "for candidate in table_candidates:\n",
    "    schema_value = candidate[\"schema_name\"]\n",
    "    table_value = candidate[\"table_name\"]\n",
    "    label = candidate[\"label\"]\n",
    "    qualified_name = _qualify_data_table(schema_value, table_value)\n",
    "    table_result: dict[str, object] = {\n",
    "        \"table_name\": label,\n",
    "        \"qualified_name\": qualified_name,\n",
    "    }\n",
    "    print(f\"-> Scanning {qualified_name}\")\n",
    "    try:\n",
    "        df = spark.read.table(qualified_name)\n",
    "    except AnalysisException as exc:\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"missing_table\", \"high\", f\"Spark could not read {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "    except Exception as exc:\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"profiling_error\", \"high\", f\"Unexpected error while reading {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "\n",
    "    row_count = df.count()\n",
    "    table_result[\"row_count\"] = int(row_count)\n",
    "    total_rows += row_count\n",
    "\n",
    "    if row_count == 0:\n",
    "        profiling_failures += 1\n",
    "        _record_anomaly(anomalies, label, None, \"empty_table\", \"high\", \"Table returned zero rows during profiling.\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "\n",
    "    profiling_successes += 1\n",
    "    profile_columns = _select_profile_columns(df)\n",
    "    table_result[\"profiled_columns\"] = profile_columns\n",
    "    full_column_count = len(df.columns)\n",
    "    if full_column_count > len(profile_columns):\n",
    "        table_result[\"profiled_columns_truncated\"] = full_column_count - len(profile_columns)\n",
    "\n",
    "    if profile_columns:\n",
    "        agg_exprs = [F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0)).alias(col_name) for col_name in profile_columns]\n",
    "        null_counts = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "        column_null_ratios: dict[str, float] = {}\n",
    "        for column in profile_columns:\n",
    "            null_ratio = float((null_counts.get(column, 0) or 0) / row_count)\n",
    "            column_null_ratios[column] = null_ratio\n",
    "            if null_ratio >= NULL_RATIO_ALERT_THRESHOLD:\n",
    "                severity = \"high\" if null_ratio >= HIGH_NULL_RATIO_THRESHOLD else \"medium\"\n",
    "                description = f\"Null ratio {null_ratio:.2%} exceeds {NULL_RATIO_ALERT_THRESHOLD:.0%} threshold.\"\n",
    "                _record_anomaly(anomalies, label, column, \"null_ratio\", severity, description, generated_at)\n",
    "        table_result[\"column_null_ratios\"] = column_null_ratios\n",
    "\n",
    "    table_profiles.append(table_result)\n",
    "\n",
    "status = \"completed\" if profiling_successes else \"failed\"\n",
    "results = {\n",
    "    \"table_group_id\": table_group_id,\n",
    "    \"profile_run_id\": profile_run_id,\n",
    "    \"table_group_name\": group_details.get(\"name\"),\n",
    "    \"status\": status,\n",
    "    \"row_count\": int(total_rows),\n",
    "    \"anomaly_count\": len(anomalies),\n",
    "    \"anomalies\": anomalies,\n",
    "    \"generated_at\": generated_at,\n",
    "    \"table_profiles\": table_profiles,\n",
    "    \"diagnostics\": {\n",
    "        \"tables_requested\": len(table_rows),\n",
    "        \"tables_profiled\": profiling_successes,\n",
    "        \"tables_failed\": profiling_failures,\n",
    "        \"include_mask_applied\": bool(include_patterns),\n",
    "        \"exclude_mask_applied\": bool(exclude_patterns),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Profiling complete: {profiling_successes} succeeded, {profiling_failures} failed, total rows={total_rows}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload and call back into the API\n",
    "\n",
    "def _resolve_callback_target(base_url: str | None, run_id: str) -> str | None:\n",
    "    if not base_url:\n",
    "        return None\n",
    "    normalized = base_url.strip()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if \"{profile_run_id}\" in normalized:\n",
    "        try:\n",
    "            return normalized.format(profile_run_id=run_id)\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "    normalized = normalized.rstrip(\"/\")\n",
    "    if normalized.endswith(\"/complete\"):\n",
    "        return normalized\n",
    "    return f\"{normalized}/{run_id}/complete\"\n",
    "\n",
    "if payload_path:\n",
    "    dbutils.fs.put(payload_path, json.dumps(results, indent=2), overwrite=True)\n",
    "    print(f\"Wrote profiling payload to {payload_path}\")\n",
    "else:\n",
    "    print(\"No payload path supplied; skipping artifact export.\")\n",
    "\n",
    "callback_target = _resolve_callback_target(callback_url, profile_run_id)\n",
    "if callback_target:\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if callback_token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {callback_token}\"\n",
    "    callback_body = {\n",
    "        \"status\": results[\"status\"],\n",
    "        \"row_count\": results[\"row_count\"],\n",
    "        \"anomaly_count\": results[\"anomaly_count\"],\n",
    "        \"anomalies\": results[\"anomalies\"],\n",
    "    }\n",
    "    response = requests.post(callback_target, headers=headers, json=callback_body, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    print(f\"Callback succeeded: {callback_target} ({response.status_code})\")\n",
    "else:\n",
    "    print(\"Callback URL not provided; skipping completion POST.\")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
