{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b43a81c",
      "metadata": {},
      "source": [
        "# ConversionCentral Managed Profiling\n",
        "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c215aba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect parameters passed by the FastAPI backend\n",
        "\n",
        "\n",
        "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
        "dbutils.widgets.text(\"table_group_id\", \"\")\n",
        "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
        "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
        "dbutils.widgets.text(\"payload_path\", \"\")\n",
        "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
        "dbutils.widgets.text(\"callback_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_token\", \"\")\n",
        "dbutils.widgets.text(\"payload_storage\", \"\")\n",
        "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"schema_name\", \"\")\n",
        "dbutils.widgets.text(\"connection_id\", \"\")\n",
        "dbutils.widgets.text(\"connection_name\", \"\")\n",
        "dbutils.widgets.text(\"system_id\", \"\")\n",
        "dbutils.widgets.text(\"project_key\", \"\")\n",
        "dbutils.widgets.text(\"http_path\", \"\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "\n",
        "_NOTEBOOK_STAGE_SEQUENCE = (\n",
        "    (\"parameters\", \"Cell 2 (widget and Spark initialization)\"),\n",
        "    (\"profiling\", \"Cell 3 (profiling logic)\"),\n",
        "    (\"payload_persistence\", \"Cell 4 (payload persistence and callbacks)\"),\n",
        "    (\"metadata_helpers\", \"Cell 5 (metadata helper definitions)\"),\n",
        "    (\"finalization\", \"Cell 6 (final persistence and callbacks)\")\n",
        ")\n",
        "_NOTEBOOK_STAGE_LOOKUP = {label: (idx, hint) for idx, (label, hint) in enumerate(_NOTEBOOK_STAGE_SEQUENCE)}\n",
        "_STAGE_SYMBOL_REQUIREMENTS = {\n",
        "    \"parameters\": (\"table_group_id\", \"profile_run_id\", \"dq_schema\"),\n",
        "    \"profiling\": (\"MAX_COLUMNS_TO_PROFILE\",),\n",
        "    \"payload_persistence\": (\"_resolve_payload_storage_mode\", \"_payload_storage_is_artifact\"),\n",
        "    \"metadata_helpers\": (\"_persist_results_to_metadata\",),\n",
        "}\n",
        "\n",
        "\n",
        "def _ensure_notebook_stage(stage_label: str) -> None:\n",
        "    if stage_label not in _NOTEBOOK_STAGE_LOOKUP:\n",
        "        raise ValueError(f\"Unknown notebook stage '{stage_label}'.\")\n",
        "    stage_index, stage_hint = _NOTEBOOK_STAGE_LOOKUP[stage_label]\n",
        "    for prior_label, prior_hint in _NOTEBOOK_STAGE_SEQUENCE[:stage_index]:\n",
        "        required_symbols = _STAGE_SYMBOL_REQUIREMENTS.get(prior_label, ())\n",
        "        missing = [symbol for symbol in required_symbols if symbol not in globals()]\n",
        "        if missing:\n",
        "            missing_list = \", \".join(sorted(missing))\n",
        "            raise RuntimeError(\n",
        "                \"Profiling notebook Cells 1-6 must run sequentially. \"\n",
        "                f\"Run {prior_hint} before {stage_hint} (missing: {missing_list}).\",\n",
        "            )\n",
        "\n",
        "\n",
        "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
        "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
        "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
        "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
        "payload_path = raw_payload_path or None\n",
        "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
        "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
        "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
        "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
        "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
        "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
        "\n",
        "\n",
        "if not table_group_id or not profile_run_id:\n",
        "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
        "if not dq_schema:\n",
        "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065630ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile the tables registered for this table group and build the result payload.\n",
        "from datetime import datetime\n",
        "import re\n",
        "from contextlib import suppress\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run Cell 2 before profiling.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"profiling\")\n",
        "\n",
        "\n",
        "import datetime as dt\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "MAX_COLUMNS_TO_PROFILE = 25\n",
        "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
        "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
        "VALUE_DISTRIBUTION_LIMIT = 25\n",
        "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
        "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
        "MAX_VALUE_DISPLAY_LENGTH = 256\n",
        "\n",
        "\n",
        "PROFILE_COLUMN_FIELDS = [\n",
        "    \"profile_run_id\",\n",
        "    \"schema_name\",\n",
        "    \"table_name\",\n",
        "    \"column_name\",\n",
        "    \"qualified_name\",\n",
        "    \"data_type\",\n",
        "    \"general_type\",\n",
        "    \"ordinal_position\",\n",
        "    \"row_count\",\n",
        "    \"null_count\",\n",
        "    \"non_null_count\",\n",
        "    \"distinct_count\",\n",
        "    \"min_value\",\n",
        "    \"max_value\",\n",
        "    \"avg_value\",\n",
        "    \"stddev_value\",\n",
        "    \"median_value\",\n",
        "    \"p95_value\",\n",
        "    \"true_count\",\n",
        "    \"false_count\",\n",
        "    \"min_length\",\n",
        "    \"max_length\",\n",
        "    \"avg_length\",\n",
        "    \"non_ascii_ratio\",\n",
        "    \"min_date\",\n",
        "    \"max_date\",\n",
        "    \"date_span_days\",\n",
        "    \"metrics_json\",\n",
        "    \"generated_at\",\n",
        "]\n",
        "\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd3b54f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist payload and call back into the API\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run prior cells before payload persistence.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"payload_persistence\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import socket\n",
        "from contextlib import suppress\n",
        "from functools import lru_cache\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
        "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
        "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
        "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
        "_VALID_PAYLOAD_STORAGE_MODES = {\"inline\", \"artifact\", \"both\"}\n",
        "def _clean_widget_value(value: Optional[str]) -> str:\n",
        "    return (value or \"\").strip()\n",
        "def _resolve_payload_storage_mode() -> str:\n",
        "    raw_value = _clean_widget_value(dbutils.widgets.get(\"payload_storage\")).lower()\n",
        "    if raw_value in _VALID_PAYLOAD_STORAGE_MODES:\n",
        "        return raw_value\n",
        "    if raw_value in {\"inline_only\", \"inline_metadata\"}:\n",
        "        return \"inline\"\n",
        "    if raw_value in {\"artifact_only\", \"artifact_metadata\"}:\n",
        "        return \"artifact\"\n",
        "    if payload_path:\n",
        "        return \"artifact\"\n",
        "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
        "def _payload_storage_is_artifact(mode: str) -> bool:\n",
        "    normalized = (mode or DEFAULT_PAYLOAD_STORAGE_MODE).strip().lower()\n",
        "    return normalized in {\"artifact\", \"both\"}\n",
        "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
        "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
        "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
        "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
        "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
        "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
        "    current = error\n",
        "    while current:\n",
        "        if isinstance(current, socket.gaierror):\n",
        "            return True\n",
        "        name = current.__class__.__name__.lower()\n",
        "        if \"nameresolution\" in name:\n",
        "            return True\n",
        "        message = str(current).lower()\n",
        "        if \"temporary failure in name resolution\" in message:\n",
        "            return True\n",
        "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _rewrite_heroku_app_host(url: Optional[str]) -> Optional[str]:\n",
        "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
        "    if not url:\n",
        "        return None\n",
        "    parsed = urlparse(url)\n",
        "    host = parsed.hostname\n",
        "    if not host:\n",
        "        return None\n",
        "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
        "    if not match:\n",
        "        return None\n",
        "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
        "    netloc = canonical_host\n",
        "    if parsed.port:\n",
        "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
        "    if parsed.username:\n",
        "        auth = parsed.username\n",
        "        if parsed.password:\n",
        "            auth = f\"{auth}:{parsed.password}\"\n",
        "        netloc = f\"{auth}@{netloc}\"\n",
        "    scheme = parsed.scheme or \"https\"\n",
        "    if scheme.lower() == \"http\":\n",
        "        scheme = \"https\"\n",
        "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7b24bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column/value persistence helpers and overrides\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "from contextlib import suppress\n",
        "from typing import Any, Mapping\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run earlier cells before defining metadata helpers.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"metadata_helpers\")\n",
        "\n",
        "\n",
        "def _escape_identifier(identifier: str) -> str:\n",
        "    cleaned = (identifier or \"\").strip().replace(\"`\", \"\")\n",
        "    if not cleaned:\n",
        "        raise ValueError(\"Metadata identifiers cannot be empty.\")\n",
        "    return f\"`{cleaned}`\"\n",
        "\n",
        "\n",
        "def _metadata_schema_reference() -> str:\n",
        "    if not dq_schema:\n",
        "        raise ValueError(\"data_quality_schema widget must be set before resolving metadata tables.\")\n",
        "    catalog = (connection_catalog or \"\").strip()\n",
        "    if catalog:\n",
        "        return f\"{_escape_identifier(catalog)}.{_escape_identifier(dq_schema)}\"\n",
        "    return _escape_identifier(dq_schema)\n",
        "\n",
        "\n",
        "def _metadata_table(table_name: str) -> str:\n",
        "    return f\"{_metadata_schema_reference()}.{_escape_identifier(table_name)}\"\n",
        "\n",
        "\n",
        "def _first_non_empty(*values):\n",
        "    for value in values:\n",
        "        if isinstance(value, str):\n",
        "            candidate = value.strip()\n",
        "            if candidate:\n",
        "                return candidate\n",
        "        elif value is not None:\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_int(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return int(value)\n",
        "    if isinstance(value, int):\n",
        "        return value\n",
        "    if isinstance(value, float):\n",
        "        if not math.isfinite(value):\n",
        "            return None\n",
        "        return int(round(value))\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            if \".\" in candidate:\n",
        "                return int(float(candidate))\n",
        "            return int(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_float(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return float(value)\n",
        "    if isinstance(value, (int, float)):\n",
        "        numeric = float(value)\n",
        "        if math.isfinite(numeric):\n",
        "            return numeric\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            numeric = float(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "        return numeric if math.isfinite(numeric) else None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _sql_literal(value) -> str:\n",
        "    if value is None:\n",
        "        return \"NULL\"\n",
        "    if isinstance(value, datetime):\n",
        "        return f\"'{value.strftime('%Y-%m-%d %H:%M:%S')}'\"\n",
        "    text = str(value).replace(\"'\", \"''\")\n",
        "    return f\"'{text}'\"\n",
        "\n",
        "\n",
        "def _sql_number(value) -> str:\n",
        "    if value is None:\n",
        "        return \"NULL\"\n",
        "    return str(value)\n",
        "\n",
        "\n",
        "def _coerce_timestamp_value(value) -> datetime | None:\n",
        "    if isinstance(value, datetime):\n",
        "        return value\n",
        "    if isinstance(value, (int, float)):\n",
        "        numeric = float(value)\n",
        "        if abs(numeric) > 1_000_000_000_000:\n",
        "            numeric /= 1000.0\n",
        "        with suppress(Exception):\n",
        "            return datetime.utcfromtimestamp(numeric)\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        text = value.strip()\n",
        "        if not text:\n",
        "            return None\n",
        "        normalized = text[:-1] + \"+00:00\" if text.endswith(\"Z\") else text\n",
        "        with suppress(ValueError):\n",
        "            return datetime.fromisoformat(normalized)\n",
        "        for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"):\n",
        "            with suppress(ValueError):\n",
        "                return datetime.strptime(normalized, fmt)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_databricks_run_id() -> str | None:\n",
        "    with suppress(Exception):\n",
        "        value = spark.conf.get(\"spark.databricks.job.runId\")\n",
        "        if value:\n",
        "            return str(value)\n",
        "    with suppress(Exception):\n",
        "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
        "        run_id = ctx.runId().get()\n",
        "        if run_id:\n",
        "            return str(run_id)\n",
        "    with suppress(Exception):\n",
        "        task_run_id = dbutils.jobs.taskRunId()\n",
        "        if task_run_id:\n",
        "            return str(task_run_id)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_profile_summary(payload: Any) -> Mapping[str, Any]:\n",
        "    if isinstance(payload, Mapping):\n",
        "        for key in (\"summary\", \"profile_summary\", \"profileSummary\", \"metadata\", \"run\"):\n",
        "            nested = payload.get(key)\n",
        "            if isinstance(nested, Mapping):\n",
        "                return nested\n",
        "        return payload\n",
        "    if isinstance(payload, list):\n",
        "        for item in payload:\n",
        "            if isinstance(item, Mapping):\n",
        "                return item\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _persist_results_to_metadata(results_payload, payload_location):\n",
        "    if not table_group_id:\n",
        "        raise ValueError(\"table_group_id must be defined before persisting metadata.\")\n",
        "    if not profile_run_id:\n",
        "        raise ValueError(\"profile_run_id must be defined before persisting metadata.\")\n",
        "\n",
        "    summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
        "    status = _first_non_empty(summary.get(\"status\"), summary.get(\"state\"), \"completed\")\n",
        "    started_at = _coerce_timestamp_value(summary.get(\"started_at\") or summary.get(\"startedAt\"))\n",
        "    completed_at = _coerce_timestamp_value(summary.get(\"completed_at\") or summary.get(\"completedAt\"))\n",
        "    row_count = _coerce_int(\n",
        "        summary.get(\"row_count\")\n",
        "        or summary.get(\"rowCount\")\n",
        "        or summary.get(\"rows\")\n",
        "        or summary.get(\"total_rows\")\n",
        "        or summary.get(\"totalRows\")\n",
        "    )\n",
        "    anomaly_count = _coerce_int(summary.get(\"anomaly_count\") or summary.get(\"anomalyCount\"))\n",
        "    if anomaly_count is None:\n",
        "        anomalies = summary.get(\"anomalies\")\n",
        "        if isinstance(anomalies, (list, tuple)):\n",
        "            anomaly_count = len(anomalies)\n",
        "\n",
        "    if started_at is None:\n",
        "        started_at = datetime.utcnow()\n",
        "    if completed_at is None:\n",
        "        completed_at = datetime.utcnow()\n",
        "\n",
        "    payload_ref = _first_non_empty(payload_location, summary.get(\"payload_path\"), summary.get(\"payloadPath\"))\n",
        "    profiles_table = _metadata_table(\"dq_profiles\")\n",
        "    profile_literal = _sql_literal(profile_run_id)\n",
        "    spark.sql(\n",
        "        f\"DELETE FROM {profiles_table} WHERE {_escape_identifier('profile_run_id')} = {profile_literal}\"\n",
        "    )\n",
        "\n",
        "    columns = (\n",
        "        \"profile_run_id\",\n",
        "        \"table_group_id\",\n",
        "        \"status\",\n",
        "        \"started_at\",\n",
        "        \"completed_at\",\n",
        "        \"row_count\",\n",
        "        \"anomaly_count\",\n",
        "        \"payload_path\",\n",
        "        \"databricks_run_id\",\n",
        "    )\n",
        "    values = [\n",
        "        _sql_literal(profile_run_id),\n",
        "        _sql_literal(table_group_id),\n",
        "        _sql_literal(status),\n",
        "        _sql_literal(started_at),\n",
        "        _sql_literal(completed_at),\n",
        "        _sql_number(row_count),\n",
        "        _sql_number(anomaly_count),\n",
        "        _sql_literal(payload_ref),\n",
        "        _sql_literal(_resolve_databricks_run_id()),\n",
        "    ]\n",
        "    columns_sql = \", \".join(_escape_identifier(column) for column in columns)\n",
        "    values_sql = \", \".join(values)\n",
        "    spark.sql(f\"INSERT INTO {profiles_table} ({columns_sql}) VALUES ({values_sql})\")\n",
        "\n",
        "    ref_label = payload_ref or \"inline\"\n",
        "    print(\n",
        "        f\"Persisted metadata for profile run {profile_run_id} with status '{status}' and payload reference {ref_label}.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56146ced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final metadata persistence and callback dispatch\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run prior cells before finalization.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"finalization\")\n",
        "\n",
        "\n",
        "def _resolve_results_payload() -> Any:\n",
        "    \"\"\"Pick the richest profiling payload produced by earlier cells.\"\"\"\n",
        "    for name in (\n",
        "        \"results_payload\",\n",
        "        \"profiling_payload\",\n",
        "        \"profile_payload\",\n",
        "        \"profiling_results\",\n",
        "        \"profile_results\",\n",
        "        \"results\",\n",
        "    ):\n",
        "        if name in globals():\n",
        "            return globals()[name]\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_payload_reference() -> str | None:\n",
        "    for candidate in (\n",
        "        globals().get(\"persisted_payload_path\"),\n",
        "        globals().get(\"payload_reference\"),\n",
        "        globals().get(\"payload_location\"),\n",
        "        globals().get(\"payload_artifact_path\"),\n",
        "        payload_path,\n",
        "        raw_payload_path,\n",
        "    ):\n",
        "        if isinstance(candidate, str):\n",
        "            normalized = candidate.strip()\n",
        "            if normalized:\n",
        "                return normalized\n",
        "    return None\n",
        "\n",
        "\n",
        "def _resolve_callback_behavior() -> str:\n",
        "    raw_value = (dbutils.widgets.get(\"callback_behavior\") or DEFAULT_CALLBACK_BEHAVIOR).strip().lower()\n",
        "    if raw_value in {\"none\", \"disabled\", \"off\"}:\n",
        "        return \"none\"\n",
        "    if raw_value in {\"inline\", \"payload\", \"inline_payload\", \"payload_inline\"}:\n",
        "        return \"inline\"\n",
        "    if raw_value in {\"artifact\", \"artifact_only\"}:\n",
        "        return \"artifact\"\n",
        "    if raw_value in {\"metadata\", \"metadata_only\"}:\n",
        "        return \"metadata_only\"\n",
        "    return DEFAULT_CALLBACK_BEHAVIOR\n",
        "\n",
        "\n",
        "def _resolve_callback_target() -> str | None:\n",
        "    if callback_url:\n",
        "        return callback_url\n",
        "    if callback_base_url and profile_run_id:\n",
        "        base = callback_base_url.rstrip(\"/\")\n",
        "        return f\"{base}/{profile_run_id}\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def _post_callback(target_url: str, payload: dict[str, Any]) -> dict[str, Any]:\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    if callback_token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {callback_token}\"\n",
        "    response = requests.post(target_url, headers=headers, json=payload, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    return {\"status_code\": response.status_code, \"text\": response.text[:512]}\n",
        "\n",
        "\n",
        "def _post_callback_with_retry(target_url: str, payload: dict[str, Any]) -> dict[str, Any]:\n",
        "    try:\n",
        "        return _post_callback(target_url, payload)\n",
        "    except requests.RequestException as exc:\n",
        "        if _looks_like_dns_failure(exc):\n",
        "            fallback = _rewrite_heroku_app_host(target_url)\n",
        "            if fallback and fallback != target_url:\n",
        "                print(f\"Retrying callback via canonical host: {fallback}\")\n",
        "                return _post_callback(fallback, payload)\n",
        "        raise\n",
        "\n",
        "\n",
        "resolved_storage_mode = _resolve_payload_storage_mode()\n",
        "results_payload = _resolve_results_payload()\n",
        "results_summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
        "status = _first_non_empty(results_summary.get(\"status\"), results_summary.get(\"state\"), \"completed\")\n",
        "payload_reference = _resolve_payload_reference()\n",
        "if not payload_reference and _payload_storage_is_artifact(resolved_storage_mode):\n",
        "    payload_reference = payload_path\n",
        "\n",
        "_persist_results_to_metadata(results_payload, payload_reference)\n",
        "\n",
        "callback_behavior = _resolve_callback_behavior()\n",
        "callback_target = _resolve_callback_target()\n",
        "callback_result = None\n",
        "\n",
        "if callback_target and callback_behavior != \"none\":\n",
        "    callback_payload: dict[str, Any] = {\n",
        "        \"profile_run_id\": profile_run_id,\n",
        "        \"table_group_id\": table_group_id,\n",
        "        \"status\": status,\n",
        "        \"payload_reference\": payload_reference,\n",
        "        \"payload_storage_mode\": resolved_storage_mode,\n",
        "        \"metadata_schema\": dq_schema,\n",
        "        \"summary\": results_summary,\n",
        "        \"databricks_run_id\": _resolve_databricks_run_id(),\n",
        "    }\n",
        "    if callback_behavior == \"inline\" and results_payload is not None:\n",
        "        callback_payload[\"results\"] = results_payload\n",
        "    try:\n",
        "        callback_result = _post_callback_with_retry(callback_target, callback_payload)\n",
        "    except Exception as exc:  # pragma: no cover - surface callback errors without stopping metadata persistence\n",
        "        callback_result = {\"error\": str(exc), \"target\": callback_target}\n",
        "        print(f\"Callback to {callback_target} failed: {exc}\")\n",
        "\n",
        "FINALIZATION_CONTEXT = {\n",
        "    \"profile_run_id\": profile_run_id,\n",
        "    \"table_group_id\": table_group_id,\n",
        "    \"status\": status,\n",
        "    \"payload_reference\": payload_reference,\n",
        "    \"payload_storage_mode\": resolved_storage_mode,\n",
        "    \"callback_behavior\": callback_behavior,\n",
        "    \"callback_target\": callback_target,\n",
        "    \"callback_result\": callback_result,\n",
        "}\n",
        "\n",
        "print(json.dumps(FINALIZATION_CONTEXT, indent=2, sort_keys=True))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
