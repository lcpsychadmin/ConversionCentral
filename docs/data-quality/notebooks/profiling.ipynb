{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"payload_storage\", \"\")\n",
    "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
    "payload_path = raw_payload_path or None\n",
    "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "from datetime import datetime\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from typing import Iterable\n",
    "\n",
    "import datetime as dt\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, BinaryType, MapType, StructType\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "VALUE_DISTRIBUTION_LIMIT = 25\n",
    "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
    "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
    "MAX_VALUE_DISPLAY_LENGTH = 256\n",
    "\n",
    "PROFILE_COLUMN_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"qualified_name\",\n",
    "    \"data_type\",\n",
    "    \"general_type\",\n",
    "    \"ordinal_position\",\n",
    "    \"row_count\",\n",
    "    \"null_count\",\n",
    "    \"non_null_count\",\n",
    "    \"distinct_count\",\n",
    "    \"min_value\",\n",
    "    \"max_value\",\n",
    "    \"avg_value\",\n",
    "    \"stddev_value\",\n",
    "    \"median_value\",\n",
    "    \"p95_value\",\n",
    "    \"true_count\",\n",
    "    \"false_count\",\n",
    "    \"min_length\",\n",
    "    \"max_length\",\n",
    "    \"avg_length\",\n",
    "    \"non_ascii_ratio\",\n",
    "    \"min_date\",\n",
    "    \"max_date\",\n",
    "    \"date_span_days\",\n",
    "    \"metrics_json\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMNS_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"qualified_name\", T.StringType(), True),\n",
    "        T.StructField(\"data_type\", T.StringType(), True),\n",
    "        T.StructField(\"general_type\", T.StringType(), True),\n",
    "        T.StructField(\"ordinal_position\", T.IntegerType(), True),\n",
    "        T.StructField(\"row_count\", T.LongType(), True),\n",
    "        T.StructField(\"null_count\", T.LongType(), True),\n",
    "        T.StructField(\"non_null_count\", T.LongType(), True),\n",
    "        T.StructField(\"distinct_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_value\", T.StringType(), True),\n",
    "        T.StructField(\"max_value\", T.StringType(), True),\n",
    "        T.StructField(\"avg_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"stddev_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"median_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"p95_value\", T.DoubleType(), True),\n",
    "        T.StructField(\"true_count\", T.LongType(), True),\n",
    "        T.StructField(\"false_count\", T.LongType(), True),\n",
    "        T.StructField(\"min_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"max_length\", T.IntegerType(), True),\n",
    "        T.StructField(\"avg_length\", T.DoubleType(), True),\n",
    "        T.StructField(\"non_ascii_ratio\", T.DoubleType(), True),\n",
    "        T.StructField(\"min_date\", T.DateType(), True),\n",
    "        T.StructField(\"max_date\", T.DateType(), True),\n",
    "        T.StructField(\"date_span_days\", T.IntegerType(), True),\n",
    "        T.StructField(\"metrics_json\", T.StringType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PROFILE_COLUMN_VALUES_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"value\",\n",
    "    \"value_hash\",\n",
    "    \"frequency\",\n",
    "    \"relative_freq\",\n",
    "    \"rank\",\n",
    "    \"bucket_label\",\n",
    "    \"bucket_lower_bound\",\n",
    "    \"bucket_upper_bound\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "PROFILE_COLUMN_VALUES_SCHEMA = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"profile_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"schema_name\", T.StringType(), True),\n",
    "        T.StructField(\"table_name\", T.StringType(), False),\n",
    "        T.StructField(\"column_name\", T.StringType(), False),\n",
    "        T.StructField(\"value\", T.StringType(), True),\n",
    "        T.StructField(\"value_hash\", T.StringType(), True),\n",
    "        T.StructField(\"frequency\", T.LongType(), True),\n",
    "        T.StructField(\"relative_freq\", T.DoubleType(), True),\n",
    "        T.StructField(\"rank\", T.IntegerType(), True),\n",
    "        T.StructField(\"bucket_label\", T.StringType(), True),\n",
    "        T.StructField(\"bucket_lower_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"bucket_upper_bound\", T.DoubleType(), True),\n",
    "        T.StructField(\"generated_at\", T.TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def _split_identifier(value: str | None) -> list[str]:\n",
    "    cleaned = (value or \"\").replace(\"`\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    return [segment.strip() for segment in cleaned.split(\".\") if segment.strip()]\n",
    "\n",
    "\n",
    "def _catalog_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _schema_component(value: str | None) -> str | None:\n",
    "    parts = _split_identifier(value)\n",
    "    if not parts:\n",
    "        return None\n",
    "    return parts[-1]\n",
    "\n",
    "\n",
    "def _qualify(*parts: Iterable[str | None]) -> str:\n",
    "    tokens: list[str] = []\n",
    "    for part in parts:\n",
    "        if isinstance(part, (list, tuple)):\n",
    "            tokens.extend([token for token in part if token])\n",
    "        elif part:\n",
    "            tokens.append(part)\n",
    "    if not tokens:\n",
    "        raise ValueError(\"Cannot build a fully qualified identifier with no parts.\")\n",
    "    return \".\".join(f\"`{token}`\" for token in tokens)\n",
    "\n",
    "\n",
    "metadata_catalog = _catalog_component(dq_schema)\n",
    "metadata_schema = _schema_component(dq_schema)\n",
    "if metadata_schema is None:\n",
    "    raise ValueError(\"Unable to resolve schema portion of the data quality schema setting.\")\n",
    "if metadata_catalog is None:\n",
    "    fallback_catalog = _catalog_component(connection_catalog)\n",
    "    if fallback_catalog:\n",
    "        metadata_catalog = fallback_catalog\n",
    "    else:\n",
    "        with suppress(Exception):\n",
    "            metadata_catalog = spark.catalog.currentCatalog()\n",
    "\n",
    "connection_catalog_clean = _catalog_component(connection_catalog)\n",
    "connection_schema_clean = _schema_component(connection_schema)\n",
    "\n",
    "\n",
    "def _metadata_table(name: str) -> str:\n",
    "    return _qualify(metadata_catalog, metadata_schema, name) if metadata_catalog else _qualify(metadata_schema, name)\n",
    "\n",
    "\n",
    "def _compile_patterns(mask: str | None) -> list[re.Pattern[str]]:\n",
    "    if not mask:\n",
    "        return []\n",
    "    tokens = [token.strip() for token in re.split(r\"[\\n,]+\", mask) if token.strip()]\n",
    "    compiled: list[re.Pattern[str]] = []\n",
    "    for token in tokens:\n",
    "        escaped = re.escape(token).replace(\"\\\\*\", \".*\").replace(\"\\\\%\", \".*\")\n",
    "        compiled.append(re.compile(f\"^{escaped}$\", re.IGNORECASE))\n",
    "    return compiled\n",
    "\n",
    "\n",
    "def _matches_pattern(patterns: list[re.Pattern[str]], schema_name: str | None, table_name: str) -> bool:\n",
    "    if not patterns:\n",
    "        return False\n",
    "    candidate_full = \".\".join(filter(None, [(schema_name or \"\").lower(), table_name.lower()])).strip(\".\")\n",
    "    short_name = table_name.lower()\n",
    "    for pattern in patterns:\n",
    "        if pattern.match(candidate_full) or pattern.match(short_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _qualify_data_table(raw_schema: str | None, table_name: str) -> str:\n",
    "    table_tokens = _split_identifier(table_name)\n",
    "    if len(table_tokens) >= 2:\n",
    "        return _qualify(table_tokens)\n",
    "\n",
    "    schema_tokens = _split_identifier(raw_schema)\n",
    "    if len(schema_tokens) >= 2:\n",
    "        return _qualify(schema_tokens + table_tokens)\n",
    "\n",
    "    catalog_part = connection_catalog_clean\n",
    "    schema_part = connection_schema_clean\n",
    "    if len(schema_tokens) == 1:\n",
    "        schema_part = schema_tokens[0]\n",
    "    elif schema_tokens:\n",
    "        schema_part = schema_tokens[-1]\n",
    "\n",
    "    return _qualify(catalog_part, schema_part, table_tokens[0] if table_tokens else table_name)\n",
    "\n",
    "\n",
    "def _select_profile_columns(df) -> list[str]:\n",
    "    allowed: list[str] = []\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, (BinaryType, MapType, ArrayType, StructType)):\n",
    "            continue\n",
    "        allowed.append(field.name)\n",
    "        if len(allowed) >= MAX_COLUMNS_TO_PROFILE:\n",
    "            break\n",
    "    return allowed\n",
    "\n",
    "\n",
    "def _record_anomaly(buffer: list[dict[str, str]], table_name: str, column_name: str | None, anomaly_type: str, severity: str, description: str, detected_at: str) -> None:\n",
    "    buffer.append(\n",
    "        {\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": column_name,\n",
    "            \"anomaly_type\": anomaly_type,\n",
    "            \"severity\": severity,\n",
    "            \"description\": description,\n",
    "            \"detected_at\": detected_at,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def _infer_general_type(data_type: T.DataType | None) -> str:\n",
    "    if isinstance(data_type, T.BooleanType):\n",
    "        return \"B\"\n",
    "    if isinstance(data_type, (T.StringType,)):\n",
    "        return \"A\"\n",
    "    if isinstance(data_type, (T.DateType, T.TimestampType)):\n",
    "        return \"D\"\n",
    "    if isinstance(\n",
    "        data_type,\n",
    "        (\n",
    "            T.ByteType,\n",
    "            T.ShortType,\n",
    "            T.IntegerType,\n",
    "            T.LongType,\n",
    "            T.FloatType,\n",
    "            T.DoubleType,\n",
    "            T.DecimalType,\n",
    "        ),\n",
    "    ):\n",
    "        return \"N\"\n",
    "    return \"X\"\n",
    "\n",
    "\n",
    "def _stringify_metric_value(value: object) -> str | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, (dt.datetime, dt.date)):\n",
    "        return value.isoformat()\n",
    "    if isinstance(value, bytes):\n",
    "        return value.hex()\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _safe_float(value: object) -> float | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        numeric_value = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "    if math.isnan(numeric_value) or math.isinf(numeric_value):\n",
    "        return None\n",
    "    return numeric_value\n",
    "\n",
    "\n",
    "def _approx_distinct_count(df, column_name: str) -> int | None:\n",
    "    try:\n",
    "        result = df.select(F.approx_count_distinct(F.col(column_name)).alias(\"distinct_count\")).collect()[0]\n",
    "        value = result.get(\"distinct_count\")\n",
    "        return int(value) if value is not None else None\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"approx_count_distinct failed for column {column_name}: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _numeric_stats(df, column_name: str) -> tuple[str | None, str | None, float | None, float | None, float | None, float | None]:\n",
    "    numeric_col = F.col(column_name).cast(\"double\")\n",
    "    try:\n",
    "        stats_row = df.select(\n",
    "            F.min(numeric_col).alias(\"min_value\"),\n",
    "            F.max(numeric_col).alias(\"max_value\"),\n",
    "            F.avg(numeric_col).alias(\"avg_value\"),\n",
    "            F.stddev_pop(numeric_col).alias(\"stddev_value\"),\n",
    "            F.percentile_approx(numeric_col, [0.5, 0.95], 1000).alias(\"percentiles\"),\n",
    "        ).collect()[0]\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Numeric metric collection failed for column {column_name}: {exc}\")\n",
    "        return (None, None, None, None, None, None)\n",
    "\n",
    "    min_value = _stringify_metric_value(stats_row.get(\"min_value\"))\n",
    "    max_value = _stringify_metric_value(stats_row.get(\"max_value\"))\n",
    "    avg_value = _safe_float(stats_row.get(\"avg_value\"))\n",
    "    stddev_value = _safe_float(stats_row.get(\"stddev_value\"))\n",
    "    percentiles = stats_row.get(\"percentiles\") or []\n",
    "    median_value = _safe_float(percentiles[0]) if len(percentiles) > 0 else None\n",
    "    p95_value = _safe_float(percentiles[1]) if len(percentiles) > 1 else None\n",
    "    return (min_value, max_value, avg_value, stddev_value, median_value, p95_value)\n",
    "\n",
    "\n",
    "def _string_length_stats(df, column_name: str, row_count: int) -> tuple[int | None, int | None, float | None, float | None]:\n",
    "    try:\n",
    "        length_col = F.length(F.col(column_name))\n",
    "        stats_row = df.select(\n",
    "            F.min(length_col).alias(\"min_length\"),\n",
    "            F.max(length_col).alias(\"max_length\"),\n",
    "            F.avg(length_col.cast(\"double\")).alias(\"avg_length\"),\n",
    "            F.sum(F.when(F.col(column_name).cast(\"string\").rlike(r\"[^\\u0000-\\u007F]\"), 1).otherwise(0)).alias(\"non_ascii_count\"),\n",
    "        ).collect()[0]\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"String metric collection failed for column {column_name}: {exc}\")\n",
    "        return (None, None, None, None)\n",
    "\n",
    "    min_length = stats_row.get(\"min_length\")\n",
    "    max_length = stats_row.get(\"max_length\")\n",
    "    avg_length = _safe_float(stats_row.get(\"avg_length\"))\n",
    "    non_ascii_count = stats_row.get(\"non_ascii_count\")\n",
    "    non_ascii_ratio = None\n",
    "    if non_ascii_count is not None and row_count:\n",
    "        non_ascii_ratio = float(non_ascii_count) / float(row_count)\n",
    "    return (\n",
    "        int(min_length) if min_length is not None else None,\n",
    "        int(max_length) if max_length is not None else None,\n",
    "        avg_length,\n",
    "        non_ascii_ratio,\n",
    "    )\n",
    "\n",
    "\n",
    "def _boolean_stats(df, column_name: str) -> tuple[int | None, int | None]:\n",
    "    bool_col = F.col(column_name).cast(\"boolean\")\n",
    "    try:\n",
    "        stats_row = df.select(\n",
    "            F.sum(F.when(bool_col == F.lit(True), 1).otherwise(0)).alias(\"true_count\"),\n",
    "            F.sum(F.when(bool_col == F.lit(False), 1).otherwise(0)).alias(\"false_count\"),\n",
    "        ).collect()[0]\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Boolean metric collection failed for column {column_name}: {exc}\")\n",
    "        return (None, None)\n",
    "\n",
    "    true_count = stats_row.get(\"true_count\")\n",
    "    false_count = stats_row.get(\"false_count\")\n",
    "    return (\n",
    "        int(true_count) if true_count is not None else None,\n",
    "        int(false_count) if false_count is not None else None,\n",
    "    )\n",
    "\n",
    "\n",
    "def _date_stats(df, column_name: str) -> tuple[dt.date | None, dt.date | None, int | None]:\n",
    "    date_col = F.to_date(F.col(column_name))\n",
    "    try:\n",
    "        stats_row = df.select(\n",
    "            F.min(date_col).alias(\"min_date\"),\n",
    "            F.max(date_col).alias(\"max_date\"),\n",
    "        ).collect()[0]\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Date metric collection failed for column {column_name}: {exc}\")\n",
    "        return (None, None, None)\n",
    "\n",
    "    min_date = stats_row.get(\"min_date\")\n",
    "    max_date = stats_row.get(\"max_date\")\n",
    "    if isinstance(min_date, dt.datetime):\n",
    "        min_date = min_date.date()\n",
    "    if isinstance(max_date, dt.datetime):\n",
    "        max_date = max_date.date()\n",
    "    date_span_days = None\n",
    "    if min_date and max_date:\n",
    "        date_span_days = (max_date - min_date).days\n",
    "    return (min_date, max_date, int(date_span_days) if date_span_days is not None else None)\n",
    "\n",
    "\n",
    "def _should_capture_value_distribution(general_type: str, row_count: int, distinct_count: int | None) -> bool:\n",
    "    if not row_count or row_count > VALUE_DISTRIBUTION_MAX_ROWS:\n",
    "        return False\n",
    "    if general_type in {\"A\", \"B\"}:\n",
    "        return True\n",
    "    if general_type in {\"N\", \"D\"} and distinct_count is not None and distinct_count <= VALUE_DISTRIBUTION_DISTINCT_THRESHOLD:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _render_value_display_and_hash(value: object) -> tuple[str | None, str]:\n",
    "    if value is None:\n",
    "        sentinel = \"__NULL__\"\n",
    "        digest = hashlib.sha256(sentinel.encode(\"utf-8\")).hexdigest()\n",
    "        return (\"NULL\", digest)\n",
    "    if isinstance(value, bytes):\n",
    "        full_text = value.hex()\n",
    "    elif isinstance(value, (dt.datetime, dt.date)):\n",
    "        full_text = value.isoformat()\n",
    "    else:\n",
    "        full_text = str(value)\n",
    "    display_text = full_text\n",
    "    if len(display_text) > MAX_VALUE_DISPLAY_LENGTH:\n",
    "        display_text = f\"{display_text[: MAX_VALUE_DISPLAY_LENGTH - 3]}...\"\n",
    "    digest = hashlib.sha256(full_text.encode(\"utf-8\")).hexdigest()\n",
    "    return (display_text, digest)\n",
    "\n",
    "\n",
    "def _collect_value_distribution_rows(\n",
    "    df,\n",
    "    column_name: str,\n",
    "    schema_name: str | None,\n",
    "    table_name: str,\n",
    "    run_id: str,\n",
    "    row_count: int,\n",
    ") -> list[dict[str, object]]:\n",
    "    rows: list[dict[str, object]] = []\n",
    "    if not row_count:\n",
    "        return rows\n",
    "    try:\n",
    "        freq_rows = (\n",
    "            df.groupBy(F.col(column_name))\n",
    "            .agg(F.count(F.lit(1)).alias(\"frequency\"))\n",
    "            .orderBy(F.desc(\"frequency\"))\n",
    "            .limit(VALUE_DISTRIBUTION_LIMIT)\n",
    "            .collect()\n",
    "        )\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Value distribution query failed for column {column_name}: {exc}\")\n",
    "        return rows\n",
    "\n",
    "    for idx, freq_row in enumerate(freq_rows, start=1):\n",
    "        frequency_value = freq_row.get(\"frequency\")\n",
    "        raw_value = freq_row.get(column_name)\n",
    "        display_value, value_hash = _render_value_display_and_hash(raw_value)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"profile_run_id\": run_id,\n",
    "                \"schema_name\": schema_name,\n",
    "                \"table_name\": table_name,\n",
    "                \"column_name\": column_name,\n",
    "                \"value\": display_value,\n",
    "                \"value_hash\": value_hash,\n",
    "                \"frequency\": int(frequency_value) if frequency_value is not None else None,\n",
    "                \"relative_freq\": (float(frequency_value) / float(row_count)) if frequency_value is not None and row_count else None,\n",
    "                \"rank\": idx,\n",
    "                \"bucket_label\": None,\n",
    "                \"bucket_lower_bound\": None,\n",
    "                \"bucket_upper_bound\": None,\n",
    "                \"generated_at\": datetime.utcnow(),\n",
    "            }\n",
    "        )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _quote_sql_literal(value: str | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    escaped = value.replace(\"'\", \"''\")\n",
    "    return f\"'{escaped}'\"\n",
    "\n",
    "\n",
    "def _replace_profile_run_rows(table_name: str, run_id: str, frame) -> None:\n",
    "    literal = _quote_sql_literal(run_id)\n",
    "    spark.sql(f\"DELETE FROM {table_name} WHERE profile_run_id = {literal}\")\n",
    "    frame.select(*frame.columns).write.insertInto(table_name, overwrite=False)\n",
    "\n",
    "\n",
    "def _persist_profile_detail_tables(run_id: str, column_rows: list[dict[str, object]], value_rows: list[dict[str, object]]) -> None:\n",
    "    if not column_rows and not value_rows:\n",
    "        print(\"No column metrics were captured; skipping dq_profile_columns persistence.\")\n",
    "        return\n",
    "    try:\n",
    "        columns_table = _metadata_table(\"dq_profile_columns\")\n",
    "        values_table = _metadata_table(\"dq_profile_column_values\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Unable to resolve metadata tables for column metrics: {exc}\")\n",
    "        return\n",
    "\n",
    "    if column_rows:\n",
    "        try:\n",
    "            column_df = spark.createDataFrame(column_rows, PROFILE_COLUMNS_SCHEMA).select(*PROFILE_COLUMN_FIELDS)\n",
    "            _replace_profile_run_rows(columns_table, run_id, column_df)\n",
    "            print(f\"Persisted {len(column_rows)} rows to {columns_table}.\")\n",
    "        except AnalysisException as exc:\n",
    "            print(f\"dq_profile_columns is unavailable; skipping column metric persistence: {exc}\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"Failed to persist dq_profile_columns: {exc}\")\n",
    "\n",
    "    if value_rows:\n",
    "        try:\n",
    "            value_df = spark.createDataFrame(value_rows, PROFILE_COLUMN_VALUES_SCHEMA).select(*PROFILE_COLUMN_VALUES_FIELDS)\n",
    "            _replace_profile_run_rows(values_table, run_id, value_df)\n",
    "            print(f\"Persisted {len(value_rows)} rows to {values_table}.\")\n",
    "        except AnalysisException as exc:\n",
    "            print(f\"dq_profile_column_values is unavailable; skipping value distribution persistence: {exc}\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"Failed to persist dq_profile_column_values: {exc}\")\n",
    "\n",
    "\n",
    "metadata_tables_name = _metadata_table(\"dq_tables\")\n",
    "group_table_name = _metadata_table(\"dq_table_groups\")\n",
    "group_rows = (\n",
    "    spark.table(group_table_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"name\", \"profiling_include_mask\", \"profiling_exclude_mask\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "if not group_rows:\n",
    "    raise ValueError(f\"Table group '{table_group_id}' not found in schema '{dq_schema}'.\")\n",
    "group_details = group_rows[0].asDict()\n",
    "include_patterns = _compile_patterns(group_details.get(\"profiling_include_mask\"))\n",
    "exclude_patterns = _compile_patterns(group_details.get(\"profiling_exclude_mask\"))\n",
    "\n",
    "table_rows = (\n",
    "    spark.table(metadata_tables_name)\n",
    "    .where(F.col(\"table_group_id\") == table_group_id)\n",
    "    .select(\"schema_name\", \"table_name\")\n",
    "    .collect()\n",
    ")\n",
    "if not table_rows:\n",
    "    raise ValueError(f\"No dq_tables rows registered for table_group_id '{table_group_id}'.\")\n",
    "\n",
    "table_candidates: list[dict[str, str]] = []\n",
    "for row in table_rows:\n",
    "    schema_value = (row[\"schema_name\"] or connection_schema_clean or \"\").strip() or None\n",
    "    table_value = (row[\"table_name\"] or \"\").strip()\n",
    "    if not table_value:\n",
    "        continue\n",
    "    if include_patterns and not _matches_pattern(include_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    if exclude_patterns and _matches_pattern(exclude_patterns, schema_value, table_value):\n",
    "        continue\n",
    "    label = \".\".join(filter(None, [schema_value, table_value])) or table_value\n",
    "    table_candidates.append({\"schema_name\": schema_value, \"table_name\": table_value, \"label\": label})\n",
    "\n",
    "if not table_candidates:\n",
    "    raise ValueError(\"All candidate tables were filtered out by include/exclude masks.\")\n",
    "\n",
    "generated_at = datetime.utcnow().isoformat() + \"Z\"\n",
    "anomalies: list[dict[str, str]] = []\n",
    "table_profiles: list[dict[str, object]] = []\n",
    "total_rows = 0\n",
    "profiling_failures = 0\n",
    "profiling_successes = 0\n",
    "column_metric_rows: list[dict[str, object]] = []\n",
    "value_distribution_rows: list[dict[str, object]] = []\n",
    "\n",
    "print(f\"Profiling {len(table_candidates)} tables for group {table_group_id}.\")\n",
    "for candidate in table_candidates:\n",
    "    schema_value = candidate[\"schema_name\"]\n",
    "    table_value = candidate[\"table_name\"]\n",
    "    label = candidate[\"label\"]\n",
    "    qualified_name = _qualify_data_table(schema_value, table_value)\n",
    "    table_result: dict[str, object] = {\n",
    "        \"table_name\": label,\n",
    "        \"table\": label,\n",
    "        \"name\": label,\n",
    "        \"qualified_name\": qualified_name,\n",
    "    }\n",
    "    print(f\"-> Scanning {qualified_name}\")\n",
    "    df = None\n",
    "    try:\n",
    "        df = spark.read.table(qualified_name).cache()\n",
    "    except AnalysisException as exc:\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"missing_table\", \"high\", f\"Spark could not read {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        profiling_failures += 1\n",
    "        table_result[\"error\"] = str(exc)\n",
    "        _record_anomaly(anomalies, label, None, \"profiling_error\", \"high\", f\"Unexpected error while reading {qualified_name}: {exc}\", generated_at)\n",
    "        table_profiles.append(table_result)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        row_count = int(df.count())\n",
    "        table_result[\"row_count\"] = row_count\n",
    "        total_rows += row_count\n",
    "\n",
    "        if row_count == 0:\n",
    "            profiling_failures += 1\n",
    "            _record_anomaly(anomalies, label, None, \"empty_table\", \"high\", \"Table returned zero rows during profiling.\", generated_at)\n",
    "            table_profiles.append(table_result)\n",
    "            continue\n",
    "\n",
    "        profiling_successes += 1\n",
    "        profile_columns = _select_profile_columns(df)\n",
    "        table_result[\"profiled_columns\"] = profile_columns\n",
    "        full_column_count = len(df.columns)\n",
    "        if full_column_count > len(profile_columns):\n",
    "            table_result[\"profiled_columns_truncated\"] = full_column_count - len(profile_columns)\n",
    "\n",
    "        column_profiles: list[dict[str, object]] = []\n",
    "        column_null_ratios: dict[str, float] = {}\n",
    "\n",
    "        if profile_columns:\n",
    "            agg_exprs = [F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0)).alias(col_name) for col_name in profile_columns]\n",
    "            null_counts = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "            schema_field_map = {field.name: field for field in df.schema.fields}\n",
    "            schema_field_positions = {field.name: idx + 1 for idx, field in enumerate(df.schema.fields)}\n",
    "            column_schema_name = schema_value or connection_schema_clean or None\n",
    "\n",
    "            for column in profile_columns:\n",
    "                null_count = int(null_counts.get(column, 0) or 0)\n",
    "                null_ratio = float(null_count / row_count) if row_count else 0.0\n",
    "                column_null_ratios[column] = null_ratio\n",
    "\n",
    "                spark_field = schema_field_map.get(column)\n",
    "                data_type = spark_field.dataType if spark_field else None\n",
    "                data_type_label = data_type.simpleString() if data_type else None\n",
    "                general_type = _infer_general_type(data_type)\n",
    "                ordinal_position = schema_field_positions.get(column)\n",
    "                non_null_count = int(max(row_count - null_count, 0))\n",
    "                distinct_count = _approx_distinct_count(df, column)\n",
    "\n",
    "                min_value = max_value = avg_value = stddev_value = median_value = p95_value = None\n",
    "                true_count = false_count = None\n",
    "                min_length = max_length = None\n",
    "                avg_length = None\n",
    "                non_ascii_ratio = None\n",
    "                min_date = max_date = None\n",
    "                date_span_days = None\n",
    "\n",
    "                if general_type == \"N\" and non_null_count:\n",
    "                    (\n",
    "                        min_value,\n",
    "                        max_value,\n",
    "                        avg_value,\n",
    "                        stddev_value,\n",
    "                        median_value,\n",
    "                        p95_value,\n",
    "                    ) = _numeric_stats(df, column)\n",
    "\n",
    "                if general_type == \"A\" and non_null_count:\n",
    "                    min_length, max_length, avg_length, non_ascii_ratio = _string_length_stats(df, column, row_count)\n",
    "\n",
    "                if general_type == \"B\" and non_null_count:\n",
    "                    true_count, false_count = _boolean_stats(df, column)\n",
    "\n",
    "                if general_type == \"D\" and non_null_count:\n",
    "                    min_date, max_date, date_span_days = _date_stats(df, column)\n",
    "\n",
    "                sampled_value_rows: list[dict[str, object]] = []\n",
    "                if _should_capture_value_distribution(general_type, row_count, distinct_count):\n",
    "                    sampled_value_rows = _collect_value_distribution_rows(\n",
    "                        df,\n",
    "                        column,\n",
    "                        column_schema_name,\n",
    "                        label,\n",
    "                        profile_run_id,\n",
    "                        row_count,\n",
    "                    )\n",
    "                    value_distribution_rows.extend(sampled_value_rows)\n",
    "\n",
    "                metrics_metadata: dict[str, object] = {}\n",
    "                if sampled_value_rows:\n",
    "                    metrics_metadata[\"value_distribution_sampled\"] = len(sampled_value_rows)\n",
    "\n",
    "                column_metric_rows.append(\n",
    "                    {\n",
    "                        \"profile_run_id\": profile_run_id,\n",
    "                        \"schema_name\": column_schema_name,\n",
    "                        \"table_name\": label,\n",
    "                        \"column_name\": column,\n",
    "                        \"qualified_name\": f\"{qualified_name}.{column}\",\n",
    "                        \"data_type\": data_type_label,\n",
    "                        \"general_type\": general_type,\n",
    "                        \"ordinal_position\": ordinal_position,\n",
    "                        \"row_count\": row_count,\n",
    "                        \"null_count\": null_count,\n",
    "                        \"non_null_count\": non_null_count,\n",
    "                        \"distinct_count\": distinct_count,\n",
    "                        \"min_value\": min_value,\n",
    "                        \"max_value\": max_value,\n",
    "                        \"avg_value\": avg_value,\n",
    "                        \"stddev_value\": stddev_value,\n",
    "                        \"median_value\": median_value,\n",
    "                        \"p95_value\": p95_value,\n",
    "                        \"true_count\": true_count,\n",
    "                        \"false_count\": false_count,\n",
    "                        \"min_length\": min_length,\n",
    "                        \"max_length\": max_length,\n",
    "                        \"avg_length\": avg_length,\n",
    "                        \"non_ascii_ratio\": non_ascii_ratio,\n",
    "                        \"min_date\": min_date,\n",
    "                        \"max_date\": max_date,\n",
    "                        \"date_span_days\": date_span_days,\n",
    "                        \"metrics_json\": json.dumps(metrics_metadata) if metrics_metadata else None,\n",
    "                        \"generated_at\": datetime.utcnow(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                metrics_summary = {\n",
    "                    \"row_count\": row_count,\n",
    "                    \"null_count\": null_count,\n",
    "                    \"non_null_count\": non_null_count,\n",
    "                    \"null_ratio\": null_ratio,\n",
    "                    \"distinct_count\": distinct_count,\n",
    "                    \"general_type\": general_type,\n",
    "                    \"min_value\": min_value,\n",
    "                    \"max_value\": max_value,\n",
    "                    \"avg_value\": avg_value,\n",
    "                    \"stddev_value\": stddev_value,\n",
    "                    \"median_value\": median_value,\n",
    "                    \"p95_value\": p95_value,\n",
    "                    \"true_count\": true_count,\n",
    "                    \"false_count\": false_count,\n",
    "                    \"min_length\": min_length,\n",
    "                    \"max_length\": max_length,\n",
    "                    \"avg_length\": avg_length,\n",
    "                    \"non_ascii_ratio\": non_ascii_ratio,\n",
    "                    \"min_date\": _stringify_metric_value(min_date),\n",
    "                    \"max_date\": _stringify_metric_value(max_date),\n",
    "                    \"date_span_days\": date_span_days,\n",
    "                }\n",
    "                if sampled_value_rows:\n",
    "                    metrics_summary[\"value_distribution_sampled\"] = len(sampled_value_rows)\n",
    "\n",
    "                column_profiles.append(\n",
    "                    {\n",
    "                        \"column_name\": column,\n",
    "                        \"column\": column,\n",
    "                        \"name\": column,\n",
    "                        \"data_type\": data_type_label,\n",
    "                        \"general_type\": general_type,\n",
    "                        \"ordinal_position\": ordinal_position,\n",
    "                        \"row_count\": row_count,\n",
    "                        \"null_count\": null_count,\n",
    "                        \"non_null_count\": non_null_count,\n",
    "                        \"null_ratio\": null_ratio,\n",
    "                        \"metrics\": metrics_summary,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if null_ratio >= NULL_RATIO_ALERT_THRESHOLD:\n",
    "                    severity = \"high\" if null_ratio >= HIGH_NULL_RATIO_THRESHOLD else \"medium\"\n",
    "                    description = f\"Null ratio {null_ratio:.2%} exceeds {NULL_RATIO_ALERT_THRESHOLD:.0%} threshold.\"\n",
    "                    _record_anomaly(anomalies, label, column, \"null_ratio\", severity, description, generated_at)\n",
    "\n",
    "            table_result[\"column_null_ratios\"] = column_null_ratios\n",
    "\n",
    "        table_result[\"column_profiles\"] = column_profiles\n",
    "        table_result[\"columns\"] = column_profiles\n",
    "        table_profiles.append(table_result)\n",
    "    finally:\n",
    "        with suppress(Exception):\n",
    "            if df is not None:\n",
    "                df.unpersist()\n",
    "\n",
    "_persist_profile_detail_tables(profile_run_id, column_metric_rows, value_distribution_rows)\n",
    "\n",
    "status = \"completed\" if profiling_successes else \"failed\"\n",
    "results = {\n",
    "    \"table_group_id\": table_group_id,\n",
    "    \"profile_run_id\": profile_run_id,\n",
    "    \"table_group_name\": group_details.get(\"name\"),\n",
    "    \"status\": status,\n",
    "    \"row_count\": int(total_rows),\n",
    "    \"anomaly_count\": len(anomalies),\n",
    "    \"anomalies\": anomalies,\n",
    "    \"generated_at\": generated_at,\n",
    "    \"table_profiles\": table_profiles,\n",
    "    \"diagnostics\": {\n",
    "        \"tables_requested\": len(table_rows),\n",
    "        \"tables_profiled\": profiling_successes,\n",
    "        \"tables_failed\": profiling_failures,\n",
    "        \"include_mask_applied\": bool(include_patterns),\n",
    "        \"exclude_mask_applied\": bool(exclude_patterns),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Profiling complete: {profiling_successes} succeeded, {profiling_failures} failed, total rows={total_rows}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload and call back into the API\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import socket\n",
    "from contextlib import suppress\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
    "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
    "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
    "\n",
    "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
    "\n",
    "\n",
    "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
    "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
    "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
    "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
    "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
    "\n",
    "\n",
    "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
    "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
    "    current = error\n",
    "    while current:\n",
    "        if isinstance(current, socket.gaierror):\n",
    "            return True\n",
    "        name = current.__class__.__name__.lower()\n",
    "        if \"nameresolution\" in name:\n",
    "            return True\n",
    "        message = str(current).lower()\n",
    "        if \"temporary failure in name resolution\" in message:\n",
    "            return True\n",
    "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
    "    return False\n",
    "\n",
    "\n",
    "def _rewrite_heroku_app_host(url: str | None) -> str | None:\n",
    "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.hostname\n",
    "    if not host:\n",
    "        return None\n",
    "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
    "    if not match:\n",
    "        return None\n",
    "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
    "    netloc = canonical_host\n",
    "    if parsed.port:\n",
    "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
    "    if parsed.username:\n",
    "        auth = parsed.username\n",
    "        if parsed.password:\n",
    "            auth = f\"{auth}:{parsed.password}\"\n",
    "        netloc = f\"{auth}@{netloc}\"\n",
    "    scheme = parsed.scheme or \"https\"\n",
    "    if scheme.lower() == \"http\":\n",
    "        scheme = \"https\"\n",
    "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))\n",
    "\n",
    "\n",
    "def _is_dbfs_path(path: str | None) -> bool:\n",
    "    return bool(path and path.lower().startswith(\"dbfs:/\"))\n",
    "\n",
    "\n",
    "def _has_uri_scheme(value: str | None) -> bool:\n",
    "    return bool(value and URI_SCHEME_PATTERN.match(value.strip()))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _dbfs_root_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/_dbfs_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DBFS_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _driver_fs_is_disabled() -> bool:\n",
    "    probe_path = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/_driver_access_probe\"\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(probe_path)\n",
    "        dbutils.fs.rm(probe_path, True)\n",
    "        return False\n",
    "    except Exception as exc:  # noqa: BLE001 - Databricks surfaces JVM errors generically\n",
    "        message = str(exc).lower()\n",
    "        return any(fragment in message for fragment in DRIVER_DISABLED_MESSAGES)\n",
    "\n",
    "\n",
    "def _warn_storage_disabled(message: str) -> None:\n",
    "    global _STORAGE_DISABLED_NOTICE_EMITTED\n",
    "    if not _STORAGE_DISABLED_NOTICE_EMITTED:\n",
    "        print(message)\n",
    "        _STORAGE_DISABLED_NOTICE_EMITTED = True\n",
    "\n",
    "\n",
    "def _redirect_dbfs_path(path: str) -> str | None:\n",
    "    global _DBFS_REDIRECT_NOTICE_EMITTED\n",
    "    if not _is_dbfs_path(path):\n",
    "        return path\n",
    "    if not _dbfs_root_is_disabled():\n",
    "        return path\n",
    "    if _driver_fs_is_disabled():\n",
    "        _warn_storage_disabled(\n",
    "            \"DBFS root access and driver filesystem writes are both disabled; payload artifacts will be skipped unless \"\n",
    "            \"a cloud storage payload_base_path is provided.\"\n",
    "        )\n",
    "        return None\n",
    "    if not _DBFS_REDIRECT_NOTICE_EMITTED:\n",
    "        print(\n",
    "            \"DBFS root access is disabled on this workspace; persisting profiling artifacts to the driver filesystem \"\n",
    "            \"instead.\"\n",
    "        )\n",
    "        _DBFS_REDIRECT_NOTICE_EMITTED = True\n",
    "    suffix = path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    redirected = f\"{DEFAULT_DRIVER_PAYLOAD_ROOT}/{suffix}\" if suffix else DEFAULT_DRIVER_PAYLOAD_ROOT\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _mkdirs_if_supported(target_path: str) -> None:\n",
    "    lowered = target_path.lower()\n",
    "    if lowered.startswith(\"dbfs:/\") and _dbfs_root_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"file:/\") and _driver_fs_is_disabled():\n",
    "        return\n",
    "    if lowered.startswith(\"dbfs:/\") or lowered.startswith(\"file:/\"):\n",
    "        parent_dir = target_path.rsplit(\"/\", 1)[0]\n",
    "        dbutils.fs.mkdirs(parent_dir)\n",
    "\n",
    "\n",
    "def _ensure_https_base_url(value: str) -> str:\n",
    "    normalized = (value or \"\").strip()\n",
    "    if not normalized:\n",
    "        return normalized\n",
    "    parsed = urlparse(normalized)\n",
    "    if not parsed.scheme:\n",
    "        normalized = f\"https://{normalized.lstrip('/')}\"\n",
    "        parsed = urlparse(normalized)\n",
    "    if parsed.scheme.lower() == \"http\":\n",
    "        parsed = parsed._replace(scheme=\"https\")\n",
    "    normalized = urlunparse(parsed).rstrip(\"/\")\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _lookup_metadata_setting(setting_key: str) -> str | None:\n",
    "    normalized_key = (setting_key or \"\").strip().lower()\n",
    "    if not normalized_key:\n",
    "        return None\n",
    "    try:\n",
    "        settings_table = _metadata_table(\"dq_settings\")\n",
    "    except NameError:\n",
    "        return None\n",
    "    try:\n",
    "        row = (\n",
    "            spark.table(settings_table)\n",
    "            .where(F.lower(F.col(\"key\")) == normalized_key)\n",
    "            .select(\"value\")\n",
    "            .limit(1)\n",
    "            .collect()\n",
    "        )\n",
    "    except AnalysisException:\n",
    "        return None\n",
    "    if not row:\n",
    "        return None\n",
    "    value = row[0].get(\"value\")\n",
    "    return value.strip() if isinstance(value, str) and value.strip() else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_payload_storage_mode(value: str | None) -> str | None:\n",
    "    normalized = (value or \"\").strip().lower()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if normalized in {\"inline\", \"database\", \"db\"}:\n",
    "        return \"inline\"\n",
    "    if normalized in {\"artifact\", \"artifacts\", \"file\", \"files\", \"path\", \"paths\", \"dbfs\", \"cloud\"}:\n",
    "        return \"artifact\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_payload_storage_mode() -> str:\n",
    "    widget_choice = _normalize_payload_storage_mode(dbutils.widgets.get(\"payload_storage\"))\n",
    "    if widget_choice:\n",
    "        return widget_choice\n",
    "    setting_choice = _normalize_payload_storage_mode(_lookup_metadata_setting(\"profile_payload_storage_mode\"))\n",
    "    if setting_choice:\n",
    "        return setting_choice\n",
    "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
    "\n",
    "\n",
    "def _payload_storage_is_artifact(mode: str) -> bool:\n",
    "    return (mode or \"\").strip().lower() == \"artifact\"\n",
    "\n",
    "\n",
    "def _encode_payload_json(payload: dict[str, object]) -> str | None:\n",
    "    try:\n",
    "        return json.dumps(payload, separators=(\",\", \":\"))\n",
    "    except TypeError as exc:\n",
    "        print(f\"Unable to serialize profiling payload: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _resolve_callback_behavior() -> str:\n",
    "    widget_value = (dbutils.widgets.get(\"callback_behavior\") or \"\").strip().lower()\n",
    "    if widget_value:\n",
    "        return widget_value\n",
    "    setting_value = (_lookup_metadata_setting(\"profile_callback_behavior\") or \"\").strip().lower()\n",
    "    if setting_value:\n",
    "        return setting_value\n",
    "    return DEFAULT_CALLBACK_BEHAVIOR\n",
    "\n",
    "\n",
    "def _callbacks_enabled(behavior: str) -> bool:\n",
    "    if behavior in {\"api\", \"callback\", \"legacy\"}:\n",
    "        return True\n",
    "    if behavior in {\"metadata_only\", \"metadata\", \"skip\", \"disabled\", \"off\"}:\n",
    "        return False\n",
    "    print(f\"Unknown callback behavior '{behavior}'; defaulting to metadata_only.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sql_string_literal(value: str | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    escaped = str(value).replace(\"'\", \"''\")\n",
    "    return f\"'{escaped}'\"\n",
    "\n",
    "\n",
    "def _sql_numeric_literal(value: int | float | None) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        return str(int(value))\n",
    "    except (TypeError, ValueError):\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "def _normalize_temp_view_name(suffix: str | None) -> str:\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", (suffix or \"profile_run\"))\n",
    "    return f\"_profile_anomalies_{cleaned}\"\n",
    "\n",
    "\n",
    "def _parse_anomaly_timestamp(value: str | None) -> datetime | None:\n",
    "    if not value:\n",
    "        return None\n",
    "    candidate = value.strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.endswith(\"Z\"):\n",
    "        candidate = f\"{candidate[:-1]}+00:00\"\n",
    "    with suppress(ValueError):\n",
    "        return datetime.fromisoformat(candidate)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _persist_results_to_metadata(results_payload: dict[str, object], payload_location: str | None) -> None:\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id widget is required before persisting profiling metadata.\")\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "    anomalies_table = _metadata_table(\"dq_profile_anomalies\")\n",
    "    assignments = [\n",
    "        f\"status = {_sql_string_literal(results_payload.get('status') or 'unknown')}\",\n",
    "        \"completed_at = current_timestamp()\",\n",
    "        f\"row_count = {_sql_numeric_literal(results_payload.get('row_count'))}\",\n",
    "        f\"anomaly_count = {_sql_numeric_literal(results_payload.get('anomaly_count'))}\",\n",
    "        f\"payload_path = {_sql_string_literal(payload_location)}\",\n",
    "    ]\n",
    "    update_sql = (\n",
    "        f\"UPDATE {profiles_table} \"\n",
    "        f\"SET {', '.join(assignments)} \"\n",
    "        f\"WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    )\n",
    "    spark.sql(update_sql)\n",
    "    print(f\"Updated dq_profiles entry for run {profile_run_id}.\")\n",
    "\n",
    "    anomalies = list(results_payload.get(\"anomalies\") or [])\n",
    "    delete_sql = f\"DELETE FROM {anomalies_table} WHERE profile_run_id = {_sql_string_literal(profile_run_id)}\"\n",
    "    spark.sql(delete_sql)\n",
    "\n",
    "    if not anomalies:\n",
    "        print(f\"No anomalies to persist for run {profile_run_id}.\")\n",
    "        return\n",
    "\n",
    "    anomaly_rows = []\n",
    "    for anomaly in anomalies:\n",
    "        anomaly_rows.append(\n",
    "            {\n",
    "                \"profile_run_id\": profile_run_id,\n",
    "                \"table_name\": anomaly.get(\"table_name\"),\n",
    "                \"column_name\": anomaly.get(\"column_name\"),\n",
    "                \"anomaly_type\": anomaly.get(\"anomaly_type\"),\n",
    "                \"severity\": anomaly.get(\"severity\"),\n",
    "                \"description\": anomaly.get(\"description\"),\n",
    "                \"detected_at\": _parse_anomaly_timestamp(anomaly.get(\"detected_at\")) or datetime.utcnow(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    anomalies_df = spark.createDataFrame(anomaly_rows)\n",
    "    view_name = _normalize_temp_view_name(profile_run_id)\n",
    "    try:\n",
    "        anomalies_df.createOrReplaceTempView(view_name)\n",
    "        spark.sql(\n",
    "            f\"INSERT INTO {anomalies_table} \"\n",
    "            \"(profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at) \"\n",
    "            f\"SELECT profile_run_id, table_name, column_name, anomaly_type, severity, description, detected_at FROM {view_name}\"\n",
    "        )\n",
    "    finally:\n",
    "        with suppress(Exception):\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "\n",
    "    print(f\"Persisted {len(anomalies)} anomalies for run {profile_run_id}.\")\n",
    "\n",
    "\n",
    "payload_base_path = payload_base_path or _lookup_metadata_setting(\"profile_payload_base_path\")\n",
    "callback_base_url = callback_base_url or _lookup_metadata_setting(\"profile_callback_base_url\")\n",
    "callback_token = callback_token or _lookup_metadata_setting(\"profile_callback_token\")\n",
    "\n",
    "\n",
    "def _normalize_payload_base(base_path: str | None) -> str | None:\n",
    "    raw_value = (base_path or \"\").strip()\n",
    "    if not raw_value:\n",
    "        redirected = _redirect_dbfs_path(DEFAULT_PRIVATE_PAYLOAD_ROOT)\n",
    "        if not redirected:\n",
    "            _warn_storage_disabled(\n",
    "                \"No writable default payload location detected; configure profile_payload_base_path to point to cloud \"\n",
    "                \"storage accessible from this workspace.\"\n",
    "            )\n",
    "        return redirected\n",
    "    if \"://\" in raw_value and not raw_value.lower().startswith(\"dbfs:/\"):\n",
    "        return raw_value.rstrip(\"/\")\n",
    "    if raw_value.lower().startswith(\"dbfs:/\"):\n",
    "        normalized = raw_value\n",
    "    elif raw_value.startswith(\"/\"):\n",
    "        normalized = f\"dbfs:{raw_value}\"\n",
    "    else:\n",
    "        normalized = f\"dbfs:/tmp/conversioncentral/{raw_value.lstrip('/')}\"\n",
    "    normalized = normalized.rstrip(\"/\")\n",
    "    if normalized.lower().startswith(\"dbfs:/filestore\"):\n",
    "        print(\"FileStore paths are disabled on this workspace; switching to private tmp storage.\")\n",
    "        normalized = DEFAULT_PRIVATE_PAYLOAD_ROOT\n",
    "    redirected = _redirect_dbfs_path(normalized)\n",
    "    if not redirected:\n",
    "        _warn_storage_disabled(\n",
    "            \"The configured payload base path resolves to a blocked filesystem; provide a supported cloud URI instead.\"\n",
    "        )\n",
    "    return redirected\n",
    "\n",
    "\n",
    "def _derive_payload_path(base_path: str | None, group_id: str, run_id: str) -> str | None:\n",
    "    normalized_base = _normalize_payload_base(base_path)\n",
    "    if not normalized_base:\n",
    "        return None\n",
    "    safe_group = (group_id or \"default\").replace(\":\", \"_\")\n",
    "    safe_run = (run_id or \"unknown\").replace(\":\", \"_\")\n",
    "    return f\"{normalized_base}/{safe_group}/{safe_run}.json\"\n",
    "\n",
    "\n",
    "def _normalize_payload_target(path: str | None, group_id: str, run_id: str, base_path: str | None) -> str | None:\n",
    "    candidate = (path or \"\").strip()\n",
    "    if not candidate:\n",
    "        return None\n",
    "    if candidate.lower().startswith(\"dbfs:/\"):\n",
    "        normalized = candidate\n",
    "    elif _has_uri_scheme(candidate):\n",
    "        normalized = candidate.rstrip(\"/\")\n",
    "    elif candidate.startswith(\"/\"):\n",
    "        normalized = f\"dbfs:{candidate}\"\n",
    "    else:\n",
    "        normalized = f\"{DEFAULT_PRIVATE_PAYLOAD_ROOT}/{candidate.lstrip('/')}\"\n",
    "    normalized = normalized.rstrip(\"/\")\n",
    "    lowered = normalized.lower()\n",
    "    if lowered.startswith(\"dbfs:/filestore\"):\n",
    "        print(\"FileStore paths are disabled on this workspace; switching to private tmp storage.\")\n",
    "        derived = _derive_payload_path(base_path, group_id, run_id)\n",
    "        normalized = derived or \"\"\n",
    "    redirected = _redirect_dbfs_path(normalized)\n",
    "    if not redirected:\n",
    "        return None\n",
    "    return redirected.rstrip(\"/\")\n",
    "\n",
    "\n",
    "def _resolve_callback_target(base_url: str | None, run_id: str) -> str | None:\n",
    "    if not base_url:\n",
    "        return None\n",
    "    normalized = base_url.strip()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    if \"{profile_run_id}\" in normalized:\n",
    "        try:\n",
    "            normalized = normalized.format(profile_run_id=run_id)\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "    normalized = _ensure_https_base_url(normalized)\n",
    "    if normalized.endswith(\"/complete\"):\n",
    "        return normalized\n",
    "    return f\"{normalized}/{run_id}/complete\"\n",
    "\n",
    "\n",
    "payload_storage_mode = _resolve_payload_storage_mode()\n",
    "payload_reference: str | None = None\n",
    "payload_json_value = _encode_payload_json(results)\n",
    "\n",
    "if _payload_storage_is_artifact(payload_storage_mode):\n",
    "    artifact_path = payload_path\n",
    "    payload_was_derived = False\n",
    "    if not artifact_path:\n",
    "        artifact_path = _derive_payload_path(payload_base_path, table_group_id, profile_run_id)\n",
    "        payload_was_derived = bool(artifact_path)\n",
    "\n",
    "    normalized_payload_path = _normalize_payload_target(artifact_path, table_group_id, profile_run_id, payload_base_path)\n",
    "    if normalized_payload_path:\n",
    "        if artifact_path and normalized_payload_path != artifact_path:\n",
    "            print(f\"Normalized payload path: {normalized_payload_path}\")\n",
    "        artifact_path = normalized_payload_path\n",
    "    elif not artifact_path:\n",
    "        print(\"Payload storage mode is 'artifact' but no valid path was supplied; inline fallback will be used.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Resolved payload path is blocked by workspace filesystem restrictions; skipping artifact export unless a \"\n",
    "            \"cloud path is provided.\"\n",
    "        )\n",
    "        artifact_path = None\n",
    "\n",
    "    if payload_was_derived and artifact_path:\n",
    "        print(f\"Derived payload artifact path: {artifact_path}\")\n",
    "\n",
    "    if artifact_path:\n",
    "        try:\n",
    "            _mkdirs_if_supported(artifact_path)\n",
    "            dbutils.fs.put(artifact_path, json.dumps(results, indent=2), overwrite=True)\n",
    "            print(f\"Wrote profiling payload to {artifact_path}\")\n",
    "            payload_reference = artifact_path\n",
    "        except Exception as exc:  # noqa: BLE001 - surface full failure for Databricks\n",
    "            print(f\"Failed to write profiling payload to {artifact_path}: {exc}\")\n",
    "            artifact_path = None\n",
    "            print(\"Falling back to inline payload storage.\")\n",
    "    else:\n",
    "        print(\"Skipping artifact export; inline payload will be stored instead.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Payload storage mode set to 'inline'; profiling results will be written directly to dq_profiles.payload_path.\"\n",
    "    )\n",
    "    artifact_path = None\n",
    "\n",
    "if artifact_path and not payload_reference:\n",
    "    payload_reference = artifact_path\n",
    "\n",
    "if not payload_reference:\n",
    "    if payload_json_value is None:\n",
    "        print(\"Unable to capture profiling payload; payload_path column will remain NULL.\")\n",
    "    else:\n",
    "        payload_reference = payload_json_value\n",
    "        print(\"Stored profiling payload inline with the profile metadata entry.\")\n",
    "\n",
    "_persist_results_to_metadata(results, payload_reference)\n",
    "\n",
    "\n",
    "callback_behavior = _resolve_callback_behavior()\n",
    "callback_source_url = callback_url or callback_base_url or _lookup_metadata_setting(\"profile_callback_base_url\")\n",
    "callback_target = _resolve_callback_target(callback_source_url, profile_run_id)\n",
    "if not _callbacks_enabled(callback_behavior):\n",
    "    print(\n",
    "        \"Skipping completion callback; profiling results were written directly to metadata tables. \"\n",
    "        \"Set callback_behavior='api' to re-enable HTTP callbacks.\"\n",
    "    )\n",
    "elif callback_target:\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    token_value = callback_token or _lookup_metadata_setting(\"profile_callback_token\")\n",
    "    if token_value:\n",
    "        headers[\"Authorization\"] = f\"Bearer {token_value}\"\n",
    "    callback_body = {\n",
    "        \"status\": results[\"status\"],\n",
    "        \"row_count\": results[\"row_count\"],\n",
    "        \"anomaly_count\": results[\"anomaly_count\"],\n",
    "        \"anomalies\": results[\"anomalies\"],\n",
    "    }\n",
    "    canonical_fallback = _rewrite_heroku_app_host(callback_target)\n",
    "    callback_candidates = [callback_target]\n",
    "    if canonical_fallback and canonical_fallback not in callback_candidates:\n",
    "        callback_candidates.append(canonical_fallback)\n",
    "    response = None\n",
    "    last_error: Exception | None = None\n",
    "    for idx, candidate in enumerate(callback_candidates):\n",
    "        try:\n",
    "            response = requests.post(candidate, headers=headers, json=callback_body, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Callback succeeded: {candidate} ({response.status_code})\")\n",
    "            break\n",
    "        except requests.exceptions.RequestException as exc:\n",
    "            last_error = exc\n",
    "            should_retry = idx == 0 and canonical_fallback and _looks_like_dns_failure(exc)\n",
    "            if should_retry:\n",
    "                print(\n",
    "                    f\"Callback host failed DNS lookup ({exc}); retrying canonical domain {canonical_fallback}.\"\n",
    "                )\n",
    "                continue\n",
    "            raise\n",
    "    if response is None:\n",
    "        raise last_error or RuntimeError(\"Callback failed without an HTTP response.\")\n",
    "else:\n",
    "    print(\"Callback URL not provided; skipping completion POST.\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
