{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b43a81c",
      "metadata": {},
      "source": [
        "# ConversionCentral Managed Profiling\n",
        "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c215aba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect parameters passed by the FastAPI backend\n",
        "\n",
        "\n",
        "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
        "dbutils.widgets.text(\"table_group_id\", \"\")\n",
        "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
        "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
        "dbutils.widgets.text(\"payload_path\", \"\")\n",
        "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
        "dbutils.widgets.text(\"callback_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
        "dbutils.widgets.text(\"callback_token\", \"\")\n",
        "dbutils.widgets.text(\"payload_storage\", \"\")\n",
        "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"schema_name\", \"\")\n",
        "dbutils.widgets.text(\"connection_id\", \"\")\n",
        "dbutils.widgets.text(\"connection_name\", \"\")\n",
        "dbutils.widgets.text(\"system_id\", \"\")\n",
        "dbutils.widgets.text(\"project_key\", \"\")\n",
        "dbutils.widgets.text(\"http_path\", \"\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "\n",
        "_NOTEBOOK_STAGE_SEQUENCE = (\n",
        "    (\"parameters\", \"Cell 2 (widget and Spark initialization)\"),\n",
        "    (\"profiling\", \"Cell 3 (profiling logic)\"),\n",
        "    (\"payload_persistence\", \"Cell 4 (payload persistence and callbacks)\"),\n",
        "    (\"metadata_helpers\", \"Cell 5 (metadata helper definitions)\"),\n",
        "    (\"finalization\", \"Cell 6 (final persistence and callbacks)\")\n",
        ")\n",
        "_NOTEBOOK_STAGE_LOOKUP = {label: (idx, hint) for idx, (label, hint) in enumerate(_NOTEBOOK_STAGE_SEQUENCE)}\n",
        "_STAGE_SYMBOL_REQUIREMENTS = {\n",
        "    \"parameters\": (\"table_group_id\", \"profile_run_id\", \"dq_schema\"),\n",
        "    \"profiling\": (\"MAX_COLUMNS_TO_PROFILE\",),\n",
        "    \"payload_persistence\": (\"_resolve_payload_storage_mode\", \"_payload_storage_is_artifact\"),\n",
        "    \"metadata_helpers\": (\"_persist_results_to_metadata\",),\n",
        "}\n",
        "\n",
        "\n",
        "def _ensure_notebook_stage(stage_label: str) -> None:\n",
        "    if stage_label not in _NOTEBOOK_STAGE_LOOKUP:\n",
        "        raise ValueError(f\"Unknown notebook stage '{stage_label}'.\")\n",
        "    stage_index, stage_hint = _NOTEBOOK_STAGE_LOOKUP[stage_label]\n",
        "    for prior_label, prior_hint in _NOTEBOOK_STAGE_SEQUENCE[:stage_index]:\n",
        "        required_symbols = _STAGE_SYMBOL_REQUIREMENTS.get(prior_label, ())\n",
        "        missing = [symbol for symbol in required_symbols if symbol not in globals()]\n",
        "        if missing:\n",
        "            missing_list = \", \".join(sorted(missing))\n",
        "            raise RuntimeError(\n",
        "                \"Profiling notebook Cells 1-6 must run sequentially. \"\n",
        "                f\"Run {prior_hint} before {stage_hint} (missing: {missing_list}).\",\n",
        "            )\n",
        "\n",
        "\n",
        "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
        "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
        "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
        "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
        "payload_path = raw_payload_path or None\n",
        "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
        "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
        "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
        "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
        "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
        "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
        "\n",
        "\n",
        "if not table_group_id or not profile_run_id:\n",
        "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
        "if not dq_schema:\n",
        "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065630ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile the tables registered for this table group and build the result payload.\n",
        "from datetime import datetime\n",
        "import re\n",
        "from contextlib import suppress\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run Cell 2 before profiling.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"profiling\")\n",
        "\n",
        "\n",
        "import datetime as dt\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "MAX_COLUMNS_TO_PROFILE = 25\n",
        "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
        "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
        "VALUE_DISTRIBUTION_LIMIT = 25\n",
        "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1000\n",
        "VALUE_DISTRIBUTION_MAX_ROWS = 5_000_000\n",
        "MAX_VALUE_DISPLAY_LENGTH = 256\n",
        "\n",
        "\n",
        "PROFILE_COLUMN_FIELDS = [\n",
        "    \"profile_run_id\",\n",
        "    \"schema_name\",\n",
        "    \"table_name\",\n",
        "    \"column_name\",\n",
        "    \"qualified_name\",\n",
        "    \"data_type\",\n",
        "    \"general_type\",\n",
        "    \"ordinal_position\",\n",
        "    \"row_count\",\n",
        "    \"null_count\",\n",
        "    \"non_null_count\",\n",
        "    \"distinct_count\",\n",
        "    \"min_value\",\n",
        "    \"max_value\",\n",
        "    \"avg_value\",\n",
        "    \"stddev_value\",\n",
        "    \"median_value\",\n",
        "    \"p95_value\",\n",
        "    \"true_count\",\n",
        "    \"false_count\",\n",
        "    \"min_length\",\n",
        "    \"max_length\",\n",
        "    \"avg_length\",\n",
        "    \"non_ascii_ratio\",\n",
        "    \"min_date\",\n",
        "    \"max_date\",\n",
        "    \"date_span_days\",\n",
        "    \"metrics_json\",\n",
        "    \"generated_at\",\n",
        "]\n",
        "\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd3b54f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist payload and call back into the API\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run prior cells before payload persistence.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"payload_persistence\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import socket\n",
        "from contextlib import suppress\n",
        "from functools import lru_cache\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "\n",
        "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
        "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
        "DEFAULT_CALLBACK_BEHAVIOR = \"metadata_only\"\n",
        "\n",
        "\n",
        "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
        "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
        "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
        "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
        "_STORAGE_DISABLED_NOTICE_EMITTED = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_dns_failure(error: BaseException) -> bool:\n",
        "    \"\"\"Detect DNS resolution failures from nested request exceptions.\"\"\"\n",
        "    current = error\n",
        "    while current:\n",
        "        if isinstance(current, socket.gaierror):\n",
        "            return True\n",
        "        name = current.__class__.__name__.lower()\n",
        "        if \"nameresolution\" in name:\n",
        "            return True\n",
        "        message = str(current).lower()\n",
        "        if \"temporary failure in name resolution\" in message:\n",
        "            return True\n",
        "        current = getattr(current, \"__cause__\", None) or getattr(current, \"__context__\", None)\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _rewrite_heroku_app_host(url: str | None) -> str | None:\n",
        "    \"\"\"Fallback to canonical Heroku hostname when review-app hosts fail DNS.\"\"\"\n",
        "    if not url:\n",
        "        return None\n",
        "    parsed = urlparse(url)\n",
        "    host = parsed.hostname\n",
        "    if not host:\n",
        "        return None\n",
        "    match = re.match(r\"^(?P<base>[a-z0-9-]+?)-[0-9a-f]{12}\\.herokuapp\\.com$\", host)\n",
        "    if not match:\n",
        "        return None\n",
        "    canonical_host = f\"{match.group('base')}.herokuapp.com\"\n",
        "    netloc = canonical_host\n",
        "    if parsed.port:\n",
        "        netloc = f\"{canonical_host}:{parsed.port}\"\n",
        "    if parsed.username:\n",
        "        auth = parsed.username\n",
        "        if parsed.password:\n",
        "            auth = f\"{auth}:{parsed.password}\"\n",
        "        netloc = f\"{auth}@{netloc}\"\n",
        "    scheme = parsed.scheme or \"https\"\n",
        "    if scheme.lower() == \"http\":\n",
        "        scheme = \"https\"\n",
        "    return urlunparse(parsed._replace(netloc=netloc, scheme=scheme))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7b24bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column/value persistence helpers and overrides\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run earlier cells before defining metadata helpers.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"metadata_helpers\")\n",
        "\n",
        "\n",
        "def _escape_identifier(identifier: str) -> str:\n",
        "    cleaned = (identifier or \"\").strip().replace(\"`\", \"\")\n",
        "    if not cleaned:\n",
        "        raise ValueError(\"Metadata identifiers cannot be empty.\")\n",
        "    return f\"`{cleaned}`\"\n",
        "\n",
        "\n",
        "def _metadata_schema_reference() -> str:\n",
        "    if not dq_schema:\n",
        "        raise ValueError(\"data_quality_schema widget must be set before resolving metadata tables.\")\n",
        "    catalog = (connection_catalog or \"\").strip()\n",
        "    if catalog:\n",
        "        return f\"{_escape_identifier(catalog)}.{_escape_identifier(dq_schema)}\"\n",
        "    return _escape_identifier(dq_schema)\n",
        "\n",
        "\n",
        "def _metadata_table(table_name: str) -> str:\n",
        "    return f\"{_metadata_schema_reference()}.{_escape_identifier(table_name)}\"\n",
        "\n",
        "\n",
        "def _first_non_empty(*values):\n",
        "    for value in values:\n",
        "        if isinstance(value, str):\n",
        "            candidate = value.strip()\n",
        "            if candidate:\n",
        "                return candidate\n",
        "        elif value is not None:\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_int(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return int(value)\n",
        "    if isinstance(value, int):\n",
        "        return value\n",
        "    if isinstance(value, float):\n",
        "        if not math.isfinite(value):\n",
        "            return None\n",
        "        return int(round(value))\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            if \".\" in candidate:\n",
        "                return int(float(candidate))\n",
        "            return int(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _coerce_float(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, bool):\n",
        "        return float(value)\n",
        "    if isinstance(value, (int, float)):\n",
        "        numeric = float(value)\n",
        "        if math.isfinite(numeric):\n",
        "            return numeric\n",
        "        return None\n",
        "    if isinstance(value, str):\n",
        "        candidate = value.strip().replace(\",\", \"\")\n",
        "        if not candidate:\n",
        "            return None\n",
        "        try:\n",
        "            numeric = float(candidate)\n",
        "        except ValueError:\n",
        "            return None\n",
        "        return numeric if math.isfinite(numeric) else None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fbc9bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalize the profiling job by persisting payload artifacts and metadata updates.\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "if \"_ensure_notebook_stage\" not in globals():\n",
        "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run Cells 2-5 before finalizing.\")\n",
        "\n",
        "\n",
        "_ensure_notebook_stage(\"finalization\")\n",
        "\n",
        "\n",
        "print(f\"Finalizing profiling run {profile_run_id}...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _guess_profiling_payload():\n",
        "    for key in (\"profiling_payload\", \"profile_payload\", \"results_payload\", \"payload\", \"raw_payload\"):\n",
        "        candidate = globals().get(key)\n",
        "        if isinstance(candidate, (dict, list)):\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _sanitize_segment(value, fallback):\n",
        "    cleaned = (value or fallback or \"\").strip() or fallback\n",
        "    return re.sub(r\"[^a-zA-Z0-9_.-]\", \"_\", cleaned)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _default_artifact_path():\n",
        "    base_root = (payload_base_path or DEFAULT_PRIVATE_PAYLOAD_ROOT or \"dbfs:/tmp/conversioncentral/profiles\").rstrip(\"/\")\n",
        "    group_segment = _sanitize_segment(table_group_id, \"table_group\")\n",
        "    run_segment = _sanitize_segment(profile_run_id, \"profile_run\")\n",
        "    return f\"{base_root}/{group_segment}/{run_segment}.json\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _materialize_payload_artifact(payload_obj, target_path):\n",
        "    destination = target_path\n",
        "    if destination.startswith(\"/dbfs/\"):\n",
        "        destination = f\"dbfs:/{destination[6:]}\"\n",
        "    if destination.startswith(\"dbfs:/\"):\n",
        "        redirected = _redirect_dbfs_path(destination)\n",
        "        if not redirected:\n",
        "            return None\n",
        "        destination = redirected\n",
        "    encoded = _encode_payload_json(payload_obj) or json.dumps(payload_obj, default=str)\n",
        "    if destination.startswith(\"dbfs:/\") or destination.startswith(\"file:/\"):\n",
        "        _mkdirs_if_supported(destination)\n",
        "        dbutils.fs.put(destination, encoded, True)\n",
        "        return destination\n",
        "    directory = os.path.dirname(destination)\n",
        "    if directory:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "    with open(destination, \"w\", encoding=\"utf-8\") as handle:\n",
        "        handle.write(encoded)\n",
        "    return destination\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results_payload = _guess_profiling_payload()\n",
        "payload_location = payload_path or None\n",
        "storage_mode = _resolve_payload_storage_mode()\n",
        "artifact_mode_requested = _payload_storage_is_artifact(storage_mode)\n",
        "\n",
        "\n",
        "if results_payload is not None and artifact_mode_requested:\n",
        "    target_path = payload_location or _default_artifact_path()\n",
        "    resolved_location = _materialize_payload_artifact(results_payload, target_path)\n",
        "    if resolved_location:\n",
        "        payload_location = resolved_location\n",
        "        print(f\"Persisted profiling payload artifact to {payload_location}.\")\n",
        "    else:\n",
        "        print(f\"Unable to persist payload artifact to {target_path}; continuing with inline payload only.\")\n",
        "elif artifact_mode_requested and not payload_location:\n",
        "    print(\"Payload storage mode requires an artifact path; defaulting once payload data is available.\")\n",
        "\n",
        "\n",
        "if payload_location and payload_location.startswith(\"/dbfs/\"):\n",
        "    payload_location = f\"dbfs:/{payload_location[6:]}\"\n",
        "\n",
        "\n",
        "if results_payload is None and not payload_location:\n",
        "    raise RuntimeError(\n",
        "        \"Profiling payload missing. Re-run the profiling cells for run \"\n",
        "        f\"{profile_run_id} or supply the payload_path widget so detail tables can be persisted.\",\n",
        "    )\n",
        "\n",
        "\n",
        "_persist_results_to_metadata(results_payload, payload_location)\n",
        "print(\"Profiling metadata persistence completed.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
