{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43a81c",
   "metadata": {},
   "source": [
    "# ConversionCentral Managed Profiling\n",
    "Run this notebook from a Databricks Repo so backend deployments control profiling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameters passed by the FastAPI backend\n",
    "\n",
    "\n",
    "# Each widget is declared up front so Databricks jobs can safely supply overrides.\n",
    "dbutils.widgets.text(\"table_group_id\", \"\")\n",
    "dbutils.widgets.text(\"profile_run_id\", \"\")\n",
    "dbutils.widgets.text(\"data_quality_schema\", \"\")\n",
    "dbutils.widgets.text(\"payload_path\", \"\")\n",
    "dbutils.widgets.text(\"payload_base_path\", \"\")\n",
    "dbutils.widgets.text(\"callback_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_base_url\", \"\")\n",
    "dbutils.widgets.text(\"callback_token\", \"\")\n",
    "dbutils.widgets.text(\"payload_storage\", \"\")\n",
    "dbutils.widgets.text(\"callback_behavior\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"connection_id\", \"\")\n",
    "dbutils.widgets.text(\"connection_name\", \"\")\n",
    "dbutils.widgets.text(\"system_id\", \"\")\n",
    "dbutils.widgets.text(\"project_key\", \"\")\n",
    "dbutils.widgets.text(\"http_path\", \"\")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "_NOTEBOOK_STAGE_SEQUENCE = (\n",
    "    (\"parameters\", \"Cell 2 (widget and Spark initialization)\"),\n",
    "    (\"profiling\", \"Cell 3 (profiling logic)\"),\n",
    "    (\"payload_persistence\", \"Cell 4 (payload persistence and callbacks)\"),\n",
    "    (\"metadata_helpers\", \"Cell 5 (metadata helper definitions)\"),\n",
    "    (\"metadata_writer\", \"Cell 6 (metadata writer integration)\"),\n",
    "    (\"finalization\", \"Cell 7 (final persistence and callbacks)\"),\n",
    ")\n",
    "_NOTEBOOK_STAGE_LOOKUP = {label: (idx, hint) for idx, (label, hint) in enumerate(_NOTEBOOK_STAGE_SEQUENCE)}\n",
    "_STAGE_SYMBOL_REQUIREMENTS = {\n",
    "    \"parameters\": (\"table_group_id\", \"profile_run_id\", \"dq_schema\"),\n",
    "    \"profiling\": (\"MAX_COLUMNS_TO_PROFILE\",),\n",
    "    \"payload_persistence\": (\"_resolve_payload_storage_mode\", \"_payload_storage_is_artifact\"),\n",
    "    \"metadata_helpers\": (\"_persist_results_to_metadata\",),\n",
    "    \"metadata_writer\": (\"_persist_profiling_metadata\",),\n",
    "}\n",
    "\n",
    "\n",
    "def _ensure_notebook_stage(stage_label: str) -> None:\n",
    "    if stage_label not in _NOTEBOOK_STAGE_LOOKUP:\n",
    "        raise ValueError(f\"Unknown notebook stage '{stage_label}'.\")\n",
    "    stage_index, stage_hint = _NOTEBOOK_STAGE_LOOKUP[stage_label]\n",
    "    for prior_label, prior_hint in _NOTEBOOK_STAGE_SEQUENCE[:stage_index]:\n",
    "        required_symbols = _STAGE_SYMBOL_REQUIREMENTS.get(prior_label, ())\n",
    "        missing = [symbol for symbol in required_symbols if symbol not in globals()]\n",
    "        if missing:\n",
    "            missing_list = \", \".join(sorted(missing))\n",
    "            raise RuntimeError(\n",
    "                \"Profiling notebook Cells 1-7 must run sequentially. \"\n",
    "                f\"Run {prior_hint} before {stage_hint} (missing: {missing_list}).\",\n",
    "            )\n",
    "\n",
    "\n",
    "table_group_id = dbutils.widgets.get(\"table_group_id\")\n",
    "profile_run_id = dbutils.widgets.get(\"profile_run_id\")\n",
    "dq_schema = (dbutils.widgets.get(\"data_quality_schema\") or \"\").strip()\n",
    "raw_payload_path = (dbutils.widgets.get(\"payload_path\") or \"\").strip()\n",
    "payload_path = raw_payload_path or None\n",
    "payload_base_path = (dbutils.widgets.get(\"payload_base_path\") or \"\").strip() or None\n",
    "callback_url = (dbutils.widgets.get(\"callback_url\") or \"\").strip() or None\n",
    "callback_base_url = (dbutils.widgets.get(\"callback_base_url\") or \"\").strip() or None\n",
    "callback_token = (dbutils.widgets.get(\"callback_token\") or \"\").strip() or None\n",
    "connection_catalog = (dbutils.widgets.get(\"catalog\") or \"\").strip()\n",
    "connection_schema = (dbutils.widgets.get(\"schema_name\") or \"\").strip()\n",
    "\n",
    "\n",
    "if not table_group_id or not profile_run_id:\n",
    "    raise ValueError(\"Required widgets missing: table_group_id/profile_run_id\")\n",
    "if not dq_schema:\n",
    "    raise ValueError(\"Data quality schema widget is required for profiling runs.\")\n",
    "\n",
    "\n",
    "_ensure_notebook_stage(\"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the tables registered for this table group and build the result payload.\n",
    "import math\n",
    "import uuid\n",
    "from contextlib import suppress\n",
    "from datetime import datetime, timezone\n",
    "from functools import lru_cache\n",
    "from typing import Any, Iterable\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run Cell 2 before profiling.\")\n",
    "\n",
    "_ensure_notebook_stage(\"profiling\")\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from app.databricks_profiling import build_metadata_frames\n",
    "\n",
    "MAX_COLUMNS_TO_PROFILE = 25\n",
    "NULL_RATIO_ALERT_THRESHOLD = 0.5\n",
    "HIGH_NULL_RATIO_THRESHOLD = 0.9\n",
    "VALUE_DISTRIBUTION_LIMIT = 25\n",
    "VALUE_DISTRIBUTION_DISTINCT_THRESHOLD = 1_000\n",
    "VALUE_DISTRIBUTION_MAX_ROWS = 1_000_000\n",
    "MAX_VALUE_DISPLAY_LENGTH = 256\n",
    "\n",
    "PROFILE_COLUMN_FIELDS = [\n",
    "    \"profile_run_id\",\n",
    "    \"schema_name\",\n",
    "    \"table_name\",\n",
    "    \"column_name\",\n",
    "    \"qualified_name\",\n",
    "    \"data_type\",\n",
    "    \"general_type\",\n",
    "    \"ordinal_position\",\n",
    "    \"row_count\",\n",
    "    \"null_count\",\n",
    "    \"non_null_count\",\n",
    "    \"distinct_count\",\n",
    "    \"min_value\",\n",
    "    \"max_value\",\n",
    "    \"avg_value\",\n",
    "    \"stddev_value\",\n",
    "    \"median_value\",\n",
    "    \"p95_value\",\n",
    "    \"true_count\",\n",
    "    \"false_count\",\n",
    "    \"min_length\",\n",
    "    \"max_length\",\n",
    "    \"avg_length\",\n",
    "    \"non_ascii_ratio\",\n",
    "    \"min_date\",\n",
    "    \"max_date\",\n",
    "    \"date_span_days\",\n",
    "    \"metrics_json\",\n",
    "    \"generated_at\",\n",
    "]\n",
    "\n",
    "\n",
    "def _now_utc() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def _qualify_metadata_table(table_name: str) -> str:\n",
    "    table = (table_name or \"\").strip()\n",
    "    if not table:\n",
    "        raise ValueError(\"Metadata table name cannot be empty.\")\n",
    "    if connection_catalog:\n",
    "        return f\"`{connection_catalog}`.`{dq_schema}`.`{table}`\"\n",
    "    return f\"`{dq_schema}`.`{table}`\"\n",
    "\n",
    "\n",
    "def _fetch_table_group_row() -> dict[str, Any]:\n",
    "    table = _qualify_metadata_table(\"dq_table_groups\")\n",
    "    groups_df = spark.table(table).filter(F.col(\"table_group_id\") == table_group_id).limit(1)\n",
    "    rows = groups_df.collect()\n",
    "    if not rows:\n",
    "        raise ValueError(f\"Table group {table_group_id} was not found in metadata tables.\")\n",
    "    return rows[0].asDict(recursive=True)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _fetch_tables_for_group() -> list[dict[str, Any]]:\n",
    "    table = _qualify_metadata_table(\"dq_tables\")\n",
    "    tables_df = spark.table(table).filter(F.col(\"table_group_id\") == table_group_id)\n",
    "    return [row.asDict(recursive=True) for row in tables_df.collect()]\n",
    "\n",
    "\n",
    "def _pattern_matches(value: str | None, pattern: str | None) -> bool:\n",
    "    from fnmatch import fnmatchcase\n",
    "\n",
    "    value = (value or \"\").strip().lower()\n",
    "    pattern = (pattern or \"\").strip().lower()\n",
    "    if not pattern:\n",
    "        return True\n",
    "    return fnmatchcase(value, pattern)\n",
    "\n",
    "\n",
    "def _table_is_selected(table_entry: dict[str, Any], include_mask: str | None, exclude_mask: str | None) -> bool:\n",
    "    logical_name = (table_entry.get(\"table_name\") or \"\").strip()\n",
    "    if include_mask and not _pattern_matches(logical_name, include_mask):\n",
    "        return False\n",
    "    if exclude_mask and _pattern_matches(logical_name, exclude_mask):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _resolve_physical_reference(table_entry: dict[str, Any]) -> tuple[str, str]:\n",
    "    schema_override = (table_entry.get(\"schema_name\") or connection_schema or dq_schema or \"\").strip()\n",
    "    schema_name = schema_override or dq_schema\n",
    "    table_name = (table_entry.get(\"table_name\") or \"\").strip()\n",
    "    if not table_name:\n",
    "        raise ValueError(\"Table entries must define table_name before profiling.\")\n",
    "    if connection_catalog:\n",
    "        qualified = f\"`{connection_catalog}`.`{schema_name}`.`{table_name}`\"\n",
    "    else:\n",
    "        qualified = f\"`{schema_name}`.`{table_name}`\"\n",
    "    return schema_name, qualified\n",
    "\n",
    "\n",
    "def _is_numeric_type(data_type: T.DataType) -> bool:\n",
    "    return isinstance(data_type, (\n",
    "        T.ByteType,\n",
    "        T.ShortType,\n",
    "        T.IntegerType,\n",
    "        T.LongType,\n",
    "        T.FloatType,\n",
    "        T.DoubleType,\n",
    "        T.DecimalType,\n",
    "    ))\n",
    "\n",
    "\n",
    "def _is_boolean_type(data_type: T.DataType) -> bool:\n",
    "    return isinstance(data_type, T.BooleanType)\n",
    "\n",
    "\n",
    "def _is_date_type(data_type: T.DataType) -> bool:\n",
    "    return isinstance(data_type, (T.DateType, T.TimestampType))\n",
    "\n",
    "\n",
    "def _resolve_general_type(data_type: T.DataType) -> str:\n",
    "    if _is_numeric_type(data_type):\n",
    "        return \"numeric\"\n",
    "    if _is_boolean_type(data_type):\n",
    "        return \"boolean\"\n",
    "    if isinstance(data_type, T.StringType):\n",
    "        return \"string\"\n",
    "    if _is_date_type(data_type):\n",
    "        return \"datetime\"\n",
    "    return data_type.simpleString()\n",
    "\n",
    "\n",
    "def _stringify(value: Any) -> str | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, (datetime, dt.date)):\n",
    "        if isinstance(value, datetime) and value.tzinfo is None:\n",
    "            value = value.replace(tzinfo=timezone.utc)\n",
    "        return value.isoformat()\n",
    "    text = str(value)\n",
    "    return text[:MAX_VALUE_DISPLAY_LENGTH]\n",
    "\n",
    "\n",
    "def _build_top_values(df: DataFrame, column: str, *, row_count: int, distinct_count: int | None) -> list[dict[str, Any]]:\n",
    "    if distinct_count is not None and distinct_count > VALUE_DISTRIBUTION_DISTINCT_THRESHOLD:\n",
    "        return []\n",
    "    working_df = df\n",
    "    if VALUE_DISTRIBUTION_MAX_ROWS and row_count > VALUE_DISTRIBUTION_MAX_ROWS:\n",
    "        working_df = df.limit(VALUE_DISTRIBUTION_MAX_ROWS)\n",
    "    agg_df = (\n",
    "        working_df.groupBy(F.col(column)).count().orderBy(F.col(\"count\").desc()).limit(VALUE_DISTRIBUTION_LIMIT)\n",
    "    )\n",
    "    rows = agg_df.collect()\n",
    "    results: list[dict[str, Any]] = []\n",
    "    for row in rows:\n",
    "        row_dict = row.asDict(recursive=True)\n",
    "        results.append(\n",
    "            {\n",
    "                \"value\": _stringify(row_dict.get(column)),\n",
    "                \"count\": int(row_dict.get(\"count\") or 0),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def _percentile_metrics(df: DataFrame, column: str) -> tuple[float | None, float | None]:\n",
    "    try:\n",
    "        percentiles = df.approxQuantile(column, [0.5, 0.95], 0.01)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    median = percentiles[0] if len(percentiles) > 0 else None\n",
    "    p95 = percentiles[1] if len(percentiles) > 1 else None\n",
    "    return median, p95\n",
    "\n",
    "\n",
    "def _profile_column(\n",
    "    df: DataFrame,\n",
    "    field: T.StructField,\n",
    "    *,\n",
    "    row_count: int,\n",
    "    ordinal: int,\n",
    "    schema_name: str,\n",
    "    table_name: str,\n",
    ") -> tuple[dict[str, Any], list[dict[str, Any]]]:\n",
    "    column = field.name\n",
    "    col_ref = F.col(column)\n",
    "\n",
    "    aggregations = [\n",
    "        F.count(col_ref).alias(\"non_null_count\"),\n",
    "        F.approx_count_distinct(col_ref, rsd=0.02).alias(\"distinct_count\"),\n",
    "        F.count(F.when(col_ref.isNull(), 1)).alias(\"null_count\"),\n",
    "    ]\n",
    "\n",
    "    if _is_numeric_type(field.dataType):\n",
    "        numeric_col = col_ref.cast(\"double\")\n",
    "        aggregations.extend(\n",
    "            [\n",
    "                F.min(numeric_col).alias(\"min_value\"),\n",
    "                F.max(numeric_col).alias(\"max_value\"),\n",
    "                F.avg(numeric_col).alias(\"avg_value\"),\n",
    "                F.stddev(numeric_col).alias(\"stddev_value\"),\n",
    "            ]\n",
    "        )\n",
    "    elif isinstance(field.dataType, T.StringType):\n",
    "        length_col = F.length(col_ref)\n",
    "        aggregations.extend(\n",
    "            [\n",
    "                F.min(col_ref).alias(\"min_value\"),\n",
    "                F.max(col_ref).alias(\"max_value\"),\n",
    "                F.min(length_col).alias(\"min_length\"),\n",
    "                F.max(length_col).alias(\"max_length\"),\n",
    "                F.avg(length_col).alias(\"avg_length\"),\n",
    "                F.avg(F.when(col_ref.rlike(\"[^\\\\x00-\\\\x7F]\"), 1).otherwise(0)).alias(\"non_ascii_ratio\"),\n",
    "            ]\n",
    "        )\n",
    "    elif _is_date_type(field.dataType):\n",
    "        date_col = col_ref.cast(\"timestamp\")\n",
    "        aggregations.extend(\n",
    "            [\n",
    "                F.min(date_col).alias(\"min_date\"),\n",
    "                F.max(date_col).alias(\"max_date\"),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        aggregations.extend(\n",
    "            [\n",
    "                F.min(col_ref).alias(\"min_value\"),\n",
    "                F.max(col_ref).alias(\"max_value\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if _is_boolean_type(field.dataType):\n",
    "        aggregations.extend(\n",
    "            [\n",
    "                F.sum(F.when(col_ref.isTrue(), 1).otherwise(0)).alias(\"true_count\"),\n",
    "                F.sum(F.when(col_ref.isFalse(), 1).otherwise(0)).alias(\"false_count\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    stats = df.agg(*aggregations).collect()[0]\n",
    "    non_null_count = int(stats.non_null_count or 0)\n",
    "    null_count = int(stats.null_count or 0)\n",
    "    distinct_count = stats.distinct_count\n",
    "    if distinct_count is not None:\n",
    "        try:\n",
    "            distinct_count = int(distinct_count)\n",
    "        except (TypeError, ValueError):\n",
    "            distinct_count = None\n",
    "\n",
    "    median_value = None\n",
    "    p95_value = None\n",
    "    if _is_numeric_type(field.dataType):\n",
    "        median_value, p95_value = _percentile_metrics(df, column)\n",
    "\n",
    "    min_length = getattr(stats, \"min_length\", None)\n",
    "    max_length = getattr(stats, \"max_length\", None)\n",
    "    avg_length = getattr(stats, \"avg_length\", None)\n",
    "    non_ascii_ratio = getattr(stats, \"non_ascii_ratio\", None)\n",
    "    min_value = getattr(stats, \"min_value\", None)\n",
    "    max_value = getattr(stats, \"max_value\", None)\n",
    "    avg_value = getattr(stats, \"avg_value\", None)\n",
    "    stddev_value = getattr(stats, \"stddev_value\", None)\n",
    "    min_date = getattr(stats, \"min_date\", None)\n",
    "    max_date = getattr(stats, \"max_date\", None)\n",
    "\n",
    "    metrics = {\n",
    "        \"percentiles\": {\"median\": median_value, \"p95\": p95_value},\n",
    "        \"top_values\": [],\n",
    "    }\n",
    "\n",
    "    top_values = _build_top_values(df, column, row_count=row_count, distinct_count=distinct_count)\n",
    "    if top_values:\n",
    "        metrics[\"top_values\"] = top_values\n",
    "\n",
    "    metrics.update(\n",
    "        {\n",
    "            \"row_count\": row_count,\n",
    "            \"null_count\": null_count,\n",
    "            \"non_null_count\": non_null_count,\n",
    "            \"distinct_count\": distinct_count,\n",
    "            \"min_length\": min_length,\n",
    "            \"max_length\": max_length,\n",
    "            \"avg_length\": avg_length,\n",
    "            \"non_ascii_ratio\": non_ascii_ratio,\n",
    "            \"min_value\": _stringify(min_value),\n",
    "            \"max_value\": _stringify(max_value),\n",
    "            \"avg_value\": avg_value,\n",
    "            \"stddev_value\": stddev_value,\n",
    "            \"min_date\": _stringify(min_date),\n",
    "            \"max_date\": _stringify(max_date),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    column_entry = {\n",
    "        \"column_name\": column,\n",
    "        \"table_name\": table_name,\n",
    "        \"schema_name\": schema_name,\n",
    "        \"data_type\": field.dataType.simpleString(),\n",
    "        \"general_type\": _resolve_general_type(field.dataType),\n",
    "        \"ordinal_position\": ordinal,\n",
    "        \"row_count\": row_count,\n",
    "        \"null_count\": null_count,\n",
    "        \"non_null_count\": non_null_count,\n",
    "        \"distinct_count\": distinct_count,\n",
    "        \"min_value\": _stringify(min_value),\n",
    "        \"max_value\": _stringify(max_value),\n",
    "        \"avg_value\": avg_value,\n",
    "        \"stddev_value\": stddev_value,\n",
    "        \"median_value\": median_value,\n",
    "        \"p95_value\": p95_value,\n",
    "        \"true_count\": getattr(stats, \"true_count\", None),\n",
    "        \"false_count\": getattr(stats, \"false_count\", None),\n",
    "        \"min_length\": min_length,\n",
    "        \"max_length\": max_length,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"non_ascii_ratio\": non_ascii_ratio,\n",
    "        \"min_date\": _stringify(min_date),\n",
    "        \"max_date\": _stringify(max_date),\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "    if min_date and max_date:\n",
    "        with suppress(Exception):\n",
    "            delta_days = (max_date - min_date).days\n",
    "            column_entry[\"date_span_days\"] = delta_days\n",
    "            metrics[\"date_span_days\"] = delta_days\n",
    "\n",
    "    column_anomalies: list[dict[str, Any]] = []\n",
    "    null_ratio = float(null_count) / row_count if row_count else 0.0\n",
    "    if null_ratio >= NULL_RATIO_ALERT_THRESHOLD:\n",
    "        severity = \"critical\" if null_ratio >= HIGH_NULL_RATIO_THRESHOLD else \"warning\"\n",
    "        column_anomalies.append(\n",
    "            {\n",
    "                \"anomaly_type\": \"high_null_ratio\",\n",
    "                \"severity\": severity,\n",
    "                \"description\": f\"Null ratio {null_ratio:.2%} exceeds threshold\",\n",
    "                \"column_name\": column,\n",
    "                \"detected_at\": _now_utc().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if column_anomalies:\n",
    "        column_entry[\"anomalies\"] = column_anomalies\n",
    "\n",
    "    return column_entry, column_anomalies\n",
    "\n",
    "\n",
    "def _profile_table(\n",
    "    table_entry: dict[str, Any],\n",
    "    *,\n",
    "    include_mask: str | None,\n",
    "    exclude_mask: str | None,\n",
    ") -> dict[str, Any] | None:\n",
    "    if not _table_is_selected(table_entry, include_mask, exclude_mask):\n",
    "        return None\n",
    "\n",
    "    schema_name, qualified = _resolve_physical_reference(table_entry)\n",
    "    table_name = table_entry.get(\"table_name\")\n",
    "    try:\n",
    "        df = spark.table(qualified)\n",
    "    except AnalysisException as exc:\n",
    "        print(f\"[profiling] Skipping table {qualified}: {exc}\")\n",
    "        return None\n",
    "\n",
    "    row_count = df.count()\n",
    "    if row_count == 0:\n",
    "        print(f\"[profiling] Table {qualified} returned zero rows; still capturing schema metadata.\")\n",
    "\n",
    "    columns = df.schema.fields[:MAX_COLUMNS_TO_PROFILE]\n",
    "    profiled_columns: list[dict[str, Any]] = []\n",
    "    anomalies: list[dict[str, Any]] = []\n",
    "\n",
    "    for ordinal, field in enumerate(columns, start=1):\n",
    "        try:\n",
    "            column_entry, column_anomalies = _profile_column(\n",
    "                df,\n",
    "                field,\n",
    "                row_count=row_count,\n",
    "                ordinal=ordinal,\n",
    "                schema_name=schema_name,\n",
    "                table_name=table_name,\n",
    "            )\n",
    "        except Exception as exc:  # pragma: no cover - defensive guards for Spark failures\n",
    "            print(f\"[profiling] Failed to profile column {field.name} on {qualified}: {exc}\")\n",
    "            continue\n",
    "        profiled_columns.append(column_entry)\n",
    "        anomalies.extend(column_anomalies)\n",
    "\n",
    "    table_payload = {\n",
    "        \"table_group_id\": table_group_id,\n",
    "        \"table_id\": table_entry.get(\"table_id\"),\n",
    "        \"schema_name\": schema_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_count\": row_count,\n",
    "        \"column_count\": len(profiled_columns),\n",
    "        \"columns\": profiled_columns,\n",
    "        \"anomalies\": anomalies,\n",
    "        \"metrics\": {\n",
    "            \"record_count\": row_count,\n",
    "            \"column_count\": len(profiled_columns),\n",
    "        },\n",
    "    }\n",
    "    return table_payload\n",
    "\n",
    "\n",
    "def _build_profiling_payload() -> dict[str, Any]:\n",
    "    group_row = _fetch_table_group_row()\n",
    "    include_mask = group_row.get(\"profiling_include_mask\")\n",
    "    exclude_mask = group_row.get(\"profiling_exclude_mask\")\n",
    "    tables = []\n",
    "    for entry in _fetch_tables_for_group():\n",
    "        profiled = _profile_table(entry, include_mask=include_mask, exclude_mask=exclude_mask)\n",
    "        if profiled:\n",
    "            tables.append(profiled)\n",
    "\n",
    "    if not tables:\n",
    "        raise RuntimeError(\"No tables were profiled. Verify metadata selections and Databricks connectivity.\")\n",
    "\n",
    "    total_rows = sum(table.get(\"row_count\") or 0 for table in tables)\n",
    "    total_columns = sum(table.get(\"column_count\", 0) for table in tables)\n",
    "    anomaly_count = sum(len(table.get(\"anomalies\") or []) for table in tables)\n",
    "\n",
    "    started_at = _now_utc()\n",
    "    completed_at = _now_utc()\n",
    "\n",
    "    summary = {\n",
    "        \"status\": \"completed\",\n",
    "        \"row_count\": total_rows,\n",
    "        \"table_count\": len(tables),\n",
    "        \"column_count\": total_columns,\n",
    "        \"anomaly_count\": anomaly_count,\n",
    "        \"profile_mode\": \"catalog\",\n",
    "        \"profile_version\": \"catalog_v1\",\n",
    "        \"started_at\": started_at.isoformat(),\n",
    "        \"completed_at\": completed_at.isoformat(),\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"table_group_id\": table_group_id,\n",
    "        \"profile_run_id\": profile_run_id,\n",
    "        \"tables\": tables,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "\n",
    "profiling_payload = _build_profiling_payload()\n",
    "profiling_results = profiling_payload\n",
    "profile_payload = profiling_payload\n",
    "profile_results = profiling_payload\n",
    "results_payload = profiling_payload\n",
    "\n",
    "frames, frame_counts = build_metadata_frames(\n",
    "    spark,\n",
    "    profiling_payload,\n",
    "    profile_run_id=profile_run_id,\n",
    "    table_group_id=table_group_id,\n",
    "    summary=profiling_payload.get(\"summary\", {}),\n",
    ")\n",
    "\n",
    "for frame_name, df in frames.items():\n",
    "    globals()[frame_name] = df\n",
    "\n",
    "print(\n",
    "    f\"[profiling] Completed catalog-driven profiling for {len(profiling_payload['tables'])} tables \"\n",
    "    f\"({profiling_payload['summary']['column_count']} columns).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist payload artifacts and helper utilities\n",
    "\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\"Profiling notebook Cells 1-6 must run sequentially; run prior cells before payload persistence.\")\n",
    "\n",
    "\n",
    "_ensure_notebook_stage(\"payload_persistence\")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from contextlib import suppress\n",
    "from functools import lru_cache\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "DEFAULT_PRIVATE_PAYLOAD_ROOT = \"dbfs:/tmp/conversioncentral/profiles\"\n",
    "DEFAULT_DRIVER_PAYLOAD_ROOT = \"file:/databricks/driver/conversioncentral/profiles\"\n",
    "DEFAULT_PAYLOAD_STORAGE_MODE = \"inline\"\n",
    "_VALID_PAYLOAD_STORAGE_MODES = {\"inline\", \"artifact\", \"both\"}\n",
    "def _clean_widget_value(value: Optional[str]) -> str:\n",
    "    return (value or \"\").strip()\n",
    "def _resolve_payload_storage_mode() -> str:\n",
    "    raw_value = _clean_widget_value(dbutils.widgets.get(\"payload_storage\")).lower()\n",
    "    if raw_value in _VALID_PAYLOAD_STORAGE_MODES:\n",
    "        return raw_value\n",
    "    if raw_value in {\"inline_only\", \"inline_metadata\"}:\n",
    "        return \"inline\"\n",
    "    if raw_value in {\"artifact_only\", \"artifact_metadata\"}:\n",
    "        return \"artifact\"\n",
    "    if payload_path:\n",
    "        return \"artifact\"\n",
    "    return DEFAULT_PAYLOAD_STORAGE_MODE\n",
    "def _payload_storage_is_artifact(mode: str) -> bool:\n",
    "    normalized = (mode or DEFAULT_PAYLOAD_STORAGE_MODE).strip().lower()\n",
    "    return normalized in {\"artifact\", \"both\"}\n",
    "DBFS_DISABLED_MESSAGES = (\"public dbfs root is disabled\", \"access is denied\")\n",
    "DRIVER_DISABLED_MESSAGES = (\"local filesystem access is forbidden\", \"workspacelocalfilesystem\")\n",
    "URI_SCHEME_PATTERN = re.compile(r\"^[a-z][a-z0-9+.\\-]*:/\", re.IGNORECASE)\n",
    "_DBFS_REDIRECT_NOTICE_EMITTED = False\n",
    "_STORAGE_DISABLED_NOTICE_EMITTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column/value persistence helpers and overrides\n",
    "import datetime as dt\n",
    "from datetime import datetime, timezone\n",
    "from contextlib import suppress\n",
    "from typing import Any, Mapping\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run earlier cells before defining metadata helpers.\")\n",
    "\n",
    "\n",
    "_ensure_notebook_stage(\"metadata_helpers\")\n",
    "\n",
    "\n",
    "def _escape_identifier(identifier: str) -> str:\n",
    "    cleaned = (identifier or \"\").strip().replace(\"`\", \"\")\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"Metadata identifiers cannot be empty.\")\n",
    "    return f\"`{cleaned}`\"\n",
    "\n",
    "\n",
    "def _metadata_schema_reference() -> str:\n",
    "    if not dq_schema:\n",
    "        raise ValueError(\"data_quality_schema widget must be set before resolving metadata tables.\")\n",
    "    catalog = (connection_catalog or \"\").strip()\n",
    "    if catalog:\n",
    "        return f\"{_escape_identifier(catalog)}.{_escape_identifier(dq_schema)}\"\n",
    "    return _escape_identifier(dq_schema)\n",
    "\n",
    "\n",
    "def _metadata_table(table_name: str) -> str:\n",
    "    return f\"{_metadata_schema_reference()}.{_escape_identifier(table_name)}\"\n",
    "\n",
    "\n",
    "def _first_non_empty(*values):\n",
    "    for value in values:\n",
    "        if isinstance(value, str):\n",
    "            candidate = value.strip()\n",
    "            if candidate:\n",
    "                return candidate\n",
    "        elif value is not None:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def _coerce_int(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bool):\n",
    "        return int(value)\n",
    "    if isinstance(value, int):\n",
    "        return value\n",
    "    if isinstance(value, float):\n",
    "        if not math.isfinite(value):\n",
    "            return None\n",
    "        return int(round(value))\n",
    "    if isinstance(value, str):\n",
    "        candidate = value.strip().replace(\",\", \"\")\n",
    "        if not candidate:\n",
    "            return None\n",
    "        try:\n",
    "            if \".\" in candidate:\n",
    "                return int(float(candidate))\n",
    "            return int(candidate)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _coerce_float(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bool):\n",
    "        return float(value)\n",
    "    if isinstance(value, (int, float)):\n",
    "        numeric = float(value)\n",
    "        if math.isfinite(numeric):\n",
    "            return numeric\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        candidate = value.strip().replace(\",\", \"\")\n",
    "        if not candidate:\n",
    "            return None\n",
    "        try:\n",
    "            numeric = float(candidate)\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return numeric if math.isfinite(numeric) else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _sql_literal(value) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    if isinstance(value, datetime):\n",
    "        if value.tzinfo is None:\n",
    "            value = value.replace(tzinfo=timezone.utc)\n",
    "        else:\n",
    "            value = value.astimezone(timezone.utc)\n",
    "        return f\"'{value.strftime('%Y-%m-%d %H:%M:%S')}'\"\n",
    "    text = str(value).replace(\"'\", \"''\")\n",
    "    return f\"'{text}'\"\n",
    "\n",
    "\n",
    "def _sql_number(value) -> str:\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _sql_literal_tuple(values) -> str:\n",
    "    inner = \", \".join(_sql_literal(value) for value in values)\n",
    "    return f\"({inner})\"\n",
    "\n",
    "\n",
    "def _sql_literal_set(values) -> str:\n",
    "    if not values:\n",
    "        return \"(NULL)\"\n",
    "    return f\"({', '.join(_sql_literal(value) for value in values)})\"\n",
    "\n",
    "\n",
    "def _coerce_timestamp_value(value) -> datetime | None:\n",
    "    if isinstance(value, datetime):\n",
    "        if value.tzinfo is None:\n",
    "            return value.replace(tzinfo=timezone.utc)\n",
    "        return value.astimezone(timezone.utc)\n",
    "    if isinstance(value, (int, float)):\n",
    "        numeric = float(value)\n",
    "        if abs(numeric) > 1_000_000_000_000:\n",
    "            numeric /= 1000.0\n",
    "        with suppress(Exception):\n",
    "            return datetime.fromtimestamp(numeric, tz=timezone.utc)\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        text = value.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        normalized = text[:-1] + \"+00:00\" if text.endswith(\"Z\") else text\n",
    "        with suppress(ValueError):\n",
    "            parsed = datetime.fromisoformat(normalized)\n",
    "            if parsed.tzinfo is None:\n",
    "                return parsed.replace(tzinfo=timezone.utc)\n",
    "            return parsed.astimezone(timezone.utc)\n",
    "        for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"):\n",
    "            with suppress(ValueError):\n",
    "                parsed = datetime.strptime(normalized, fmt)\n",
    "                return parsed.replace(tzinfo=timezone.utc)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_databricks_run_id() -> str | None:\n",
    "    with suppress(Exception):\n",
    "        value = spark.conf.get(\"spark.databricks.job.runId\")\n",
    "        if value:\n",
    "            return str(value)\n",
    "    with suppress(Exception):\n",
    "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        run_id = ctx.runId().get()\n",
    "        if run_id:\n",
    "            return str(run_id)\n",
    "    with suppress(Exception):\n",
    "        task_run_id = dbutils.jobs.taskRunId()\n",
    "        if task_run_id:\n",
    "            return str(task_run_id)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_profile_summary(payload: Any) -> Mapping[str, Any]:\n",
    "    if isinstance(payload, Mapping):\n",
    "        for key in (\"summary\", \"profile_summary\", \"profileSummary\", \"metadata\", \"run\"):\n",
    "            nested = payload.get(key)\n",
    "            if isinstance(nested, Mapping):\n",
    "                return nested\n",
    "        return payload\n",
    "    if isinstance(payload, list):\n",
    "        for item in payload:\n",
    "            if isinstance(item, Mapping):\n",
    "                return item\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _persist_results_to_metadata(results_payload, payload_location):\n",
    "    if not table_group_id:\n",
    "        raise ValueError(\"table_group_id must be defined before persisting metadata.\")\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id must be defined before persisting metadata.\")\n",
    "\n",
    "    summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
    "    status = _first_non_empty(summary.get(\"status\"), summary.get(\"state\"), \"completed\")\n",
    "    started_at = _coerce_timestamp_value(summary.get(\"started_at\") or summary.get(\"startedAt\"))\n",
    "    completed_at = _coerce_timestamp_value(summary.get(\"completed_at\") or summary.get(\"completedAt\"))\n",
    "    row_count = _coerce_int(\n",
    "        summary.get(\"row_count\")\n",
    "        or summary.get(\"rowCount\")\n",
    "        or summary.get(\"rows\")\n",
    "        or summary.get(\"total_rows\")\n",
    "        or summary.get(\"totalRows\")\n",
    "    )\n",
    "    anomaly_count = _coerce_int(summary.get(\"anomaly_count\") or summary.get(\"anomalyCount\"))\n",
    "    if anomaly_count is None:\n",
    "        anomalies = summary.get(\"anomalies\")\n",
    "        if isinstance(anomalies, (list, tuple)):\n",
    "            anomaly_count = len(anomalies)\n",
    "\n",
    "    if started_at is None:\n",
    "        started_at = datetime.now(timezone.utc)\n",
    "    if completed_at is None:\n",
    "        completed_at = datetime.now(timezone.utc)\n",
    "\n",
    "    payload_ref = _first_non_empty(payload_location, summary.get(\"payload_path\"), summary.get(\"payloadPath\"))\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "    profile_literal = _sql_literal(profile_run_id)\n",
    "    spark.sql(\n",
    "        f\"DELETE FROM {profiles_table} WHERE {_escape_identifier('profile_run_id')} = {profile_literal}\"\n",
    "    )\n",
    "\n",
    "    columns = (\n",
    "        \"profile_run_id\",\n",
    "        \"table_group_id\",\n",
    "        \"status\",\n",
    "        \"started_at\",\n",
    "        \"completed_at\",\n",
    "        \"row_count\",\n",
    "        \"anomaly_count\",\n",
    "        \"payload_path\",\n",
    "        \"databricks_run_id\",\n",
    "    )\n",
    "    values = [\n",
    "        _sql_literal(profile_run_id),\n",
    "        _sql_literal(table_group_id),\n",
    "        _sql_literal(status),\n",
    "        _sql_literal(started_at),\n",
    "        _sql_literal(completed_at),\n",
    "        _sql_number(row_count),\n",
    "        _sql_number(anomaly_count),\n",
    "        _sql_literal(payload_ref),\n",
    "        _sql_literal(_resolve_databricks_run_id()),\n",
    "    ]\n",
    "    columns_sql = \", \".join(_escape_identifier(column) for column in columns)\n",
    "    values_sql = \", \".join(values)\n",
    "    spark.sql(f\"INSERT INTO {profiles_table} ({columns_sql}) VALUES ({values_sql})\")\n",
    "\n",
    "    ref_label = payload_ref or \"inline\"\n",
    "    print(\n",
    "        f\"Persisted metadata for profile run {profile_run_id} with status '{status}' and payload reference {ref_label}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84335550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata persistence refinements to avoid duplicate dq_profiles rows\n",
    "if \"_persist_results_to_metadata\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Profiling notebook metadata helpers must run before overriding _persist_results_to_metadata.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _persist_results_to_metadata(results_payload, payload_location):\n",
    "    \"\"\"Persist profiling run metadata using a MERGE to update the existing row.\"\"\"\n",
    "    if not table_group_id:\n",
    "        raise ValueError(\"table_group_id must be defined before persisting metadata.\")\n",
    "    if not profile_run_id:\n",
    "        raise ValueError(\"profile_run_id must be defined before persisting metadata.\")\n",
    "\n",
    "    summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
    "    status = _first_non_empty(summary.get(\"status\"), summary.get(\"state\"), \"completed\")\n",
    "    started_at = _coerce_timestamp_value(summary.get(\"started_at\") or summary.get(\"startedAt\"))\n",
    "    completed_at = _coerce_timestamp_value(summary.get(\"completed_at\") or summary.get(\"completedAt\"))\n",
    "    row_count = _coerce_int(\n",
    "        summary.get(\"row_count\")\n",
    "        or summary.get(\"rowCount\")\n",
    "        or summary.get(\"rows\")\n",
    "        or summary.get(\"total_rows\")\n",
    "        or summary.get(\"totalRows\")\n",
    "    )\n",
    "    anomaly_count = _coerce_int(summary.get(\"anomaly_count\") or summary.get(\"anomalyCount\"))\n",
    "    if anomaly_count is None:\n",
    "        anomalies = summary.get(\"anomalies\")\n",
    "        if isinstance(anomalies, (list, tuple)):\n",
    "            anomaly_count = len(anomalies)\n",
    "\n",
    "    if started_at is None:\n",
    "        started_at = datetime.now(timezone.utc)\n",
    "    if completed_at is None:\n",
    "        completed_at = datetime.now(timezone.utc)\n",
    "\n",
    "    payload_ref = _first_non_empty(payload_location, summary.get(\"payload_path\"), summary.get(\"payloadPath\"))\n",
    "    profiles_table = _metadata_table(\"dq_profiles\")\n",
    "\n",
    "    supported_columns = set()\n",
    "    try:\n",
    "        schema = spark.table(profiles_table).schema\n",
    "        supported_columns = {field.name for field in schema}\n",
    "    except Exception as exc:  # pragma: no cover - descriptive logging only\n",
    "        print(f\"[metadata] Unable to inspect schema for {profiles_table}: {exc}\")\n",
    "\n",
    "    if \"payload_path\" not in supported_columns:\n",
    "        with suppress(Exception):\n",
    "            spark.sql(f\"ALTER TABLE {profiles_table} ADD COLUMNS (payload_path STRING)\")\n",
    "            try:\n",
    "                schema = spark.table(profiles_table).schema\n",
    "                supported_columns = {field.name for field in schema}\n",
    "            except Exception as schema_exc:  # pragma: no cover - descriptive logging only\n",
    "                print(f\"[metadata] Unable to refresh schema for {profiles_table}: {schema_exc}\")\n",
    "\n",
    "    def _column_supported(column: str) -> bool:\n",
    "        return not supported_columns or column in supported_columns\n",
    "\n",
    "    column_specs = [\n",
    "        (\"profile_run_id\", _sql_literal(profile_run_id)),\n",
    "        (\"table_group_id\", _sql_literal(table_group_id)),\n",
    "        (\"status\", _sql_literal(status)),\n",
    "        (\"started_at\", _sql_literal(started_at)),\n",
    "        (\"completed_at\", _sql_literal(completed_at)),\n",
    "        (\"row_count\", _sql_number(row_count)),\n",
    "        (\"anomaly_count\", _sql_number(anomaly_count)),\n",
    "        (\"payload_path\", _sql_literal(payload_ref)),\n",
    "        (\"databricks_run_id\", _sql_literal(_resolve_databricks_run_id())),\n",
    "    ]\n",
    "\n",
    "    filtered_specs = []\n",
    "    skipped_columns = []\n",
    "    for column, value in column_specs:\n",
    "        if column != \"profile_run_id\" and not _column_supported(column):\n",
    "            skipped_columns.append(column)\n",
    "            continue\n",
    "        filtered_specs.append((column, value))\n",
    "\n",
    "    if len(filtered_specs) < 2:\n",
    "        raise RuntimeError(\n",
    "            \"Profiling metadata merge could not map any target columns; verify dq_profiles schema permissions.\"\n",
    "        )\n",
    "\n",
    "    column_names = [name for name, _ in filtered_specs]\n",
    "    columns_sql = \", \".join(_escape_identifier(name) for name in column_names)\n",
    "    updates_sql = \", \".join(\n",
    "        f\"target.{_escape_identifier(name)} = source.{_escape_identifier(name)}\"\n",
    "        for name in column_names\n",
    "        if name != \"profile_run_id\"\n",
    "    )\n",
    "    source_select_sql = \", \".join(\n",
    "        f\"{value} AS {_escape_identifier(name)}\" for name, value in filtered_specs\n",
    "    )\n",
    "    insert_values_sql = \", \".join(\n",
    "        f\"source.{_escape_identifier(name)}\" for name in column_names\n",
    "    )\n",
    "\n",
    "    merge_sql = (\n",
    "        f\"MERGE INTO {profiles_table} AS target \"\n",
    "        f\"USING (SELECT {source_select_sql}) AS source \"\n",
    "        f\"ON target.{_escape_identifier('profile_run_id')} = source.{_escape_identifier('profile_run_id')} \"\n",
    "    )\n",
    "    if updates_sql:\n",
    "        merge_sql += f\"WHEN MATCHED THEN UPDATE SET {updates_sql} \"\n",
    "    merge_sql += f\"WHEN NOT MATCHED THEN INSERT ({columns_sql}) VALUES ({insert_values_sql})\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "\n",
    "    if skipped_columns:\n",
    "        skipped = \", \".join(sorted(set(skipped_columns)))\n",
    "        print(f\"[metadata] Skipped unsupported columns during dq_profiles merge: {skipped}\")\n",
    "\n",
    "    ref_label = payload_ref or \"inline\"\n",
    "    print(\n",
    "        f\"Persisted metadata for profile run {profile_run_id} with status '{status}' and payload reference {ref_label}.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd242e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata writer integration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Sequence\n",
    "\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run prior cells before metadata writer integration.\")\n",
    "\n",
    "\n",
    "_ensure_notebook_stage(\"metadata_writer\")\n",
    "\n",
    "\n",
    "_PROFILE_METADATA_WRITER: \"ProfilingMetadataWriter | None\" = None\n",
    "_METADATA_FRAME_SPECS: Sequence[dict[str, Any]] = (\n",
    "    {\n",
    "        \"frame_name\": \"profile_results_df\",\n",
    "        \"table_name\": \"dq_profile_results\",\n",
    "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\"],\n",
    "    },\n",
    "    {\n",
    "        \"frame_name\": \"profile_columns_df\",\n",
    "        \"table_name\": \"dq_profile_columns\",\n",
    "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\"],\n",
    "    },\n",
    "    {\n",
    "        \"frame_name\": \"profile_column_values_df\",\n",
    "        \"table_name\": \"dq_profile_column_values\",\n",
    "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\", \"value_hash\"],\n",
    "    },\n",
    "    {\n",
    "        \"frame_name\": \"profile_anomalies_df\",\n",
    "        \"table_name\": \"dq_profile_anomaly_results\",\n",
    "        \"key_columns\": [\"profile_run_id\", \"table_name\", \"column_name\", \"anomaly_type_id\"],\n",
    "    },\n",
    "    {\n",
    "        \"frame_name\": \"table_characteristics_df\",\n",
    "        \"table_name\": \"dq_data_table_chars\",\n",
    "        \"key_columns\": [\"table_id\"],\n",
    "    },\n",
    "    {\n",
    "        \"frame_name\": \"column_characteristics_df\",\n",
    "        \"table_name\": \"dq_data_column_chars\",\n",
    "        \"key_columns\": [\"column_id\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def _ensure_repo_root_on_path() -> Path | None:\n",
    "    candidates = [Path.cwd(), *Path.cwd().parents]\n",
    "    for candidate in candidates:\n",
    "        app_dir = candidate / \"app\"\n",
    "        if app_dir.exists():\n",
    "            candidate_str = str(candidate)\n",
    "            if candidate_str not in sys.path:\n",
    "                sys.path.insert(0, candidate_str)\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_metadata_writer() -> \"ProfilingMetadataWriter\":\n",
    "    global _PROFILE_METADATA_WRITER\n",
    "    if _PROFILE_METADATA_WRITER is not None:\n",
    "        return _PROFILE_METADATA_WRITER\n",
    "\n",
    "    repo_root = _ensure_repo_root_on_path()\n",
    "    if repo_root is None:\n",
    "        raise RuntimeError(\"Unable to locate repo root containing the 'app' package. Ensure the notebook runs from a Databricks Repo checkout.\")\n",
    "\n",
    "    from app.databricks_profiling import ProfilingMetadataWriter\n",
    "\n",
    "    _PROFILE_METADATA_WRITER = ProfilingMetadataWriter(\n",
    "        spark,\n",
    "        schema=dq_schema,\n",
    "        catalog=connection_catalog or None,\n",
    "        profile_run_id=profile_run_id,\n",
    "    )\n",
    "    return _PROFILE_METADATA_WRITER\n",
    "\n",
    "\n",
    "def _merge_metadata_dataframe(\n",
    "    df,\n",
    "    *,\n",
    "    target_table: str,\n",
    "    key_columns: Sequence[str],\n",
    "    update_columns: Sequence[str] | None = None,\n",
    ") -> int:\n",
    "    if df is None:\n",
    "        return 0\n",
    "    writer = _resolve_metadata_writer()\n",
    "    return writer.merge_dataframe(\n",
    "        df,\n",
    "        target_table=target_table,\n",
    "        key_columns=key_columns,\n",
    "        update_columns=update_columns,\n",
    "    )\n",
    "\n",
    "\n",
    "def _autofill_missing_metadata_frames() -> dict[str, int]:\n",
    "    if \"_autopopulate_metadata_frames\" not in globals():\n",
    "        return {}\n",
    "    missing_frames = [\n",
    "        spec[\"frame_name\"]\n",
    "        for spec in _METADATA_FRAME_SPECS\n",
    "        if spec[\"frame_name\"] not in globals()\n",
    "    ]\n",
    "    if not missing_frames:\n",
    "        return {}\n",
    "    created_counts = _autopopulate_metadata_frames()\n",
    "    for frame_name in missing_frames:\n",
    "        if frame_name in created_counts:\n",
    "            print(\n",
    "                f\"[metadata] Auto-built DataFrame '{frame_name}' with {created_counts[frame_name]} rows before persistence.\"\n",
    "            )\n",
    "    return created_counts\n",
    "\n",
    "\n",
    "def _persist_profiling_metadata() -> dict[str, int]:\n",
    "    autofill_counts = _autofill_missing_metadata_frames()\n",
    "    summary: dict[str, int] = {}\n",
    "    for spec in _METADATA_FRAME_SPECS:\n",
    "        frame_name = spec[\"frame_name\"]\n",
    "        target_table = spec[\"table_name\"]\n",
    "        key_columns = spec[\"key_columns\"]\n",
    "        df = globals().get(frame_name)\n",
    "        if df is None:\n",
    "            summary[frame_name] = 0\n",
    "            if frame_name not in autofill_counts:\n",
    "                print(f\"[metadata] DataFrame '{frame_name}' not defined; skipping {target_table}.\")\n",
    "            continue\n",
    "        rows = _merge_metadata_dataframe(\n",
    "            df,\n",
    "            target_table=target_table,\n",
    "            key_columns=key_columns,\n",
    "            update_columns=spec.get(\"update_columns\"),\n",
    "        )\n",
    "        summary[frame_name] = rows\n",
    "        print(f\"[metadata] Persisted {rows} rows from {frame_name} into {target_table}.\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata payload normalization and DataFrame bootstrap\n",
    "import json\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Profiling notebook Cells 1-7 must run sequentially; run prior cells before metadata frame bootstrap.\",\n",
    "    )\n",
    "\n",
    "_ensure_notebook_stage(\"metadata_writer\")\n",
    "\n",
    "_METADATA_PAYLOAD_CANDIDATES = (\n",
    "    \"profiling_payload\",\n",
    "    \"results_payload\",\n",
    "    \"profile_payload\",\n",
    "    \"profiling_results\",\n",
    "    \"profile_results\",\n",
    "    \"results\",\n",
    ")\n",
    "\n",
    "\n",
    "def _resolve_metadata_payload_value():\n",
    "    for name in _METADATA_PAYLOAD_CANDIDATES:\n",
    "        if name in globals():\n",
    "            value = globals()[name]\n",
    "            if value is not None:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalize_metadata_payload(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        text = value.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        if text.startswith(\"{\") or text.startswith(\"[\"):\n",
    "            try:\n",
    "                return json.loads(text)\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        return None\n",
    "    return value\n",
    "\n",
    "\n",
    "def _autopopulate_metadata_frames() -> dict[str, int]:\n",
    "    missing_frames = [spec[\"frame_name\"] for spec in _METADATA_FRAME_SPECS if spec[\"frame_name\"] not in globals()]\n",
    "    if not missing_frames:\n",
    "        return {}\n",
    "\n",
    "    payload = _normalize_metadata_payload(_resolve_metadata_payload_value())\n",
    "    if payload is None:\n",
    "        print(\"[metadata] Profiling payload unavailable; skipping automatic DataFrame creation.\")\n",
    "        return {}\n",
    "\n",
    "    repo_root = _ensure_repo_root_on_path()\n",
    "    if repo_root is None:\n",
    "        print(\"[metadata] Repo root not found; skipping automatic DataFrame creation.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        from app.databricks_profiling import build_metadata_frames\n",
    "    except Exception as exc:  # pragma: no cover - defensive import guard\n",
    "        print(f\"[metadata] Unable to import profiling frame builder: {exc}\")\n",
    "        return {}\n",
    "\n",
    "    summary = _extract_profile_summary(payload) if \"_extract_profile_summary\" in globals() else {}\n",
    "    try:\n",
    "        frames, counts = build_metadata_frames(\n",
    "            spark,\n",
    "            payload,\n",
    "            profile_run_id=profile_run_id,\n",
    "            table_group_id=table_group_id,\n",
    "            summary=summary,\n",
    "        )\n",
    "    except Exception as exc:  # pragma: no cover - builder errors during notebook execution\n",
    "        print(f\"[metadata] Failed to build profiling DataFrames: {exc}\")\n",
    "        return {}\n",
    "\n",
    "    created_counts: dict[str, int] = {}\n",
    "    for name in missing_frames:\n",
    "        df = frames.get(name)\n",
    "        if df is None:\n",
    "            continue\n",
    "        globals()[name] = df\n",
    "        created_counts[name] = counts.get(name, 0)\n",
    "\n",
    "    if not created_counts:\n",
    "        print(\"[metadata] Profiling payload parsed but produced no rows for the requested frames.\")\n",
    "    return created_counts\n",
    "\n",
    "\n",
    "_METADATA_AUTOFILL_COUNTS = _autopopulate_metadata_frames()\n",
    "if _METADATA_AUTOFILL_COUNTS:\n",
    "    for frame_name, row_count in sorted(_METADATA_AUTOFILL_COUNTS.items()):\n",
    "        print(f\"[metadata] Auto-built DataFrame '{frame_name}' with {row_count} rows from profiling payload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56146ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metadata persistence summary\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "if \"_ensure_notebook_stage\" not in globals():\n",
    "    raise RuntimeError(\"Profiling notebook Cells 1-7 must run sequentially; run prior cells before finalization.\")\n",
    "\n",
    "\n",
    "_ensure_notebook_stage(\"finalization\")\n",
    "\n",
    "\n",
    "def _resolve_results_payload() -> Any:\n",
    "    \"\"\"Pick the richest profiling payload produced by earlier cells.\"\"\"\n",
    "    for name in (\n",
    "        \"results_payload\",\n",
    "        \"profiling_payload\",\n",
    "        \"profile_payload\",\n",
    "        \"profiling_results\",\n",
    "        \"profile_results\",\n",
    "        \"results\",\n",
    "    ):\n",
    "        if name in globals():\n",
    "            return globals()[name]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_payload_reference() -> str | None:\n",
    "    for candidate in (\n",
    "        globals().get(\"persisted_payload_path\"),\n",
    "        globals().get(\"payload_reference\"),\n",
    "        globals().get(\"payload_location\"),\n",
    "        globals().get(\"payload_artifact_path\"),\n",
    "        payload_path,\n",
    "        raw_payload_path,\n",
    "    ):\n",
    "        if isinstance(candidate, str):\n",
    "            normalized = candidate.strip()\n",
    "            if normalized:\n",
    "                return normalized\n",
    "    return None\n",
    "\n",
    "\n",
    "resolved_storage_mode = _resolve_payload_storage_mode()\n",
    "results_payload = _resolve_results_payload()\n",
    "results_summary = _extract_profile_summary(results_payload) if results_payload is not None else {}\n",
    "status = _first_non_empty(results_summary.get(\"status\"), results_summary.get(\"state\"), \"completed\")\n",
    "payload_reference = _resolve_payload_reference()\n",
    "if not payload_reference and _payload_storage_is_artifact(resolved_storage_mode):\n",
    "    payload_reference = payload_path\n",
    "\n",
    "_persist_results_to_metadata(results_payload, payload_reference)\n",
    "\n",
    "metadata_write_counts: dict[str, Any] = {}\n",
    "if \"_persist_profiling_metadata\" in globals():\n",
    "    try:\n",
    "        metadata_write_counts = _persist_profiling_metadata()\n",
    "    except Exception as exc:  # pragma: no cover - defensive logging for notebook runtime\n",
    "        metadata_write_counts = {\"error\": str(exc)}\n",
    "        print(f\"[metadata] Failed to persist profiling metadata: {exc}\")\n",
    "else:\n",
    "    print(\"[metadata] Metadata writer helper not defined; skipping DataFrame persistence.\")\n",
    "\n",
    "FINALIZATION_CONTEXT = {\n",
    "    \"profile_run_id\": profile_run_id,\n",
    "    \"table_group_id\": table_group_id,\n",
    "    \"status\": status,\n",
    "    \"payload_reference\": payload_reference,\n",
    "    \"payload_storage_mode\": resolved_storage_mode,\n",
    "    \"metadata_schema\": dq_schema,\n",
    "    \"databricks_run_id\": _resolve_databricks_run_id(),\n",
    "    \"results_summary\": results_summary,\n",
    "    \"metadata_write_counts\": metadata_write_counts,\n",
    "}\n",
    "\n",
    "print(json.dumps(FINALIZATION_CONTEXT, indent=2, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
