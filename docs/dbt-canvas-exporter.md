# dbt Canvas Exporter Backend Plan

## Overview

Task 8 introduces a backend service that can accept the Canvas graph generated by the React Flow builder and translate it into dbt project artifacts. The long term goal is to feed the exporter output into downstream deployment pipelines, but the initial milestone focuses on deterministic file generation and a simple synchronous API surface.

## Input Contract

The frontend now emits a `CanvasGraph` payload that mirrors the shared TypeScript interfaces. The backend must accept the JSON representation and validate it using new Pydantic schemas. Key elements:

- `metadata`: optional context such as project name, data definition id, generator identity, or timestamps.
- `nodes`: required array describing each table/model. Includes
  - resource identification (`id`, `name`, `label`, `resourceType`)
  - layout information (`position`, `size`)
  - dbt metadata (layer, description, config, columns, hooks, incremental settings)
  - lineage origin (data definition ids, source system info)
- `edges`: required array describing dependencies. Includes join semantics, edge type (`ref`, `source`, etc.), and optional relationship ids.
- `groups` and `warnings`: optional arrays that can drive folder grouping or user feedback.

The Python schemas will live under `app/schemas/canvas.py` and expose types that stay 1:1 with the frontend definitions while still permitting custom/extended strings when needed.

## Output Contract

The exporter should emit an in-memory bundle of dbt files:

| File Type            | Example Location                                   | Notes |
|----------------------|----------------------------------------------------|-------|
| `dbt_project.yml`    | project root                                       | Extends/merges existing project config and lists model/seed paths. |
| Model SQL            | `models/<layer>/<node-name>.sql`                   | Generated for nodes with `resourceType` of `model`, `seed`, or `snapshot`. |
| Source YAML          | `models/sources/<system>.yml` or similar           | Aggregates source definitions and column metadata. |
| Schema YAML          | `models/<layer>/<node-name>.yml`                   | Includes tests, descriptions, tags, owner, access, exposures, metrics. |
| Canvas manifest      | `canvas.json` (optional)                           | Mirrors the input graph for downstream reconciliation. |
| dbt manifest         | `target/manifest.json` (optional)                  | Produced by running `dbt parse`; gated by `DBT_GENERATE_MANIFEST=1`. |

The service will return a `DbtArtifactBundle` describing the file list (path + content) plus any warnings that were raised during translation. A later iteration can stream a zip archive or upload the bundle to object storage.

## Translation Pipeline

1. **Normalize Graph** – Deduplicate nodes/edges, enforce naming rules (slugify, handle collisions), and resolve defaults (e.g., infer model name from label when missing).
2. **Group Nodes** – Map nodes to dbt folders/layers using `layer` metadata, with a fallback based on `resourceType` or canvas groups.
3. **Render Templates** – Use lightweight Jinja templates or string formatters for:
   - SQL skeletons (SELECT from sources, join edges using provided conditions, stub incremental logic)
   - YAML documents for models/sources/tests/exposures/metrics
   - Project configuration fragments
4. **Assemble Bundle** – Collect all generated content alongside optional `canvas.json` and a provenance note (generated timestamp, backend version).
5. **Validate** – Ensure filenames are unique, YAML is parsable, and any unsupported constructs bubble up as warnings instead of hard failures.
6. **Optional Manifest** – When `DBT_GENERATE_MANIFEST=1` (or the translator is initialized with `generate_manifest=True`) the service will materialize the bundle in a temp directory, run `dbt parse`, and append `target/manifest.json` to the results. Failures, missing CLI, or skipped runs surface as warnings on the bundle.
7. **Manifest Mapping** – `ManifestToDataQualityMapper` consumes the generated `manifest.json` and emits `DataQualitySeed` structures (projects, connections, table groups) that plug directly into the existing TestGen provisioning helpers.

## CLI Workflow

Use the helper script to regenerate artifacts from a saved Canvas export:

```bash
python scripts/generate_canvas_assets.py path/to/canvas-graph.json \
  --output-dir dist/dbt \
  --databricks-host "$DATABRICKS_HOST" \
  --databricks-http-path "$DATABRICKS_HTTP_PATH" \
  --databricks-token "$DATABRICKS_TOKEN" \
  --databricks-catalog main \
  --databricks-schema analytics \
  --databricks-dq-schema data_quality
```

The command will:
- Generate the full dbt bundle (SQL, YAML, and `target/manifest.json`).
- Synchronize the manifest into the Databricks/TestGen tables via `sync_canvas_graph_to_testgen`.
- Write the artifacts to `--output-dir` when provided.

Set `--skip-testgen` to limit the run to artifact generation (optionally add `--generate-manifest` to force a local `dbt parse`). Databricks parameters default to `DATABRICKS_*` environment variables when present.

## Follow-Up Enhancements

The current pipeline focuses on deterministic artifact generation and TestGen synchronization. The next milestones aim to bring the backend closer to dbt Canvas parity:

- **Group-aware exports** – map Canvas group metadata into dbt subdirectories and YAML packages so logical areas (domains, layers) surface in both the generated project and downstream manifest seeds.
- **Rich annotations** – persist node and edge notes (free-form comments, owner guidance) into model YAML `meta` blocks and optionally surface them in TestGen descriptions.
- **Expanded exposures & metrics** – translate Canvas exposure/metric definitions into dedicated YAML bundles, ensuring ownership, maturity, and downstream dependency tracking align with dbt conventions.
- **Custom join semantics** – extend SQL templating so annotated joins (filters, join columns) materialize as example queries and are captured in TestGen metadata for lineage.
- **Validation hooks** – add manifest linting to flag unsupported combinations (e.g., missing source references, conflicting group assignments) before artifacts reach CI/CD.

These enhancements will be tackled once the core exporter and TestGen workflow stabilizes in production.

## Environment Setup

End-to-end translator runs (CI or local) require dbt with a DuckDB or Databricks adapter. Use the helper script to bootstrap the environment:

```bash
python scripts/setup_dbt_environment.py --adapter duckdb
```

The script installs `dbt-core`, the requested adapter, and emits a ready-to-use `profiles.yml`. When targeting Databricks, supply warehouse credentials:

```bash
python scripts/setup_dbt_environment.py --adapter databricks \
  --host "$DATABRICKS_HOST" \
  --http-path "$DATABRICKS_HTTP_PATH" \
  --token "$DATABRICKS_TOKEN" \
  --catalog default \
  --schema analytics \
  --profiles-dir ~/.dbt
```

Generated profiles reference the configured catalog/schema and authenticate using the provided token. The CLI returns the directory location so subsequent commands (translator tests, `generate_canvas_assets.py`) can reuse the profile via `DBT_PROFILES_DIR`.

## API Surface

- Endpoint: `POST /data-definitions/{definition_id}/canvas/export`
- Request body: `CanvasGraph`
- Response body: `DbtArtifactBundle` (JSON) or `application/zip` once streaming is added
- Error handling: 400 for validation issues, 422 for unsupported nodes, 500 for renderer errors

The initial implementation will return JSON to keep iteration fast; packaging and storage integrations can follow once the translator stabilizes.

## Open Questions / Follow-Ups

- How should we reconcile existing dbt projects with newly generated content (merge vs overwrite)?
- Do we need environment-specific configs (dev/prod targets) baked into the generated files now or later?
- Should exposures/metrics live alongside models or in dedicated YAML bundles?
- Do we need to support custom materializations beyond the default dbt set in Task 7?
- What level of SQL generation is expected for joins (full SELECT statements vs skeleton `{{ config(...) }}` blocks)?

These decisions can be addressed incrementally once the foundational translator is in place.
