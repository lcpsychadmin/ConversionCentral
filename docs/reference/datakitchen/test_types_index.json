{
  "Aggregate_Balance": {
    "default_parm_columns": "subset_condition,groupby_names,having_condition,match_schema_name,match_table_name,match_column_names,match_subset_condition,match_groupby_names,match_having_condition",
    "default_severity": "Fail",
    "dq_dimension": "Consistency",
    "id": "1500",
    "run_type": "QUERY",
    "source_file": "test_types_Aggregate_Balance.yaml",
    "test_name_long": "Aggregate values per group match reference",
    "test_name_short": "Aggregate Balance",
    "usage_notes": "This test compares sums or counts of a column rolled up to one or more category combinations across two different tables. Both tables must be accessible at the same time. It's ideal for confirming that two datasets exactly match -- that the sum of a measure or count of a value hasn't changed or shifted between categories. Use this test to compare a raw and processed version of the same dataset, or to confirm that an aggregated table exactly matches the detail table that it's built from. An error here means that one or more value combinations fail to match. New categories or combinations will cause failure."
  },
  "Aggregate_Balance_Percent": {
    "default_parm_columns": "subset_condition,groupby_names,having_condition,match_schema_name,match_table_name,match_column_names,match_subset_condition,match_groupby_names,match_having_condition,lower_tolerance,upper_tolerance",
    "default_severity": "Fail",
    "dq_dimension": "Consistency",
    "id": "1504",
    "run_type": "QUERY",
    "source_file": "test_types_Aggregate_Balance_Percent.yaml",
    "test_name_long": "Aggregate measure per group within percent of reference",
    "test_name_short": "Aggregate Balance Percent",
    "usage_notes": "This test compares sums or counts of a column rolled up to one or more category combinations across two different tables. Both tables must be accessible at the same time. Use it to confirm that two datasets closely match within the tolerance you set -- that the sum of a measure or count of a value remains sufficiently consistent between categories. You could use this test compare sales per product within one month to another, when you want to be alerted if the difference for any product falls outside of the range defined as 5% below to 10% above the prior month. An error here means that one or more value combinations fail to match within the set tolerances. New categories or combinations will cause failure."
  },
  "Aggregate_Balance_Range": {
    "default_parm_columns": "subset_condition,groupby_names,having_condition,match_schema_name,match_table_name,match_column_names,match_subset_condition,match_groupby_names,match_having_condition,lower_tolerance,upper_tolerance",
    "default_severity": "Fail",
    "dq_dimension": "Consistency",
    "id": "1505",
    "run_type": "QUERY",
    "source_file": "test_types_Aggregate_Balance_Range.yaml",
    "test_name_long": "Aggregate measure per group within hard range of reference",
    "test_name_short": "Aggregate Balance Range",
    "usage_notes": "This test compares sums or counts of a column rolled up to one or more category combinations across two different tables. Both tables must be accessible at the same time. Use it to confirm that two datasets closely match within the tolerances you define as specific values above or below the aggregate measure for the same categories in the reference dataset -- that the sum of a measure or count of a value remains sufficiently consistent between categories. For instance, you can use this test to compare sales per product within one month to another, when you want to be alerted if the difference for any product falls outside of the range defined as 10000 dollars above or below the prior week. An error here means that one or more value combinations fail to match within the set tolerances. New categories or combinations will cause failure."
  },
  "Aggregate_Minimum": {
    "default_parm_columns": "subset_condition,groupby_names,having_condition,match_schema_name,match_table_name,match_column_names,match_subset_condition,match_groupby_names,match_having_condition",
    "default_severity": "Fail",
    "dq_dimension": "Accuracy",
    "id": "1501",
    "run_type": "QUERY",
    "source_file": "test_types_Aggregate_Minimum.yaml",
    "test_name_long": "Aggregate values per group are at or above reference",
    "test_name_short": "Aggregate Minimum",
    "usage_notes": "This test compares sums or counts of a column rolled up to one or more category combinations, but requires a match or increase in the aggregate value, rather than an exact match, across two different tables. Both tables must be accessible at the same time. Use this to confirm that aggregate values have not dropped for any set of categories, even if some values may rise. This test is useful to compare an older and newer version of a cumulative dataset. An error here means that one or more values per category set fail to match or exceed the prior dataset. New categories or combinations are allowed (but can be restricted independently with a Combo_Match test). Both tables must be present to run this test."
  },
  "Alpha_Trunc": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1004",
    "run_type": "CAT",
    "source_file": "test_types_Alpha_Trunc.yaml",
    "test_name_long": "Maximum character count consistent",
    "test_name_short": "Alpha Truncation",
    "usage_notes": "Alpha Truncation tests that the longest text value in a column hasn't become shorter than the defined threshold, initially 95% of the longest value at baseline. This could indicate a problem in a cumulative dataset, where prior values should still exist unchanged. A failure here would suggest that some process changed data that you would still expect to be present and matching its value when the column was profiled. This test would not be appropriate for an incremental or windowed dataset."
  },
  "Avg_Shift": {
    "default_parm_columns": "baseline_value_ct,baseline_avg,baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Consistency",
    "id": "1005",
    "run_type": "CAT",
    "source_file": "test_types_Avg_Shift.yaml",
    "test_name_long": "Column mean is consistent with reference",
    "test_name_short": "Average Shift",
    "usage_notes": "Average Shift tests that the average of a numeric column has not significantly changed since baseline, when profiling was done. A significant shift may indicate errors in processing, differences in source data, or valid changes that may nevertheless impact assumptions in downstream data products. The test uses Cohen's D, a statistical technique to identify significant shifts in a value. Cohen's D measures the difference between the two averages, reporting results on a standardized scale, which can be interpreted via a rule-of-thumb from small to huge. Depending on your data, some difference may be expected, so it's reasonable to adjust the threshold value that triggers test failure. This test works well for measures, or even for identifiers if you expect them to increment consistently. You may want to periodically adjust the expected threshold, or even the expected average value if you expect shifting over time. Consider this test along with Variability Increase. If variability rises too, process or measurement flaws could be at work. If variability remains consistent, the issue is more likely to be with the source data itself."
  },
  "CUSTOM": {
    "default_parm_columns": "custom_query",
    "default_severity": "Fail",
    "dq_dimension": "Accuracy",
    "id": "1008",
    "run_type": "QUERY",
    "source_file": "test_types_CUSTOM.yaml",
    "test_name_long": "Custom-defined business rule",
    "test_name_short": "Custom Test",
    "usage_notes": "This business-rule test is highly flexible, covering any error state that can be expressed by a SQL query against one or more tables in the database. In operation, the user-defined query is embedded within a parent query returning the count of error rows identified. Any row returned by the query is interpreted as a single error condition in the test. Note that this query is run independently of other tests, and that performance will be slower, depending in large part on the efficiency of the query you write. Interpretation is based on the user-defined meaning of the test. Your query might be written to return errors in individual rows identified by joining tables. Or it might return an error based on a multi-column aggregate condition returning a single row if an error is found. This query is run separately when you click `Review Source Data` from Test Results, so be sure to include enough data in your results to follow-up. Interpretation is based on the user-defined meaning of the test."
  },
  "Combo_Match": {
    "default_parm_columns": "subset_condition,having_condition,match_schema_name,match_table_name,match_groupby_names,match_subset_condition,match_having_condition",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1502",
    "run_type": "QUERY",
    "source_file": "test_types_Combo_Match.yaml",
    "test_name_long": "Column values or combinations found in reference",
    "test_name_short": "Reference Match",
    "usage_notes": "This test verifies that values, or combinations of values, that are present in the main table are also found in a reference table. This is a useful test for referential integrity between fact and dimension tables. You can also use it to confirm the validity of a code or category, or of combinations of values that should only be found together within each record, such as product/size/color.  An error here means that one  or more category combinations in the main table are not found in the reference table. Both tables must be present to run this test."
  },
  "Condition_Flag": {
    "default_parm_columns": "threshold_value,custom_query",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1006",
    "run_type": "CAT",
    "source_file": "test_types_Condition_Flag.yaml",
    "test_name_long": "Column values match pre-defined condition",
    "test_name_short": "Custom Condition",
    "usage_notes": "Custom Condition is a business-rule test for a user-defined error condition based on the value of one or more columns. The condition is applied to each record within the table, and the count of records failing the condition is added up. If that count exceeds a threshold of errors, the test as a whole is failed. This test is ideal for error conditions that TestGen cannot automatically infer, and any condition that involves the values of more than one column in the same record. Performance of this test is fast, since it is performed together with other aggregate tests. Interpretation is based on the user-defined meaning of the test."
  },
  "Constant": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1007",
    "run_type": "CAT",
    "source_file": "test_types_Constant.yaml",
    "test_name_long": "All column values match constant value",
    "test_name_short": "Constant Match",
    "usage_notes": "Constant Match tests that a single value determined to be a constant in baseline profiling is still the only value for the column that appears in subsequent versions of the dataset. Sometimes new data or business knowledge may reveal that the value is not a constant at all, even though only one value was present at profiling. In this case, you will want to disable this test. Alternatively, you can use the Value Match test to provide a limited number of valid values for the column."
  },
  "Daily_Record_Ct": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Completeness",
    "id": "1009",
    "run_type": "CAT",
    "source_file": "test_types_Daily_Record_Ct.yaml",
    "test_name_long": "All dates present within date range",
    "test_name_short": "Daily Records",
    "usage_notes": "Daily Records tests that at least one record is present for every day within the minimum and maximum date range for the column. The test is relevant for transactional data, where you would expect at least one transaction to be recorded each day. A failure here would suggest missing records for the number of days identified without data. You can adjust the threshold to accept a number of days that you know legitimately have no records. "
  },
  "Dec_Trunc": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1011",
    "run_type": "CAT",
    "source_file": "test_types_Dec_Trunc.yaml",
    "test_name_long": "Sum of fractional values at or above reference",
    "test_name_short": "Decimal Truncation",
    "usage_notes": "Decimal Truncation tests that the fractional (decimal) part of a numeric column has not been truncated since Baseline.  This works by summing all the fractional values after the decimal point and confirming that the total is at least equal to the fractional total at baseline.  This could indicate a problem in a cumulative dataset, where prior values should still exist unchanged. A failure here would suggest that some process changed data that you would still expect to be present and matching its value when the column was profiled. This test would not be appropriate for an incremental or windowed dataset."
  },
  "Distinct_Date_Ct": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Timeliness",
    "id": "1012",
    "run_type": "CAT",
    "source_file": "test_types_Distinct_Date_Ct.yaml",
    "test_name_long": "Count of distinct dates at or above reference",
    "test_name_short": "Date Count",
    "usage_notes": "Date Count tests that the count of distinct dates present in the column has not dropped since baseline. The test is relevant for cumulative datasets, where old records are retained. A failure here would indicate missing records, which could be caused by a processing error or changed upstream data sources."
  },
  "Distinct_Value_Ct": {
    "default_parm_columns": "baseline_value_ct,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1013",
    "run_type": "CAT",
    "source_file": "test_types_Distinct_Value_Ct.yaml",
    "test_name_long": "Count of distinct values has not dropped",
    "test_name_short": "Value Count",
    "usage_notes": "Value Count tests that the  count of unique values present in the column has not dropped since baseline. The test is relevant for cumulative datasets, where old records are retained, or for any dataset where you would expect a set number of distinct values should be present. A failure here would indicate missing records or a change in categories or value assignment."
  },
  "Distribution_Shift": {
    "default_parm_columns": "subset_condition,match_schema_name,match_table_name,match_groupby_names,match_subset_condition",
    "default_severity": "Warning",
    "dq_dimension": "Consistency",
    "id": "1503",
    "run_type": "QUERY",
    "source_file": "test_types_Distribution_Shift.yaml",
    "test_name_long": "Probability distribution consistent with reference",
    "test_name_short": "Distribution Shift",
    "usage_notes": "This test measures the similarity of two sets of counts per categories, by using their proportional counts as probability distributions.  Using Jensen-Shannon divergence, a measure of relative entropy or difference between two distributions, the test assigns a score ranging from 0, meaning that the distributions are identical, to 1, meaning that the distributions are completely unrelated. This test can be used to compare datasets that may not match exactly, but should have similar distributions.  For example, it is a useful sanity check for data from different sources that you would expect to have a consistent spread, such as shipment of building materials per state and construction projects by state. Scores can be compared over time even if the distributions are not identical -- a dataset can be expected to maintain a comparable divergence score with a reference dataset over time. Both tables must be present to run this test."
  },
  "Dupe_Rows": {
    "default_parm_columns": "groupby_names",
    "default_severity": "Fail",
    "dq_dimension": "Uniqueness",
    "id": "1510",
    "run_type": "QUERY",
    "source_file": "test_types_Dupe_Rows.yaml",
    "test_name_long": "Rows are not duplicated in table",
    "test_name_short": "Duplicate Rows",
    "usage_notes": "This test verifies that combinations of values are not repeated within the table. By default when auto-generated, the test considers all columns to protect against duplication of entire rows. If you know the minimum columns that should constitute a unique record, such as a set of ID's, you should use those to make the test as sensitive as possible. Alternatively, if you know of columns you can always exclude, such as file_date or refresh_snapshot_id, remove them to tighten the test somewhat."
  },
  "Email_Format": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1014",
    "run_type": "CAT",
    "source_file": "test_types_Email_Format.yaml",
    "test_name_long": "Email is correctly formatted",
    "test_name_short": "Email Format",
    "usage_notes": null
  },
  "Future_Date": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Timeliness",
    "id": "1015",
    "run_type": "CAT",
    "source_file": "test_types_Future_Date.yaml",
    "test_name_long": "Latest date is prior to test run date",
    "test_name_short": "Past Dates",
    "usage_notes": null
  },
  "Future_Date_1Y": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Timeliness",
    "id": "1016",
    "run_type": "CAT",
    "source_file": "test_types_Future_Date_1Y.yaml",
    "test_name_long": "Future dates within year of test run date",
    "test_name_short": "Future Year",
    "usage_notes": "Future Year looks for date values in the column that extend beyond one year after the test date. This would be appropriate for transactional dates where you would expect to find dates in the  near future, but not beyond one year ahead.  Errors could indicate invalid entries or possibly dummy dates representing blank values."
  },
  "Incr_Avg_Shift": {
    "default_parm_columns": "baseline_value_ct,baseline_sum,baseline_avg,baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Accuracy",
    "id": "1017",
    "run_type": "CAT",
    "source_file": "test_types_Incr_Avg_Shift.yaml",
    "test_name_long": "New record mean is consistent with reference",
    "test_name_short": "New Shift",
    "usage_notes": "This is a more sensitive test than Average Shift, because it calculates an incremental difference in the average of new values compared to the average of values at baseline. This is appropriate for a cumulative dataset only, because it calculates the average of new entries based on the assumption that the count and average of records present at baseline are still present at the time of the test. This test compares the mean of new values with the standard deviation of the baseline average to calculate a Z-score.  If the new mean falls outside the Z-score threshold, a shift is detected. Potential Z-score thresholds may range from 0 to 3, depending on the sensitivity you prefer.  A failed test could indicate a quality issue or a legitimate shift in new data that should be noted and assessed by business users. Consider this test along with Variability Increase. If variability rises too, process, methodology or measurement flaws could be at issue. If variability remains consistent, the problem is more likely to be with the source data itself."
  },
  "LOV_All": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1018",
    "run_type": "CAT",
    "source_file": "test_types_LOV_All.yaml",
    "test_name_long": "List of expected values all present in column",
    "test_name_short": "Value Match All",
    "usage_notes": "This is a more restrictive form of Value Match, testing that all values in the dataset match the list provided, and also that all values present in the list appear at least once in the dataset. This would be appropriate for tables where all category values in the column are represented at least once."
  },
  "LOV_Match": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1019",
    "run_type": "CAT",
    "source_file": "test_types_LOV_Match.yaml",
    "test_name_long": "All column values present in expected list",
    "test_name_short": "Value Match",
    "usage_notes": "This tests that all values in the column match the hard-coded list provided. This is relevant when the list of allowable values is small and not expected to change often. Even if new values might occasionally be added, this test is useful for downstream data products to provide warning that assumptions and logic may need to change."
  },
  "Min_Date": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1020",
    "run_type": "CAT",
    "source_file": "test_types_Min_Date.yaml",
    "test_name_long": "All dates on or after set minimum",
    "test_name_short": "Minimum Date",
    "usage_notes": "This test is appropriate for a cumulative dataset only, because it assumes all prior values are still present. It's appropriate where new records are added with more recent dates, but old dates dates do not change."
  },
  "Min_Val": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1021",
    "run_type": "CAT",
    "source_file": "test_types_Min_Val.yaml",
    "test_name_long": "All values at or above set minimum",
    "test_name_short": "Minimum Value",
    "usage_notes": "This test is appropriate for a cumulative dataset only, assuming all prior values are still present. It is also appropriate for any measure that has an absolute, definable minimum value, or a heuristic that makes senes for valid data."
  },
  "Missing_Pct": {
    "default_parm_columns": "baseline_ct,baseline_value_ct,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Completeness",
    "id": "1022",
    "run_type": "CAT",
    "source_file": "test_types_Missing_Pct.yaml",
    "test_name_long": "Consistent ratio of missing values",
    "test_name_short": "Percent Missing",
    "usage_notes": "This test uses Cohen's H, a statistical test to identify a significant difference between two ratios.  Results are reported on a standardized scale, which can be interpreted via a rule-of-thumb from small to huge.  An uptick in missing data may indicate a collection issue at the source.  A larger change may indicate a processing failure. A drop in missing data may also be significant, if it affects assumptions built into analytic products downstream. You can refine the expected threshold value as you view legitimate results of the measure over time."
  },
  "Monthly_Rec_Ct": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Completeness",
    "id": "1023",
    "run_type": "CAT",
    "source_file": "test_types_Monthly_Rec_Ct.yaml",
    "test_name_long": "At least one date per month present within date range",
    "test_name_short": "Monthly Records",
    "usage_notes": "Monthly Records tests that at least one record is present for every calendar month within the minimum and maximum date range for the column. The test is relevant for transactional data, where you would expect at least one transaction to be recorded each month. A failure here would suggest missing records for the number of months identified without data. You can adjust the threshold to accept a number of month that you know legitimately have no records."
  },
  "Outlier_Pct_Above": {
    "default_parm_columns": "baseline_avg,baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Accuracy",
    "id": "1024",
    "run_type": "CAT",
    "source_file": "test_types_Outlier_Pct_Above.yaml",
    "test_name_long": "Consistent outlier counts over 2 SD above mean",
    "test_name_short": "Outliers Above",
    "usage_notes": "This test counts the number of data points that may be considered as outliers, determined by whether their value exceeds 2 standard deviations above the mean at baseline.  Assuming a normal distribution, a small percentage (defaulted to 5%) of outliers is expected. The actual number may vary for different distributions. The expected threshold reflects the maximum percentage of outliers you expect to see.  This test uses the baseline mean rather than the mean for the latest dataset to capture systemic shift as well as individual outliers. "
  },
  "Outlier_Pct_Below": {
    "default_parm_columns": "baseline_avg,baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Accuracy",
    "id": "1025",
    "run_type": "CAT",
    "source_file": "test_types_Outlier_Pct_Below.yaml",
    "test_name_long": "Consistent outlier counts under 2 SD below mean",
    "test_name_short": "Outliers Below",
    "usage_notes": "This test counts the number of data points that may be considered as outliers, determined by whether their value exceeds 2 standard deviations below the mean at baseline.  Assuming a normal distribution, a small percentage (defaulted to 5%) of outliers is expected. The actual number may vary for different distributions. The expected threshold reflects the maximum percentage of outliers you expect to see.  This test uses the baseline mean rather than the mean for the latest dataset to capture systemic shift as well as individual outliers. "
  },
  "Pattern_Match": {
    "default_parm_columns": "baseline_value,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1026",
    "run_type": "CAT",
    "source_file": "test_types_Pattern_Match.yaml",
    "test_name_long": "Column values match alpha-numeric pattern",
    "test_name_short": "Pattern Match",
    "usage_notes": "This test is appropriate for character fields that are expected to appear in a consistent format. It uses pattern matching syntax as appropriate for your database:  REGEX matching if available, otherwise LIKE expressions. The expected threshold is the number of records that fail to match the defined pattern."
  },
  "Recency": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Timeliness",
    "id": "1028",
    "run_type": "CAT",
    "source_file": "test_types_Recency.yaml",
    "test_name_long": "Latest date within expected range of test date",
    "test_name_short": "Recency",
    "usage_notes": "This test evaluates recency based on the latest referenced dates in the column.  The test is appropriate for transactional dates and timestamps.  The test can be especially valuable because timely data deliveries themselves may not assure that the most recent data is present. You can adjust the expected threshold to the maximum number of days that you expect the data to age before the dataset is refreshed."
  },
  "Required": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Completeness",
    "id": "1030",
    "run_type": "CAT",
    "source_file": "test_types_Required.yaml",
    "test_name_long": "Required non-null value present",
    "test_name_short": "Required Entry",
    "usage_notes": null
  },
  "Row_Ct": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Completeness",
    "id": "1031",
    "run_type": "CAT",
    "source_file": "test_types_Row_Ct.yaml",
    "test_name_long": "Number of rows is at or above threshold",
    "test_name_short": "Row Count",
    "usage_notes": "Because this tests the row count against a constant minimum threshold, it's appropriate for any dataset, as long as the number of rows doesn't radically change from refresh to refresh.  But it's not responsive to change over time. You may want to adjust the threshold periodically if you are dealing with a cumulative dataset."
  },
  "Row_Ct_Pct": {
    "default_parm_columns": "baseline_ct,threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Completeness",
    "id": "1032",
    "run_type": "CAT",
    "source_file": "test_types_Row_Ct_Pct.yaml",
    "test_name_long": "Number of rows within percent range of threshold",
    "test_name_short": "Row Range",
    "usage_notes": "This test is better than Row Count for an incremental or windowed dataset where you would expect the row count to range within a percentage of baseline."
  },
  "Street_Addr_Pattern": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1033",
    "run_type": "CAT",
    "source_file": "test_types_Street_Addr_Pattern.yaml",
    "test_name_long": "Enough street address entries match defined pattern",
    "test_name_short": "Street Address",
    "usage_notes": "The street address pattern used in this test should match the vast majority of USA addresses.  You can adjust the threshold percent of matches based on the results you are getting -- you may well want to tighten it to make the test more sensitive to invalid entries."
  },
  "Table_Freshness": {
    "default_parm_columns": "history_calculation,history_lookback,subset_condition,custom_query",
    "default_severity": "Log",
    "dq_dimension": "Recency",
    "id": "1511",
    "run_type": "QUERY",
    "source_file": "test_types_Table_Freshness.yaml",
    "test_name_long": "Stale Table Not Updated",
    "test_name_short": "Table Freshness",
    "usage_notes": "This test compares the current table fingerprint, calculated signature of column contents, to confirm that the table has been updated. The table fingerprint is derived from a set of values and aggregates from columns most likely to change. This test allows you to track the schedule and frequency of updates and refreshes to the table."
  },
  "Timeframe_Combo_Gain": {
    "default_parm_columns": "window_date_column,window_days,subset_condition",
    "default_severity": "Fail",
    "dq_dimension": "Consistency",
    "id": "1508",
    "run_type": "QUERY",
    "source_file": "test_types_Timeframe_Combo_Gain.yaml",
    "test_name_long": "Latest timeframe has at least all value combinations from prior period",
    "test_name_short": "Timeframe No Drops",
    "usage_notes": "This test checks a single transactional table to verify that categorical values or combinations that are present in the most recent time window you define include at least all those found in the prior time window of the same duration. Missing values in the latest time window will trigger the test to fail. New values are permitted. Use this test to confirm that codes or categories are not lost across successive time periods in a transactional table."
  },
  "Timeframe_Combo_Match": {
    "default_parm_columns": "window_date_column,window_days,subset_condition",
    "default_severity": "Fail",
    "dq_dimension": "Consistency",
    "id": "1509",
    "run_type": "QUERY",
    "source_file": "test_types_Timeframe_Combo_Match.yaml",
    "test_name_long": "Column value combinations from latest timeframe same as prior period",
    "test_name_short": "Timeframe Match",
    "usage_notes": "This test checks a single transactional table (such as a fact table) to verify that categorical values or combinations that are present in the most recent time window you define match those found in the prior time window of the same duration. New or missing values in the latest time window will trigger the test to fail. Use this test to confirm the consistency in the occurrence of codes or categories across successive time periods in a transactional table."
  },
  "US_State": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1036",
    "run_type": "CAT",
    "source_file": "test_types_US_State.yaml",
    "test_name_long": "Column value is two-letter US state code",
    "test_name_short": "US State",
    "usage_notes": "This test validates entries against a fixed list of two-character US state codes and related Armed Forces codes."
  },
  "Unique": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Uniqueness",
    "id": "1034",
    "run_type": "CAT",
    "source_file": "test_types_Unique.yaml",
    "test_name_long": "Each column value is unique",
    "test_name_short": "Unique Values",
    "usage_notes": "This test is ideal when the database itself does not enforce a primary key constraint on the table. It serves as an independent check on uniqueness.  If's also useful when there are a small number of exceptions to uniqueness, which can be reflected in the expected threshold count of duplicates."
  },
  "Unique_Pct": {
    "default_parm_columns": "baseline_value_ct,baseline_unique_ct,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Uniqueness",
    "id": "1035",
    "run_type": "CAT",
    "source_file": "test_types_Unique_Pct.yaml",
    "test_name_long": "Consistent ratio of unique values",
    "test_name_short": "Percent Unique",
    "usage_notes": "You can think of this as a test of similarity that measures whether the percentage of unique values is consistent with the percentage at baseline.  A significant change might indicate duplication or a telling shift in cardinality between entities. The test uses Cohen's H, a statistical test to identify a significant difference between two ratios.  Results are reported on a standardized scale, which can be interpreted via a rule-of-thumb from small to huge.  You can refine the expected threshold value as you view legitimate results of the measure over time."
  },
  "Valid_Characters": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Validity",
    "id": "1043",
    "run_type": "CAT",
    "source_file": "test_types_Valid_Characters.yaml",
    "test_name_long": "Column contains no invalid characters",
    "test_name_short": "Valid Characters",
    "usage_notes": "This test looks for the presence of non-printing ASCII characters that are considered non-standard in basic text processing. It also identifies leading spaces and values enclosed in quotes. Values that fail this test may be artifacts of data conversion, or just more difficult to process or analyze downstream."
  },
  "Valid_Month": {
    "default_parm_columns": "threshold_value,baseline_value",
    "default_severity": "Fail",
    "dq_dimension": "Validity",
    "id": "1042",
    "run_type": "CAT",
    "source_file": "test_types_Valid_Month.yaml",
    "test_name_long": "Valid calendar month in expected format",
    "test_name_short": "Valid Month",
    "usage_notes": null
  },
  "Valid_US_Zip": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Validity",
    "id": "1044",
    "run_type": "CAT",
    "source_file": "test_types_Valid_US_Zip.yaml",
    "test_name_long": "Valid USA Postal Codes",
    "test_name_short": "Valid US Zip",
    "usage_notes": null
  },
  "Valid_US_Zip3": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Validity",
    "id": "1045",
    "run_type": "CAT",
    "source_file": "test_types_Valid_US_Zip3.yaml",
    "test_name_long": "Valid USA Zip-3 Prefix",
    "test_name_short": "Valid US Zip-3  ",
    "usage_notes": "This test looks for the presence of values that fail to match the three-digit numeric code expected for US Zip Code regional prefixes. These prefixes are often used to roll up Zip Code data to a regional level, and may be critical to anonymize detailed data and protect PID. Depending on your needs and regulatory requirements, longer zip codes could place PID at risk."
  },
  "Variability_Decrease": {
    "default_parm_columns": "baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Accuracy",
    "id": "1041",
    "run_type": "CAT",
    "source_file": "test_types_Variability_Decrease.yaml",
    "test_name_long": "Variability has decreased below threshold",
    "test_name_short": "Variability Decrease",
    "usage_notes": "This test looks for percent shifts in standard deviation as a measure of the stability of a measure over time.  A significant change could indicate that new values are erroneous, or that the cohort being evaluated is significantly different from baseline.  A decrease in particular could indicate an improved process, better precision in measurement, the elimination of outliers, or a more homogeneous cohort. "
  },
  "Variability_Increase": {
    "default_parm_columns": "baseline_sd,threshold_value",
    "default_severity": "Warning",
    "dq_dimension": "Accuracy",
    "id": "1040",
    "run_type": "CAT",
    "source_file": "test_types_Variability_Increase.yaml",
    "test_name_long": "Variability has increased above threshold",
    "test_name_short": "Variability Increase",
    "usage_notes": "This test looks for percent shifts in standard deviation as a measure of the stability of a measure over time.  A significant change could indicate that new values are erroneous, or that the cohort being evaluated is significantly different from baseline.  An increase in particular could mark new problems in measurement,  a more heterogeneous cohort, or that significant outliers have been introduced. Consider this test along with Average Shift and New Shift.  If the average shifts as well, there may be a fundamental shift in the dataset or process used to collect the data point.  This might suggest a data shift that should be noted and assessed by business users. If the average does not shift, this may point to a data quality or data collection problem. "
  },
  "Weekly_Rec_Ct": {
    "default_parm_columns": "threshold_value",
    "default_severity": "Fail",
    "dq_dimension": "Completeness",
    "id": "1037",
    "run_type": "CAT",
    "source_file": "test_types_Weekly_Rec_Ct.yaml",
    "test_name_long": "At least one date per week present within date range",
    "test_name_short": "Weekly Records",
    "usage_notes": "Weekly Records tests that at least one record is present for every calendar week within the minimum and maximum date range for the column. The test is relevant for transactional data, where you would expect at least one transaction to be recorded each week. A failure here would suggest missing records for the number of weeks identified without data. You can adjust the threshold to accept a number of weeks that you know legitimately have no records."
  }
}