--- Instructions --
heroku apps:
conversion-central-backend
conversion-central-frontend

--- git commands ---
git add . 
git commit -m "Profiling Notebook Update"
git push origin main

Steps:
1. After executing each task run lint and tests with the following commands:
cd "c:\Users\jwesc\OneDrive\Documents\VSCode Workspace\ConversionCentral\frontend"; npm.cmd run lint
cd "c:\Users\jwesc\OneDrive\Documents\VSCode Workspace\ConversionCentral"; py -m pytest
2. Correct errors and run lint and tests again to confirm
3. Run any migrations that are required
4. Then rebuild and bring up the containers with the following command:
cd "c:\Users\jwesc\OneDrive\Documents\VSCode Workspace\ConversionCentral"; docker compose up --build -d
5. Correct any issues identified and then rebuild and bring up the containers to confirm
6. I will test and confirm the changes are correct or offer any adjustemnts that need to be made
7. Once I confirm we can mark task in todo.txt as [DONE]
8. After all piror steps succeed commit and push the application changes to git
9. Once git is updated we can proceed to the next task that is not marked [DONE]

--- Enhancements to Existing Code ---

1. [DONE] combine applications and the source catalog into one page and move the page to the Data Connections Menu Item and call it Source Systems
2. [DONE] Reorder the Page Links under Data Connections as follows
	a. Application Database
	b. Data Warehouse
	c. Source Systems
	d. Ingestion Schedules
3. [DONE] Add in better filter capabilities in the ingestion schedules page. This will grow as we add more source systems into the application. Perhaps add in
a Tree view at the top that is by source system, connection, and then schema
4. [DONE] in the data definition table set Enterprise Attribute to a Yes/No field. The field type should be a drop down of the available data types in the Databricks SQL Warehouse. Grey out Decimal places unless a float/decimal field type is used. Legal/Regulatory Requirements and Security Classifications should be dropdown boxes based on new models for each. The admin pages for both should be under the application settings menu item.
5. [DONE] On the report designer page, rename the publish to data object button to publish to report catalog
6. [DONE] Ensure that the sql that is run in the reporting designer runs on the databricks server. This will mean that we need to change the tables that are available for reporting should only be from tables in databricks
7. [DONE] Remove the add table button from the data defintion creation process, we should only have two option; Add source table or create table. 
8. [DONE] The amount of tables that will be available in the create data definition modal could be very large at some point in the future and having just a dropdown will not work. Can we add in functionality, possibley a tree structure
that is by Datbricks Schema, source system, source system schema, and then table. We should also add in the abilty to be able to select multiple tables at once. Also, any table that is already in the data definiiton should not show
9. [DONE] on the create/edit data definintion we can remove the ability to rearrange the fields since we can now do this in the grid. We can remove the fields displayed entirely from this modal
10. [DONE] Add a sub menu item under Data Configuration labled "Data Definition Settings" and move the Legal Requirements and SEcurity Classifications page under this menu
11. [DONE] Move the Data Management menu item below Data Configuration
12. [DONE] On the report catalog page the refresh, expand all, and collapse all actions are overlaying other sections. Perhaps we move these actions to below the header and consolidate the expand and collapse into one action
13. [DONE] Add in site styled delete confirmation on the report designer when removing a table from the canvas
14. [DONE] Add in site styled delete confirmation on the report catalog when deleteing a report
15. [DONE] Fix the length and decimal places fields in the create new field modal in the data definition page to align with the inputs above and below
16. [DONE] Rework table ingestion to create schemas in databricks for each source system instead of appending the source system to the table name. Tables would then be ingested into databricks based on the system assigned to the connection.
we can use the schema defined in the databricks warehouse settings as a fallback.
17. [DONE] Unselecting a table from the source catalog should cascade the delete to the ingestion schedules, data definitions, and reports in the catalog. If an ingestion schedule had been crated then the table should also be dropped from databricks.
18. [DONE] remove "No tables are available for the selected system yet. Use the Create Table action below to add one before building the data definition." from the Edit data definition modal. Also, only new tables or tables from the constructed schema should have the constructed data toggle
19. [DONE] Move Data Definition Settings as the first sub menu item under data configuration.
20. [DONE] In the report designer table pallete can we add an unassigned section to the tree and include any table from the dtaabricks source catalag that is not assigned to a data object
21. [DONE] Move the product teams menu item to the Data Configuration Top menu, place it above data objects
22. [DONE] Export to CSV is not working on the report catalog "Failed to execute 'createObjectURL' on 'URL': Overload resolution failed."
23. [DONE] The refresh when selecting tables in the source catalog takes a long time. Can we add a save button, so the user can select multiple tables without the refresh and then hit save.
24. [DONE] In the create new field data definition modal, the enterprise attribute dropdown should be changed to a checkbox and moved to where the other checkboxes are in the form.
25. [DONE] Application needs to run on https
26. [DONE] Is it possible to push the docker file to heroku

--- Data Quality Implementation ---
https://github.com/DataKitchen/dataops-testgen
This is the URL for Open Source Data Quality Application that I would like to implement into our application. Some things will need to change to fit into our data hierarchy, but the functionality that is here is what I want to implement.
I also have this application running locally at http://localhost:8501/
1. [DONE] Can you review the code and design a plan on how we can integrate this funtionality into our application. I do know that we will have one top level menu item called "Data Quality" all other sub menus and pages will reside under this.

Detailed Implementation Plan:
1. [DONE] Inventory the DataOps TestGen repositories (UI, CLI, Docker) and document integration points we must preserve (profiling, test generation, run history, alerts).
2. [DONE] Design a Databricks-backed persistence layer that mirrors TestGen metadata tables; define schema defaults and configurable name exposed in warehouse settings.
3. [DONE] Extend Databricks warehouse settings API/UI to capture the Data Quality schema name (with validation and rename flow) and propagate it through existing configuration services.
4. [DONE] Build a provisioning job that creates/updates the Data Quality schema in Databricks, seeds required tables, and stores dataset-to-testgen mappings tied to our systems and connections.
5. [DONE] Implement a backend client/service that wraps TestGen functionality (CLI or API) while redirecting storage operations to Databricks and authenticating with managed warehouse credentials.
6. [DONE] Create FastAPI routes under `/data-quality` for datasets, tests, runs, alerts, and actions, enforcing RBAC and surfacing status sourced from the new Databricks schema.
7. [DONE] Wire ingestion/validation workflows so new or updated system connections automatically register datasets and so post-ingestion steps trigger TestGen validation runs.
8. [DONE] Develop the React `Data Quality` menu with sub-pages (Overview, Datasets, Test Library, Run History & Alerts, Settings) that consume the new endpoints and cross-link to Source Systems/Data Definitions.
9. [DONE] Add notifications/webhooks so TestGen anomalies flow into our alerting pipeline and optionally block downstream approvals when severity thresholds are met.
10. Expand CI/CD to include the Data Quality service/container (if needed), add integration tests against a Databricks test schema, run full lint/pytest/npm suites, and update deployment docs for local/Heroku setups.

dbt canvas model upgrade:
1. [DONE] Capture baseline of current table relationship builder graph payload and document required metadata for dbt Canvas (node types, coordinates, edge semantics, join details).
2. [DONE] Research dbt Canvas JSON schema, sample exports, and minimum viable fields; archive references in docs/ for the team.
3. [DONE] Define a shared TypeScript interface for a "canvas graph" (nodes, edges, dbt attributes) that can be serialized by the UI and processed by the backend.
4. [DONE] Extend React Flow node/edge state to store dbt-specific settings (model layer, materialization, tags, column test flags, descriptions, exposures/metrics).
5. [DONE] Build UI controls (side panel or modal) for editing the new dbt attributes on tables and joins, including validation and sensible defaults.
6. [DONE] Add transformation-focused controls (materialization options, pre/post hooks, incremental settings) so Canvas users can manage dbt transformation behavior directly from the relationship builder.
7. [DONE] Implement a frontend exporter that converts the React Flow state into the shared canvas graph structure, preserving positions and metadata.
8. [DONE] Add a backend service that accepts the canvas graph and generates dbt artifacts (dbt_project.yml, sources, models, schema yaml, canvas.json).
9. [DONE] Create automated tests/fixtures that feed sample graphs through the translator and verify the resulting dbt files compile (`dbt parse`) and match expectations.
10. [DONE] Integrate generated dbt outputs into the existing TestGen seeding flow (ingest manifest.json to refresh projects/connections/table groups).
11. [DONE] Provide a CLI/script entry point to regenerate dbt + TestGen assets from a data definition, and document the workflow in README/docs.
12. [DONE] Update CI/CD to run the new translator tests, bundle artifacts as needed, and validate the dbt project builds without breaking existing pipelines.
13. [DONE] Plan follow-up enhancements (grouping, annotations, exposures) to reach feature parity with dbt Canvas UX once the core pipeline works.
14. [In Progress] Capture dbt environment prerequisites (install dbt-core + dbt-databricks, configure profiles.yml for the Databricks warehouse) and automate the bootstrap so all translator commands run against Databricks.
15. Reframe the relationship builder state so tables map into source, transform, and output groups while keeping the Canvas export backwards compatible.
16. Replace the current input drawer with a docked palette that exposes source tables, transform steps, and output targets with quick-add actions.
17. Introduce distinct React Flow node types for source, transform, and output (plus optional metric/exposure) with tier-specific styling and mini toolbars.
18. Enhance drag-and-drop joins: highlight valid drop zones, auto-launch the join dialog on connect, and preserve existing field drag semantics.
19. Split the Canvas side panel into node metadata vs. joins/tests tabs so dbt attributes, hooks, and column settings stay organized.
20. Add layout aids (swim lanes, snap-to-layer alignment, mini-map overlays) to make transform/output tiers obvious on the canvas.
21. Extend the Canvas exporter to capture the new metadata and validate the backend translator still emits correct dbt artifacts.
22. Wrap the UX behind a feature flag, add focused unit/E2E coverage for the new interactions, and document rollout steps in docs/dbt-canvas-model-upgrade.md.



Refactor Initial Build
I've created a folder that contains screenshots from the testgen system that we will use for reference
1. [Done] Update datasets to be based on the Product Team, Application, Data Object Hierarchy with the Data Defintion tables being the lowest level.
2. [Done] Update run history & alerts to utilize the new dataset hierarchy.
3. We need a place where we can create, view, and modify test suites. These test suites will be the definition of what the test runs will execute. landing page screenshot is located at /screenshots/data quality/test suites.png 
4. When creating a new test suite we will capture the following information test suite name, test suite description, application/data object, and severity. screenshot is located at /screenshots/data quality/test suite creat modal.png
5. Next we need to build the test suite definition page, a link to this will be on the test suite landing page for each of the test suites. This page will let us define by table/field what tests to run. the landing page screenshot is at /screenshots/data quality/test suite definition.png
6. When adding a new test we'll be presented with a number of different test types (/screenshots/data quality/test types.txt). AFter selecting the type the modal will display the fields required for that test type (/screenshots/data quality/test suite create modal.png)
7. when selecting a test from the test suite definition page details will show below the table at the bottom (/screenshots/data quality/test suite definition details.png). There is also an button to view the column profiling for this test (/screenshots/data quality/test suite column profiling)


--- Data Migration Implementation ---
https://github.com/dbt-labs/dbt-core





--- SSO, Users, Roles, and Security ---






--- Approvals & Workflows ---


heroku config:set DATABRICKS_PROFILE_NOTEBOOK_PATH=/Repos/<workspace>/<repo>/profiling.py --app conversion-central-backend
/Workspace/Users/j.wes.collins@outlook.com/conversion-central/data quality/data profiling
heroku config:set DATABRICKS_PROFILE_POLICY_ID=<policy-id> --app conversion-central-backend
:: or, if you already have a dedicated cluster:
heroku config:set DATABRICKS_PROFILE_EXISTING_CLUSTER_ID=<cluster-id> --app conversion-central-backend